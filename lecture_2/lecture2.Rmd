# (PART) Lattice Dynamics and Molecular Dynamics {-}

# Introduction

Part I introduced the potential energy surface — the mapping from atomic configuration to energy. For any arrangement of atoms, there is an associated energy, and the features of this surface encode the physics we care about. Minima correspond to stable structures. Curvature at a minimum determines how atoms vibrate around equilibrium. Barriers between minima control the rates of transitions — diffusion, phase transformations, chemical reactions.

We then covered three approaches to calculating $E(r)$: classical potentials, which are fast but bounded by their functional form; density functional theory, which is transferable but expensive; and machine-learned interatomic potentials, which learn from DFT data and can approach DFT accuracy at near-classical cost. These methods differ in their tradeoffs, but any of them can be combined with the techniques we'll cover today.

The goal now is to answer scientific questions. What structure is stable at a given composition? How do atoms vibrate, and what frequencies characterise that motion? How do atoms move at finite temperature — do they diffuse, and by what mechanism? These questions connect directly to what you measure: stable structures to diffraction, vibrations to inelastic neutron scattering, diffusive motion to quasielastic scattering.

This lecture covers methods that use $E(r)$ to interrogate the potential energy surface. Geometry optimisation locates minima — finding the stable structures. Phonon calculations characterise the curvature at those minima — predicting vibrational frequencies. Molecular dynamics simulates atomic motion at finite temperature, capturing behaviour that goes beyond the harmonic picture. Each of these methods works with any of the $E(r)$ approaches from Part I.

# Finding minima: geometry optimisation

## Why compute a structure?

Geometry optimisation locates minima on the potential energy surface — finding stable structures. But diffraction already tells us where atoms are. Why compute structures at all?

Sometimes the goal is direct comparison: validating a structural model from diffraction analysis. If the optimised structure matches the experimental one, both the calculation and the diffraction analysis are supported. If they disagree, something needs investigation — perhaps the experimental model is incomplete, perhaps the computational method is inadequate for this system, perhaps the disagreement points to something interesting.

More often, computation provides complementary information. Diffraction tells you *what* structure forms; computation tells you *why*. By calculating the energies of competing structures, we can understand polymorph stability, site preferences for dopants, or the driving forces behind structural distortions. We can also calculate energies for structures that *don't* form — understanding what's metastable, what's unstable, and why.

Computation can also resolve what Bragg diffraction struggles with. Some elements have similar scattering lengths — oxygen and fluorine, for instance, are nearly indistinguishable by neutron diffraction, making oxyfluoride structures hard to solve from scattering data alone. Computation can test whether O or F at a given site is energetically preferred. Similarly, partial site occupancies appear as averages in Bragg diffraction, but computation can test the energetics of specific configurations.

Local distortions and short-range order present a different challenge. Bragg diffraction reports only the average structure, so local correlations are invisible. Computation can capture these by optimising large supercells with different local arrangements.

Finally, there is prediction. Computational screening can assess the stability of hypothetical materials before synthesis — testing whether a proposed composition would be thermodynamically stable, and if so, what structure it would adopt. This is increasingly important for materials discovery.

## Structures and minima

When we say a material has a particular "structure", we typically mean the positions determined by diffraction — the time-averaged positions of atoms at the measurement temperature. Computationally, a structure corresponds to a basin on the potential energy surface — a region of configuration space that drains to a particular minimum. Different basins correspond to different structures: polymorphs of the same composition, different orderings of atoms on sites, different local arrangements. A real material's PES has many such basins.

The simplest description of a basin is the position of its minimum — the 0&nbsp;K atomic positions, where atoms sit with no thermal motion. In the harmonic approximation, these are also the mean positions at finite temperature; with anharmonicity they can differ, thermal expansion being the most familiar example.

The procedure for finding such minima is called geometry optimisation. The force on an atom is the negative gradient of energy:

$$\mathbf{F}_i = -\frac{\partial E}{\partial \mathbf{r}_i}$$

At a minimum, all forces are zero. Geometry optimisation adjusts atomic positions (and usually cell parameters) until this condition is satisfied.

## Steepest descent

The simplest approach is steepest descent: calculate the forces on all atoms and move each atom in the direction of its force. Since forces point downhill on the potential energy surface, this reduces the energy. Recalculate forces, move again, iterate.

Steepest descent tells you which direction to move, but not how far. Too small a step and convergence is slow; too large and you overshoot, oscillate, or diverge. Various approaches exist — fixed step sizes, line searches to find the minimum along each descent direction, adaptive schemes that adjust based on whether the energy decreased — but none is universally optimal. The algorithm also tends to zigzag in narrow valleys of the energy surface, where the steepest direction points across the valley rather than along it towards the minimum.

## Doing better with curvature

Newton-Raphson can do better by using more information. Consider first a one-dimensional case: we have $E(x)$ and want to find the minimum. Near any point $x_0$, we can approximate the energy as a Taylor expansion:

$$E(x) \approx E(x_0) + E'(x_0)(x - x_0) + \frac{1}{2}E''(x_0)(x - x_0)^2$$

This expansion is truncated at second order — we assume the energy surface is approximately quadratic near our current point. This is the harmonic approximation, and its validity depends on where we are on the surface. Near a minimum, where the surface is smooth and bowl-shaped, the quadratic approximation is accurate. Far from a minimum, where the surface may have more complex shape, the approximation can be poor.

For a quadratic function, we can find the minimum exactly: differentiate, set to zero, solve. The result is:

$$x = x_0 - \frac{E'(x_0)}{E''(x_0)}$$

This is the predicted position of the minimum, under the assumption that the PES is locally harmonic. The step to take — gradient divided by curvature — makes physical sense: if the curvature is large (a steep, narrow well), we take a small step; if the curvature is small (a shallow, broad well), we take a large step. The curvature tells us how far away the minimum is likely to be.

Because the harmonic assumption is not exact, $x$ won't be the exact minimum — but it usually provides a good estimate. From the new position we can repeat the procedure: recalculate the gradient and curvature, predict a new minimum position, move there. Each iteration refines the approximation. Near a minimum, where the harmonic approximation is accurate, Newton-Raphson converges rapidly — often in just a few iterations.

In three dimensions with <i>N</i> atoms, the same principle applies. The first derivative $E^\prime$ becomes a gradient vector $\mathbf{g}$ with 3<i>N</i> components — one for each atomic coordinate. The second derivative E″ becomes the Hessian matrix **H**, a 3<i>N</i> &times; 3<i>N</i> matrix of second derivatives:

$$H_{ij} = \frac{\partial^2 E}{\partial r_i \partial r_j}$$

The quadratic approximation becomes:

$$E(\mathbf{r}) \approx E_0 + \mathbf{g}^T \mathbf{u} + \frac{1}{2}\mathbf{u}^T \mathbf{H} \mathbf{u}$$

where **u** is the displacement from the current position. The Newton-Raphson step generalises to:

$$\mathbf{u} = -\mathbf{H}^{-1}\mathbf{g}$$

The same iterative logic applies: predict the minimum position, move there, repeat until converged.

Newton-Raphson performs well when the starting guess is close to a minimum, where the harmonic approximation is reasonable. Far from a minimum, where the harmonic approximation breaks down, the algorithm may converge slowly, take erratic steps, or fail to find a minimum at all. In practice, optimisation codes often use methods that are more robust across the whole energy surface — conjugate gradient methods, or quasi-Newton methods (such as BFGS) that build up an approximate Hessian from the gradient information accumulated over successive steps. These handle the early stages of optimisation more reliably, while still converging rapidly near a minimum where the harmonic approximation holds.

## Local and global minima

Geometry optimisation finds a *local* minimum — the one nearest your starting point. A real potential energy surface has many local minima, corresponding to different polymorphs, different site orderings, different local arrangements of atoms. Which minimum you find depends on where you start. This is usually fine: you're testing a specific structural hypothesis, not searching for the global minimum. If you want to compare polymorphs, you optimise each one separately and compare their energies.

Finding the global minimum without knowing where to start — structure prediction — is a harder problem, but not one we need to solve for most purposes. When it is needed, the approach is conceptually simple: generate many starting points and optimise each one. Methods differ in how they generate those starting points — random structures with sensible constraints, evolutionary algorithms that breed new candidates from successful ones, or perturbations from known minima — but at the core, they all rely on the same local optimisation we've just discussed.

# Phonon calculations

## Vibrations around equilibrium

Geometry optimisation finds a minimum on the potential energy surface — a configuration where forces vanish. But atoms don't sit motionless at these positions. At any temperature above absolute zero, thermal energy causes atoms to vibrate around their equilibrium sites. Even at 0 K, quantum zero-point motion means atoms are never truly stationary, though for our purposes the classical picture of thermal vibrations suffices.

This atomic motion is precisely what inelastic neutron scattering probes. When a neutron scatters inelastically from a sample, it exchanges energy with vibrational modes. The measured spectrum S(Q,ω) encodes information about how atoms move — their vibrational frequencies and displacement patterns.

Phonon calculations aim to predict this vibrational behaviour. At a minimum, forces vanish but the Hessian — the matrix of second derivatives — remains, encoding how the energy changes when atoms are displaced from equilibrium. This curvature of the potential energy surface approximately determines vibrational behaviour. Stiff bonds (high curvature) give strong restoring forces and high vibrational frequencies. Soft bonds (low curvature) give weak restoring forces and low frequencies.

## The harmonic approximation

Near a minimum, the potential energy surface is smooth and bowl-shaped. Expanding the energy in a Taylor series around equilibrium:

$$E = E_0 + \sum_i \frac{\partial E}{\partial r_i} \delta r_i + \frac{1}{2} \sum_{ij} \frac{\partial^2 E}{\partial r_i \partial r_j} \delta r_i \delta r_j + \ldots$$

At equilibrium, the first derivatives vanish. If we truncate at second order — the harmonic approximation — the energy becomes quadratic in displacements:

$$E = E_0 + \frac{1}{2} \sum_{ij} H_{ij} \, \delta r_i \, \delta r_j$$

where H is the Hessian matrix. This is the same approximation we used in Newton-Raphson optimisation, but now it serves a different purpose. In optimisation, it was a computational convenience for finding minima efficiently. Here, it makes the vibrational problem tractable — and for many systems, it's also physically accurate.

A quadratic energy means linear restoring forces: F = −Hδr. The equation of motion for atom i is then:

$$m_i \frac{d^2 \delta r_i}{dt^2} = -\sum_j H_{ij} \delta r_j$$

This is a system of coupled linear differential equations. We seek solutions where all atoms oscillate at the same frequency — normal modes. For a harmonic oscillator, solutions are sinusoidal in time. It is mathematically convenient to write these as complex exponentials δrᵢ(t) = uᵢ exp(iωt), with the understanding that the physical displacement is the real part. Differentiating twice:

$$\frac{d^2 \delta r_i}{dt^2} = -\omega^2 u_i \exp(i\omega t)$$

Substituting into the equation of motion, the exp(iωt) factors appear on both sides and cancel, leaving:

$$-m_i \omega^2 u_i = -\sum_j H_{ij} u_j$$

This is an eigenvalue problem, though not quite in standard form because of the mass factor. Introducing the mass-weighted dynamical matrix:

$$D_{ij} = \frac{H_{ij}}{\sqrt{m_i m_j}}$$

converts this to standard form. Diagonalising D gives eigenvalues ω² and eigenvectors that describe the displacement patterns — which atoms move, in what directions, with what relative amplitudes. Each eigenvector corresponds to a normal mode: a collective motion in which all atoms oscillate at the same frequency, maintaining fixed phase relationships.

For a system with N atoms in three dimensions, there are 3N eigenvalues and hence 3N normal modes. Three of these correspond to uniform translation (zero frequency); for periodic solids these become the acoustic modes at q = 0.

## Periodic systems and phonon dispersion

The derivation above applies to any collection of atoms, but for a macroscopic crystal we cannot diagonalise a matrix with 3N rows for every atom in the sample. Translational symmetry solves this.

A normal mode requires all atoms to oscillate at the same frequency — that's what makes it a normal mode. But they need not oscillate in phase. In a periodic crystal, translational symmetry constrains which phase relationships are allowed: if the physics is unchanged by shifting the whole crystal by a lattice vector **R**, the solution must transform consistently under that shift.

The functions with well-defined behaviour under discrete translations are plane waves exp(i**q**·**R**). Different wavevectors **q** correspond to different phase relationships between unit cells. At **q** = 0, exp(i**q**·**R**) = 1 for all **R**, so all cells move in phase. As **q** increases, the phase difference between adjacent cells grows. At the Brillouin zone boundary, exp(i**q**·**R**) = −1 for nearest-neighbour cells — adjacent cells move in antiphase. Beyond the zone boundary, you're relabelling the same physical modes, which is why reciprocal space is periodic.

For each **q**, the displacement pattern within a unit cell is an eigenvector of the dynamical matrix D(**q**); the full solution across the crystal is this pattern multiplied by exp(i**q**·**R**). The key result — derived in the appendix — is that D(**q**) has size 3n × 3n, where n is the number of atoms in the primitive cell. Computationally, this is essential: we diagonalise a small matrix at each **q**, rather than a single impossibly large matrix for the whole crystal.

At each **q**, diagonalisation gives 3n eigenvalues and eigenvectors, corresponding to the 3n branches of the dispersion relation ω(**q**). The phonon band structure plots these dispersion curves along high-symmetry directions in the Brillouin zone. The phonon density of states integrates over all wavevectors, giving the distribution of frequencies regardless of which **q** they came from — this is what powder INS measures directly.

## Calculating phonons in practice

Phonon calculations require the Hessian — the matrix of second derivatives of energy with respect to atomic positions. Most E(r) methods provide forces (first derivatives) directly, but not second derivatives. However, second derivatives can be constructed from forces numerically.

The Hessian element Hᵢⱼ = ∂²E/∂rᵢ∂rⱼ can be written as:

$$H_{ij} = -\frac{\partial F_i}{\partial r_j}$$

where Fᵢ = −∂E/∂rᵢ is the force on coordinate i. This suggests a numerical approach: displace coordinate j by a small amount δr and see how the forces change.

The finite displacement method constructs the Hessian by displacing each atom in turn. For each atom and each Cartesian direction, we shift the atom by +δr, calculate the forces on all atoms, then shift by −δr and recalculate. The central difference gives:

$$H_{ij} \approx -\frac{F_i(r_j + \delta r) - F_i(r_j - \delta r)}{2\delta r}$$

For a system with n atoms in the primitive cell, this requires 6n force calculations (positive and negative displacements in <i>x</i>, <i>y</i>, <i>z</i> for each atom). Each force calculation gives one column of the Hessian. Symmetry can reduce the number of required calculations.

For a periodic system, we work in a supercell. The supercell must be large enough that the displaced atom doesn't interact significantly with its own periodic images — otherwise the force constants are contaminated by artificial periodicity. The supercell size also determines the **q**-point sampling: a 2&times;2&times;2 supercell gives the dynamical matrix at **q**-points commensurate with that supercell.

Density functional perturbation theory (DFPT) takes a different approach, calculating the response of the electronic structure to atomic displacements analytically within perturbation theory. This avoids the need for supercells and gives phonons at arbitrary **q**-points directly, but is more complex to implement and only available for certain $E(r)$ methods.

In practice, both approaches are implemented in standard codes. The finite displacement method works with any $E(r)$ method that provides forces — DFT, MLIPs, even classical potentials. DFPT is typically used with DFT.

## What you get

Phonon calculations provide both eigenvalues (frequencies) and eigenvectors (displacement patterns). Experimentally, INS gives the vibrational spectrum — the density of states $g(\omega)$ — but extracting mode-specific information about which atoms move and how requires careful analysis, often guided by computation. The calculation provides both quantities directly.

The phonon band structure shows dispersion curves along high-symmetry paths, revealing the frequencies of specific modes and any soft modes or instabilities. Single-crystal measurements can probe specific branches, but for complex materials or difficult-to-grow crystals, computation may be the only practical route to the full dispersion. The phonon density of states gives the distribution of vibrational frequencies, summed over all wavevectors and branches — this is what powder INS measures directly.

From the phonon frequencies, thermodynamic quantities follow from standard statistical mechanics. The vibrational free energy:

$$F_{\text{vib}} = \sum_{\mathbf{q},\nu} \left[ \frac{\hbar\omega_{\mathbf{q}\nu}}{2} + k_B T \ln\left(1 - e^{-\hbar\omega_{\mathbf{q}\nu}/k_B T}\right) \right]$$

where the sum runs over wavevectors $q$ and branches $\nu$. From this, heat capacity and vibrational entropy follow by differentiation. These quantities matter for phase stability at finite temperature — a phase with lower energy may not be stable if another phase has higher entropy.

## Limitations

The harmonic approximation assumes small displacements around a well-defined minimum. This breaks down in several situations.

At high temperatures, atomic displacements become large and the quadratic approximation fails. Anharmonic terms in the potential — the higher-order terms we truncated — become significant. This leads to effects like phonon-phonon scattering, finite thermal conductivity, and temperature-dependent phonon frequencies. This is true anharmonicity: atoms still oscillate around equilibrium positions, but the potential well is not quadratic.

When the harmonic approximation fails in this sense, one option stays within the lattice dynamics framework but includes higher-order terms. Recall that the harmonic approximation truncates the Taylor expansion at second order. Including third and fourth order terms:

$$E = E_0 + \frac{1}{2}\sum_{ij} H_{ij} \delta r_i \delta r_j + \frac{1}{6}\sum_{ijk} \Phi_{ijk} \delta r_i \delta r_j \delta r_k + \frac{1}{24}\sum_{ijkl} \Phi_{ijkl} \delta r_i \delta r_j \delta r_k \delta r_l + \ldots$$

The third-order terms $\Phi_{ijk} describe three-phonon processes — one phonon decaying into two, or two combining into one. These are responsible for finite thermal conductivity. The fourth-order terms describe four-phonon processes.

The problem is that while the Hessian has <i>n</i><sup>2</sup> elements (for <i>n</i> atomic coordinates), the third-order tensor has <i>n</i><sup>3</sup> elements and the fourth-order tensor has <i>n</i><sup>4</sup>. More fundamentally, the harmonic problem gives 3<i>N</i> independent modes. Including anharmonicity couples these modes to each other: you need to consider how each mode interacts with every other mode (for three-phonon) or every pair of modes (for four-phonon). The number of terms grows combinatorially, making these calculations substantially more expensive.

Some systems go beyond anharmonicity — the picture of atoms oscillating around fixed equilibrium positions breaks down entirely. Superionic conductors, where ions diffuse rapidly through a solid framework, cannot be described by vibrations around fixed sites. Soft modes that approach zero frequency signal structural instabilities. Phase transitions involve large-amplitude motion between different structural basins. For these, no Taylor expansion around a single minimum will suffice.

# Molecular dynamics: principles

## From local curvature to global exploration

Geometry optimisation finds minima on the potential energy surface. Phonon calculations characterise the curvature at those minima — how the energy changes for small displacements. Both assume atoms remain close to their equilibrium positions: optimisation iterates toward a minimum, and the harmonic approximation expands the energy around it.

At finite temperature, this picture becomes incomplete. Atoms have kinetic energy and explore the potential energy surface. For mildly anharmonic systems, we can extend lattice dynamics with higher-order terms — but as we saw, this becomes expensive. For systems where atoms don't remain near any single minimum — diffusing ions, phase transitions, liquids — no Taylor expansion around one point will suffice.

Molecular dynamics takes a different approach. Rather than expanding E(r) and truncating, we follow the actual motion of atoms over the potential energy surface. Given positions and velocities at some instant, we calculate the forces on all atoms, use these to update the velocities and positions a short time later, and repeat. The result is a trajectory: a sequence of configurations showing how the system evolves in time.

## Equations of motion

The force on atom i is the negative gradient of the potential energy:

$$\mathbf{F}_i = -\nabla_i E(\mathbf{r})$$

Given the force, Newton's second law gives the acceleration:

$$m_i \frac{d^2 \mathbf{r}_i}{dt^2} = \mathbf{F}_i$$

The motion of our system is described by two coupled differential equations:

$$\frac{d\mathbf{r}}{dt} = \mathbf{v}$$

$$\frac{d\mathbf{v}}{dt} = \mathbf{a} = \frac{\mathbf{F}}{m}$$

If we could integrate these equations exactly, we could predict the positions and velocities at any future time. But the forces depend on positions, which change as the system evolves — for any realistic system, we cannot solve these equations analytically.

## Numerical integration

Instead, we integrate numerically. The principle is to approximate the continuous equations over small time intervals. If we assume the acceleration is roughly constant over a short time Δt, we can write:

$$\mathbf{v}(t + \Delta t) \approx \mathbf{v}(t) + \mathbf{a}(t) \Delta t$$

$$\mathbf{r}(t + \Delta t) \approx \mathbf{r}(t) + \mathbf{v}(t) \Delta t + \frac{1}{2}\mathbf{a}(t) \Delta t^2$$

This is Euler integration: use the current acceleration and velocity to predict the state at the next timestep, then repeat. The approximation becomes exact as Δt → 0, but practical calculations require finite timesteps.

Euler integration has problems. First, it introduces systematic errors at each step — small discrepancies between the predicted trajectory and the true one. These errors accumulate over many steps, causing the total energy to drift rather than remaining constant as it should for an isolated system.

Second, Euler integration is not time-reversible. Newtonian mechanics is symmetric under time reversal: if we reverse all velocities, the system should retrace its path. Euler's method breaks this symmetry because it uses information only from the beginning of each timestep. A forward step followed by reversing velocities and stepping again does not return to the starting point.

The velocity Verlet algorithm, used in most practical MD codes, avoids these problems. It updates positions using both the current and next accelerations:

$$\mathbf{r}(t + \Delta t) = \mathbf{r}(t) + \mathbf{v}(t) \Delta t + \frac{1}{2}\mathbf{a}(t) \Delta t^2$$

$$\mathbf{v}(t + \Delta t) = \mathbf{v}(t) + \frac{1}{2}[\mathbf{a}(t) + \mathbf{a}(t + \Delta t)] \Delta t$$

The velocity update uses the average of the old and new accelerations, making the algorithm time-reversible. Energy is not conserved exactly, but fluctuates around the correct value without systematic drift.

The timestep must be small enough to capture the fastest motion in the system — typically the highest-frequency vibrations — which means Δt is usually around 1 femtosecond. A nanosecond trajectory therefore requires 10⁶ timesteps, each requiring a force evaluation.

## Phase space and trajectories

The state of a classical system is specified by the positions and momenta of all atoms — a point in phase space. As we integrate the equations of motion, this point traces out a trajectory through phase space.

For an isolated system (no external heat bath), the total energy E = K + V is conserved — kinetic energy converts to potential and back, but the sum remains constant. The trajectory is confined to a surface of constant energy in phase space. This is the microcanonical ensemble: all accessible states have the same total energy.

The temperature in such a system is related to the average kinetic energy:

$$\langle K \rangle = \frac{3}{2} N k_B T$$

where N is the number of atoms. This follows from the equipartition theorem: each quadratic degree of freedom contributes ½kT to the average energy, and kinetic energy is quadratic in the velocities.

## Ensembles and what they mean

Real experiments are usually not performed at constant energy. A sample in a furnace exchanges heat with its surroundings; it has a well-defined temperature, not a fixed total energy. Different experimental conditions correspond to different statistical ensembles.

The **microcanonical ensemble (NVE)** has fixed particle number N, volume V, and energy E. This is what unmodified Newtonian dynamics samples. It describes an isolated system.

The **canonical ensemble (NVT)** has fixed N, V, and temperature T. The system exchanges energy with a heat bath at temperature T. States are weighted by the Boltzmann factor exp(−E/kT): lower-energy configurations are more probable, but higher-energy configurations are accessible with probability that decreases exponentially.

The **isothermal-isobaric ensemble (NPT)** has fixed N, pressure P, and temperature T. The system exchanges both energy and volume with its surroundings. This corresponds to most laboratory conditions — a sample at ambient pressure and controlled temperature.

Which ensemble we simulate determines what statistical distribution we sample. This matters for calculating thermodynamic properties and for comparing with experiment.

## Thermostats and barostats

To simulate NVT or NPT, we need to modify the equations of motion so that the trajectory samples the correct statistical distribution. A thermostat couples the system to a heat bath; a barostat couples it to a pressure reservoir.

The details of how thermostats work are technical, but the key point is conceptual: a good thermostat is designed so that, over a long trajectory, configurations are visited with the correct Boltzmann weights. The time-average of any property over the trajectory then equals the ensemble average — the thermodynamic expectation value.

This is the ergodic hypothesis in practice: for a sufficiently long trajectory, the time spent in each region of phase space is proportional to its statistical weight. We can therefore compute ensemble averages — the quantities that connect to thermodynamics and to time-averaged experimental measurements — from a single long trajectory.

## What you get

A molecular dynamics simulation produces a trajectory: atomic positions (and velocities) at each timestep. From this trajectory, we can calculate:

**Equilibrium properties** — averages over the trajectory. The mean potential energy, the average structure, the distribution of atomic positions. These are ensemble averages, directly comparable to time-averaged experimental measurements.

**Dynamic properties** — how the system evolves in time. How do atomic positions correlate between different times? How quickly do correlations decay? These time correlation functions contain information about dynamics that equilibrium measurements cannot access.

**Transport properties** — diffusion coefficients, viscosities, thermal conductivities. These emerge from how quickly the system loses memory of its initial state.

## The E(r) choice

Every MD timestep requires calculating forces — one evaluation of E(r) and its gradient. A typical simulation might run for millions of timesteps. The cost of E(r) therefore determines what simulations are feasible.

**Classical potentials** are cheap to evaluate. Simulations of millions of atoms for nanoseconds to microseconds are routine. The limitation is accuracy: the functional form constrains what physics can be captured.

**Ab initio MD (AIMD)** calculates forces from DFT at each timestep. This is accurate but expensive — practical for hundreds of atoms and picoseconds. Many dynamical processes happen on longer timescales or require larger system sizes.

**Machine-learned interatomic potentials** approach DFT accuracy at near-classical cost. They are increasingly the method of choice when classical potentials aren't accurate enough but AIMD is too expensive. The caveats from Lecture I apply: they inherit limitations from their training data, and can fail outside their training domain.

The choice is problem-dependent. For a quick estimate of dynamics in a well-characterised material, classical potentials may suffice. For quantitative comparison with experiment in a system where accuracy matters, MLIPs or AIMD may be necessary.



