# Conclusion {-}

These notes have covered substantial ground: methods for calculating$ $E(r)$, techniques for using those calculations to answer scientific questions, and approaches for treating configurational disorder. Before closing, it is worth drawing out the threads that run through this material.

## Four themes

**Neutrons and computation probe the same physics.** This is not a loose analogy. Phonon densities of states, dynamic structure factors, pair distribution functions — these quantities can be obtained from either neutron experiments or computation, and the results can be compared directly. When a modeller calculates $S(Q,\omega)$ from molecular dynamics and an experimentalist measures $S(Q,\omega)$ with QENS, the comparison is quantitative: the curves can be overlaid, the agreement or disagreement assessed. This shared language exists because neutrons scatter from nuclei, and the computational methods covered in these lectures model nuclear positions and motion. Both approaches interrogate the same underlying physics from different directions.

**Method choice depends on the question.** There is no universally best computational approach. Classical potentials are fast but limited by their functional form. DFT is transferable but expensive. Machine-learned potentials offer a middle path but require careful validation. Geometry optimisation finds static structures; molecular dynamics captures finite-temperature motion; Monte Carlo samples configuration space without following dynamics. The choice of $E(r)$ method and the choice of what to do with it are independent decisions, both determined by what you want to learn. A quick survey of dynamics in a well-characterised material might use classical potentials; a quantitative comparison of competing structures might require DFT; extensive sampling of configurational disorder might need cluster expansion. Recognising which tool fits which problem is more important than knowing the technical details of any single method.

**Structural information becomes increasingly subtle.** Bragg diffraction determines average, periodic structure — the positions that atoms occupy on average, with the periodicity of the crystal. This is the starting point, but not always the complete picture. Finite-temperature dynamics means atoms are not stationary at their crystallographic sites. Ensemble averaging becomes relevant when thermal motion or disorder is present. And local correlations — short-range order — may exist even when long-range periodicity does not. The trajectory through these lectures has moved from 0 K structures to finite-temperature dynamics to ensemble averages to configurational disorder and short-range order. Each step requires both experimental techniques (moving from Bragg diffraction to total scattering, from elastic to inelastic and quasielastic scattering) and computational approaches suited to the question.

**Computation provides the "why" behind observations.** Experiment reveals what is present: this structure, this diffusion rate, this vibrational spectrum, this PDF. Computation can explain why. Why does this phase have lower energy than that one? What barrier controls diffusion, and what mechanism operates? Why does Li₂FeSO adopt *cis*-OLi<sub>4</sub>Fe<sub>2</sub> coordination rather than the *trans* arrangement that simple electrostatics predicts? This explanatory power is distinct from prediction. Computation does not merely reproduce experimental observations — it provides access to the energetics, the mechanisms, and the physical principles that underlie them. This is information that experiment alone cannot directly access.

## What now?

When reading computational papers in your field, you are now equipped to ask critical questions. What $E(r)$ method was used — classical potential, DFT, machine-learned potential? Is that method appropriate for the question being asked? How do the calculated quantities connect to experimental observables? Was the comparison to experiment direct and quantitative, or indirect and qualitative? What are the limitations of the approach, and are they acknowledged? These questions do not require you to run calculations yourself, but they do require understanding what different methods can and cannot do.

When considering collaboration with modellers, you can ask more productive questions. What can computation provide that your measurements cannot directly access — energetics, barriers, mechanisms, partials of the PDF? What experimental data would constrain the modelling or validate the results? Collaboration works best when both sides understand what the other can contribute.

When interpreting your own experimental results, you might recognise opportunities. An unexpected feature in a PDF, a relaxation process that does not fit standard models, a structure that does not quite match expectations — these are the puzzles that drive productive computational investigation. The case study in the previous section began with exactly this kind of observation: diffraction said no long-range order, but is the distribution really random?

You do not need to become a computational scientist. But recognising what computation can and cannot do, understanding how its outputs connect to your measurements, and knowing what questions to ask — these make you a more effective scientist and a more valuable collaborator. Computation and experiment together are more powerful than either alone.