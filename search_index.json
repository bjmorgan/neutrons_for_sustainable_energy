[["index.html", "Computational Techniques for Sustainable Energy Materials About", " Computational Techniques for Sustainable Energy Materials Benjamin J. Morgan 2025-12-07 About These notes accompany the Computational Tools lectures for the 2025 Neutrons for Sustainable Energy training school at KTH Stockholm. The material is organised in three parts: Part Topic Lecture I Calculating \\(E(r)\\) Lecture I II Structure and Dynamics Lecture II III Configurational Disorder Lecture III The notes can be read before the lectures as preparation, used during the lectures as reference, or consulted afterwards for review. They are also intended to stand alone for readers who did not attend the training school. "],["introduction.html", "Introduction Shared observables Scope Energy materials", " Introduction Neutron scattering probes structure and dynamics — where atoms are and how they move. Computational modelling calculates structure and dynamics. In many cases, both approaches produce the same observables: pair distribution functions, phonon densities of states, dynamic structure factors. This overlap is not coincidence. Neutrons scatter from nuclei; computational methods model nuclear positions and motion. The two approaches interrogate the same physics from different directions. This complementarity makes computation valuable for neutron scientists. Calculated observables can be compared directly with measurements. Where experiment and calculation agree, both are supported. Where they disagree, something interesting demands investigation — an inadequate model, an unexpected feature of the material, or a sample that differs from its idealised description. Computation also provides what experiment cannot directly access. Scattering data reveal that atoms occupy certain positions and move at certain rates. Computation can explain why: the energetics that favour one structure over another, the barriers that control diffusion, the anharmonicity that limits thermal conductivity. This explanatory power complements the descriptive power of measurement. The relationship runs both ways. Experimental puzzles drive computational investigation. A PDF that does not fit the expected structure, a relaxation process faster than models predict, an anomalous temperature dependence — these observations prompt computational work. The most productive research often emerges from this dialogue. Shared observables The following tables summarise quantities accessible from both neutron experiments and computation: Structure — where are atoms? Observable From neutrons From computation Structure (ordered) Diffraction Geometry optimisation, structure prediction Structure (disordered / finite T) Diffraction Ensemble average from MD or MC g(r) / PDF Total scattering MD or MC configurations Dynamics — how do atoms move? Observable From neutrons From computation S(Q,ω) — vibrational INS Lattice dynamics or MD S(Q,ω) — diffusive QENS MD → Van Hove → Fourier transform These are not loose analogies. When a modeller calculates S(Q,ω) from molecular dynamics and an experimentalist measures S(Q,ω) with QENS, the results can be overlaid directly. Scope These notes are not an exhaustive survey of computational methods. The focus is on techniques that either calculate observables measurable with neutrons, or provide complementary information about structure and dynamics. This means largely ignoring electronic properties — band gaps, optical response, electronic transport, and magnetism — even though these are important for some applications. Photovoltaics require optical properties, excited states, and electron-phonon coupling. Thermoelectrics require electronic transport properties alongside phonon properties. Magnetocalorics require treatment of magnetic ordering and spin-lattice coupling. These are beyond the present scope — the focus here is structure and dynamics, where neutron scattering and computational methods most directly connect. Neutrons primarily probe nuclear positions and motion — that is where these notes focus. The level of detail reflects the goal of building understanding rather than expertise. The emphasis is on conceptual foundations: what each method does, what approximations it makes, what questions it can answer, and how its outputs connect to experiment. Detailed derivations and underlying theory are not always included, though key results are stated and their significance explained. Practical aspects of running calculations — software choices, input file preparation, convergence testing — are largely omitted. The aim is to produce informed consumers of computational work and effective collaborators, not practicing computational scientists. Energy materials The materials relevant to sustainable energy — batteries, supercapacitors, hydrogen storage, photovoltaics, thermoelectrics — share common features: Ion transport matters (Li, H, O moving through structures) Disorder is often present (mixed site occupancy, local distortions) Finite-temperature behaviour is important (materials operate at room temperature or above, not 0 K) These are exactly the areas where computation and neutron scattering are most powerfully complementary. Neutrons are sensitive to light elements — hydrogen, lithium, oxygen — that X-rays struggle with. Computation can decompose complex dynamics into contributions from different species or different mechanisms, which experiment averages over. "],["the-potential-energy-surface.html", "1 The potential energy surface 1.1 Configuration and energy 1.2 Features of the PES 1.3 Two questions", " 1 The potential energy surface All computational modelling of materials rests on a common foundation: for any arrangement of atoms, there is an associated energy. This mapping — configuration to energy — defines the potential energy surface (PES). 1.1 Configuration and energy Consider a crystal with two types of atoms in a square unit cell. Call this structure \\(S_1\\), with atomic positions \\(\\mathbf{r}_1\\) and energy \\(E_1\\). Swap some atoms: sites that held A atoms now hold B atoms, and vice versa. The atoms sit at different positions, \\(\\mathbf{r}_2\\), and this structure \\(S_2\\) has a different energy \\(E_2\\). We can consider different changes — shearing the unit cell into a parallelogram, for example, gives structure \\(S_3\\), with another different set of positions \\(\\mathbf{r}_3\\) and another different energy \\(E_3\\). This behaviour generalises: a structure is defined by a set of atomic positions, and each set of positions maps to a corresponding energy. The shear transformation reveals something more. The transition from \\(S_1\\) to \\(S_3\\) is continuous: gradually shearing the cell, every point along the way has positions slightly different from the last, with a correspondingly different energy. The mapping from positions to energy is not just defined at discrete structures — it is a continuous surface. This is the potential energy surface: \\(E(\\mathbf{r})\\), where \\(\\mathbf{r}\\) represents the positions of all atoms. For \\(N\\) atoms in three dimensions, \\(\\mathbf{r}\\) has \\(3N\\) coordinates. Any point in this \\(3N\\)-dimensional space is a configuration — a complete specification of all atomic positions. And every configuration has an energy \\(E(\\mathbf{r})\\). The potential energy surface is a mathematical object. A direct physical pathway between two points is not required for both to exist on the surface. The transition from \\(S_1\\) to \\(S_2\\) has no direct pathway — exchanging atom types would require atoms to pass through each other. Yet both are well-defined points with well-defined energies. In real materials, such changes happen by indirect routes: vacancy diffusion, interstitial migration, or other mechanisms that avoid atomic overlap. These indirect routes are themselves paths on the surface. 1.2 Features of the PES The features of the potential energy surface have direct physical significance. Minima are stable structures. A minimum is a configuration where small displacements in any direction increase the energy — forces on all atoms are zero, and the structure is locally stable. These are the crystal structures determined from diffraction: the system sits at a minimum (or near one, at finite temperature). Relative energies of minima determine thermodynamics. If \\(S_1\\) has lower energy than \\(S_2\\), then \\(S_1\\) is thermodynamically preferred — at least at zero temperature. At finite temperature, entropy also matters, and the relevant quantity becomes the free energy rather than the energy. Barriers control kinetics. Along the pathway from \\(S_1\\) to \\(S_2\\), there is typically a maximum — an energy barrier. The height of this barrier determines how fast the transformation happens. A high barrier means the process is slow; a low barrier means it is fast. This applies to phase transitions, ionic diffusion, and any process where the system moves from one configuration to another. Diffraction reveals which structure is present; the barrier height explains why a system may be trapped in a metastable minimum. The shape of the PES determines what is physically possible, not just what is energetically preferred. Consider a direct atom swap — two atoms exchanging positions. A continuous pathway for this exists on the PES, but it requires atoms to pass through each other, where the energy diverges. The barrier is infinite; the pathway is kinetically forbidden. In real materials, atoms change places by other mechanisms — via vacancies, interstitials, or correlated motion — pathways through configuration space that avoid catastrophic overlap. Curvature at minima determines vibrational properties. The energy change for small displacements around a minimum depends on the shape of the potential well. A steep, narrow well produces strong restoring forces and high vibrational frequencies. A shallow, broad well produces softer vibrations at lower frequencies. This curvature is what phonon calculations determine — and what inelastic neutron scattering measures. The shape of the PES around a minimum appears directly in INS spectra. 1.3 Two questions How is \\(E(r)\\) calculated? Given an arrangement of atoms, how is its energy determined? Different methods — classical potentials, density functional theory, machine-learned potentials — give different answers with different tradeoffs of accuracy and computational cost. What is done with \\(E(r)\\)? Given the ability to calculate energies, what questions can be answered? Finding the lowest-energy structure is one application. Characterising vibrations around that structure is another. Simulating atomic motion at finite temperature, or sampling over different configurations — these require different approaches. These are independent choices. Molecular dynamics can use a classical potential, DFT, or a machine-learned potential — the choice of how to calculate \\(E(r)\\) is independent of the choice to do MD. A common misconception is that “molecular dynamics” implies classical potentials, or that “DFT” implies static calculations. Neither is true. Any method for calculating \\(E(r)\\) can be combined with any method for using it. The remainder of Part I covers three approaches to calculating E(r): classical potentials, density functional theory, and machine-learned interatomic potentials. Parts II and III turn to what is done with that ability — geometry optimisation, phonon calculations, molecular dynamics, and Monte Carlo sampling. "],["classical-potentials.html", "2 Classical potentials 2.1 Why start here? 2.2 The simplest model: pair potentials 2.3 Building a potential from physics 2.4 Periodic systems 2.5 When pair potentials aren’t enough 2.6 The parameterisation problem 2.7 Strengths and limitations", " 2 Classical potentials 2.1 Why start here? Classical potentials are not the most accurate method for calculating E(r), but they are the most transparent. You can write down the function, plot it, and see the potential energy surface directly. This builds intuition before moving to methods where \\(E(r)\\) comes out of a more complex calculation. 2.2 The simplest model: pair potentials Classical potentials are parameterised analytical functions that give the energy of a configuration of atoms. A mathematical form is chosen, parameters are fitted to reproduce known properties, and the resulting function is used to calculate energies for any configuration. The simplest approach is to decompose the total energy into contributions from pairs of atoms: \\[E = \\sum_{ij} V(r_{ij})\\] where \\(r_{ij}\\) is the distance between atoms \\(i\\) and \\(j\\), and \\(V(r)\\) is a pair potential. For each pair of atoms, we look up their separation, evaluate the pair potential, and sum over all pairs. This is conceptually simple and computationally cheap. 2.3 Building a potential from physics What should \\(V(r)\\) look like for an ionic crystal? The dominant contribution is electrostatic. Ions carry charges, so opposite charges attract and like charges repel. For two ions with charges \\(q_i\\) and \\(q_j\\) separated by distance \\(r\\), the Coulomb energy is \\(q_iq_j/r\\). This is long-range — it falls off slowly with distance. Electrostatics alone would cause the crystal to collapse — opposite charges attracting without limit. What prevents this is short-range repulsion. When ions get close, their electron clouds overlap, and this costs energy. The repulsion is modelled with a steeply rising function at short range. A common choice is the Born-Mayer form: \\(A\\exp(−r/\\rho)\\), an exponential that increases rapidly as \\(r\\) decreases. There is also a weak, long-range attraction from van der Waals forces, typically modelled as \\(−C/r^6\\). For hard ions this is often a small correction, but it matters more for polarisable species. Combining Coulomb, exponential repulsion, and dispersion gives a Buckingham-type potential — the workhorse for simulating ionic solids like oxides and halides. 2.4 Periodic systems Most simulations aim to model bulk materials, but a computer simulation is necessarily finite — typically thousands to millions of atoms. This creates a problem: a finite cell has edges, and atoms near those edges behave differently from those in the interior. The solution is to apply periodic boundary conditions: the simulation cell tiles infinitely in all directions, so an atom near one boundary sees atoms from the opposite side as nearest neighbours. The result is a system with no surfaces — only bulk. Periodicity introduces its own complication. Each atom now interacts not just with other atoms in its cell, but with all their periodic images — infinitely many copies stretching in every direction. The total energy becomes an infinite sum. For short-range interactions, this infinite sum is manageable. The exponential repulsion and dispersion terms decay rapidly with distance, so contributions beyond a nanometre or so are negligible. But which atoms count as “nearby” in a periodic system? Consider atom A interacting with atom B. There is a copy of B in every periodic image — infinitely many, at different distances from A. For a short-range interaction, only the nearest copy of B is close enough to matter. This is the minimum image convention: for each pair of atoms, consider only the nearest periodic image. Combined with a cutoff distance beyond which interactions are ignored entirely, and provided this cutoff is less than half the cell length, the infinite sum reduces to a finite and well-defined calculation. The Coulomb interaction poses a harder problem. It decays as \\(1/r\\) — too slowly to truncate. Cutting off the sum at any finite distance gives results that depend on where you cut, which is unphysical; the sum over periodic images is only conditionally convergent. Ewald summation resolves this by splitting the interaction into two parts. Each point charge is screened with a compensating Gaussian distribution. The screened charges interact only at short range and sum quickly in real space. The smooth Gaussian distributions sum quickly in reciprocal space. A correction removes the spurious self-interaction of each charge with its own screening cloud. The result is two absolutely convergent sums that together give the correct electrostatic energy. The reciprocal-space treatment will be familiar from diffraction. 2.5 When pair potentials aren’t enough Pair potentials assume the energy contribution from two atoms depends only on their separation. This works well for ionic materials where bonding is non-directional. But for covalent or partially covalent materials, where bond angles matter, pair potentials are insufficient. In a silicate, for instance, the O–Si–O angle isn’t arbitrary — there’s a preferred tetrahedral geometry. Capturing this requires three-body terms that penalise deviations from the preferred angle. For molecular systems or structures with well-defined bonded units, the “force field” approach uses harmonic springs for bond stretches, angle terms for bond angles, and torsional terms for dihedrals. More complex functional forms exist for metals (embedded atom method), semiconductors (Tersoff potentials), and reactive systems (ReaxFF). The basic ionic model also treats ions as fixed point charges. Real ions are polarisable — their electron clouds respond to local electric fields — and may have non-spherical charge distributions. Extensions address this through explicit induced dipoles, higher-order multipoles, variable ion size and shape, or charge equilibration schemes where charges vary self-consistently subject to a total charge constraint. Classical potentials can be much more sophisticated than the basic Buckingham form. But there is always a tradeoff: more realism means more parameters, more complexity, and more fitting required. 2.6 The parameterisation problem A classical potential has parameters — charges, repulsion coefficients, cutoffs. Where do they come from? Historically, parameters were fitted to reproduce experimental properties: lattice constants, elastic moduli, phonon frequencies, defect energies. The potential is only as good as the data it was fitted to, and only as transferable as that data allows. There is also a risk of overfitting: fitting five parameters to three experimental observables leaves the problem underdetermined. Parameter sets may reproduce those observables perfectly but for the wrong reasons, and fail elsewhere. Increasingly, potentials are fitted to DFT calculations. A set of configurations is generated, their energies and forces are calculated with DFT, and the potential parameters are fitted to reproduce them. This covers regions of configuration space that experiment doesn’t probe directly, and provides enough data points to constrain all parameters. Either way, the quality of a classical potential depends entirely on how well the parameterisation captures the relevant physics. And there is a more fundamental issue: the functional form itself bounds the accuracy. The physics that can be captured is limited by the mathematical form chosen. If the real interactions don’t match that form, no amount of parameter fitting will fix it. 2.7 Strengths and limitations Classical potentials are fast — orders of magnitude cheaper than quantum-mechanical methods. Simulations of millions of atoms for nanoseconds are routine. With methods like particle-mesh Ewald or fast multipole for electrostatics, computational cost scales as O(N log N) with system size. Classical potentials are also interpretable: each term’s contribution is visible, and parameter–property relationships can be understood. When something goes wrong, diagnosis is possible. For some material classes — simple ionic solids, for instance — good potentials exist and have been extensively validated. The limitations are significant. The treatment of electrostatics is typically simple: fixed point charges throughout the simulation, with no polarisation response or charge transfer. Electronic structure is implicit — electrons aren’t explicitly treated, so processes involving electronic rearrangement cannot be described. Developing a good classical potential is hard work — designing a functional form, generating training data, fitting, validating, finding problems, and iterating. This requires substantial effort with no guarantee of success. Classical potentials also have limited transferability. A potential fitted to one material, or one region of configuration space, may fail for another. Bulk doesn’t guarantee surfaces; ambient conditions don’t guarantee high pressure. Extrapolation fails silently — the potential gives a number even outside its valid domain. Classical potentials are the right choice for large system sizes or long timescales, for well-characterised material classes where good potentials exist, or for quick exploratory calculations. They are the wrong choice when the required accuracy exceeds what the potential can provide, or when moving outside its validated domain. "],["density-functional-theory.html", "3 Density functional theory 3.1 Why first principles? 3.2 From wavefunctions to density 3.3 The Kohn-Sham approach 3.4 Exchange-correlation functionals 3.5 Strengths and limitations", " 3 Density functional theory 3.1 Why first principles? Classical potentials have fundamental limitations: the accuracy is bounded by the functional form you choose, and someone has to develop and validate a potential for your specific system. First-principles methods address this. Instead of parameterising interactions, we calculate \\(E(r)\\) from quantum mechanics—solving (approximately) for the electrons in your system. The physics is built in, not fitted. 3.2 From wavefunctions to density In principle, the energy of a collection of electrons and nuclei comes from solving the Schrödinger equation. The ground-state wavefunction \\(\\psi(r_1, r_2, \\ldots, r_n)\\) gives everything — the energy, the electron density, all ground-state properties. The problem is dimensionality. For \\(N\\) electrons, the wavefunction is a function of \\(3N\\) spatial coordinates. The complexity grows exponentially with the number of electrons — intractable for real materials. The electron density \\(\\rho(\\mathbf{r})\\) offers an alternative. Unlike the wavefunction, the density is always three-dimensional — ten electrons or ten thousand, it is still just \\(\\rho(x, y, z)\\). The first Hohenberg-Kohn theorem (1964) established that for ground states, the energy can be written as a functional of the density: \\(E = E[\\rho]\\). A functional maps a function to a number — here \\(\\rho(\\mathbf{r})\\) is a function of position, and \\(E\\) is a single number. The theorem does not specify what this functional is, only that it exists. The second Hohenberg-Kohn theorem establishes that this functional is variational: the density that minimises \\(E[\\rho]\\) is the true ground-state density. Together, these theorems imply that the exact ground-state energy could be found by minimising over three-dimensional densities rather than \\(3N\\)-dimensional wavefunctions — if we knew \\(E[\\rho]\\). We can write: \\[E[\\rho] = T[\\rho] + V_\\mathrm{ee}[\\rho] + \\int v(\\mathbf{r})\\rho(\\mathbf{r})\\,d\\mathbf{r}\\] where \\(T[\\rho]\\) is the kinetic energy, \\(V_\\mathrm{ee}[\\rho]\\) is the electron-electron repulsion, and the final term is the electron-nuclear attraction. The last term is straightforward — we know \\(v\\) from the nuclear positions. The first two are not: we do not know how to write them as explicit functionals of the density. 3.3 The Kohn-Sham approach Kohn and Sham (1965) addressed this by separating each unknown term into a part that can be calculated plus a remainder. For the electron-electron interaction, the classical Coulomb energy of a charge distribution is: \\[J[\\rho] = \\frac{1}{2}\\int\\int \\frac{\\rho(\\mathbf{r})\\rho(\\mathbf{r}&#39;)}{|\\mathbf{r}-\\mathbf{r}&#39;|}\\, d\\mathbf{r}\\, d\\mathbf{r}&#39;\\] This can be evaluated directly from the density. What it misses are exchange (electrons with parallel spin avoid each other, a consequence of the Pauli principle) and correlation (electrons avoid each other due to their mutual repulsion). For the kinetic energy, there is a system for which this is straightforward: non-interacting electrons. For such a system, the ground state is a Slater determinant of one-electron orbitals \\(\\phi_i\\), and the kinetic energy is the sum of one-electron contributions: \\[T_s = -\\frac{1}{2}\\sum_i \\langle\\phi_i|\\nabla^2|\\phi_i\\rangle\\] The true kinetic energy \\(T[\\rho]\\) differs from \\(T_s[\\rho]\\) because interacting electrons move differently from non-interacting ones — but the difference is small. Both systems have the same density, so electrons are confined to the same regions of space, and kinetic energy is largely determined by spatial confinement. The remainders from both separations are bundled into a single correction, the exchange-correlation functional: \\[E_\\mathrm{xc}[\\rho] = (T[\\rho] - T_s[\\rho]) + (V_\\mathrm{ee}[\\rho] - J[\\rho])\\] The total energy becomes: \\[E[\\rho] = T_s[\\rho] + J[\\rho] + \\int v(\\mathbf{r})\\rho(\\mathbf{r})\\,d\\mathbf{r} + E_\\mathrm{xc}[\\rho]\\] The first three terms can be calculated; all the unknowns are in \\(E_\\mathrm{xc}\\). To evaluate \\(T_s\\) requires finding the orbitals of the non-interacting system. These satisfy one-electron Schrödinger equations in an effective potential \\(v_\\mathrm{eff} = v + v_J + v_\\mathrm{xc}\\), where \\(v\\) is the nuclear potential, \\(v_J\\) is the classical Coulomb potential from the electron density, and \\(v_\\mathrm{xc}\\) is the functional derivative of \\(E_\\mathrm{xc}\\) with respect to \\(\\rho\\). The density is constructed from the occupied orbitals: \\[\\rho(\\mathbf{r}) = \\sum_i |\\phi_i(\\mathbf{r})|^2\\] Since \\(v_J\\) and \\(v_\\mathrm{xc}\\) depend on \\(\\rho\\), which depends on the orbitals, which depend on \\(v_\\mathrm{eff}\\), the equations must be solved iteratively until self-consistent. Nothing in this development is an approximation — it is exact repackaging into terms that can be evaluated plus a correction. If \\(E_\\mathrm{xc}[\\rho]\\) were known exactly, Kohn-Sham DFT would give the exact ground-state energy. All the approximation in practical DFT lives in one place: \\(E_\\mathrm{xc}\\). 3.4 Exchange-correlation functionals The simplest approximation is the local density approximation (LDA): at each point, use the exchange-correlation energy of a uniform electron gas with that density. This is surprisingly effective, but tends to overbind—predicting bonds that are too strong and too short. Generalised gradient approximations (GGAs) improve on this by including how the density varies—not just \\(\\rho\\) but also \\(\\nabla \\rho\\). PBE is the most common GGA in materials science. Meta-GGAs go further, including the kinetic energy density as well. r2SCAN is increasingly the reference level for materials science calculations. Hybrid functionals mix in a fraction of exact exchange from Hartree-Fock theory. These often give better band gaps and reaction energies, but are significantly more expensive—you’re reintroducing the costly exact exchange integrals. 3.5 Strengths and limitations DFT is first-principles and transferable—the same functional works across different chemistries without system-specific parameterisation. It’s accurate for structures and relative energies, and scales as roughly N3, making it practical for systems of hundreds of atoms. The limitations are significant. The computational cost means DFT is practical for hundreds of atoms, not thousands, and for static calculations or short dynamics rather than the long timescales often needed to study diffusion or phase transitions. For those problems, you need classical potentials or machine-learned interatomic potentials. More fundamentally, DFT offers no systematic path to the exact answer. In wavefunction methods, you can improve accuracy by including more excitations or larger basis sets—the path is clear even if expensive. In DFT, accuracy depends on choosing a good functional, and a better functional requires new physical insight, not just more computer time. This also means different functionals give different answers, and it’s not always obvious which to trust. Finally, approximate functionals suffer from self-interaction error: an electron spuriously repels itself because \\(J[\\rho]\\) includes the interaction of each electron with the total density, including its own contribution. This causes particular problems for localised states and transition-metal chemistry, where the error doesn’t cancel out. "],["mlips.html", "4 Machine-learned interatomic potentials 4.1 Motivation 4.2 How MLIPs work 4.3 The landscape 4.4 Strengths and limitations 4.5 When to use", " 4 Machine-learned interatomic potentials 4.1 Motivation Classical potentials are fast and scale well, but require choosing a functional form and fitting parameters — the accuracy is bounded by these choices. DFT is transferable and does not assume a functional form, but the computational cost limits it to hundreds of atoms and short timescales. Machine-learned interatomic potentials sit between these. Like classical potentials, they are parameterised models fitted to data. But unlike classical potentials, they do not assume a functional form for the interactions — the model learns the shape of \\(E(r)\\) from training data. And the training data typically comes from DFT, so the model inherits DFT’s accuracy for systems similar to those it was trained on. 4.2 How MLIPs work The principle is supervised learning. A dataset of atomic configurations is generated, with energies and forces calculated using DFT. A model — typically a neural network — is then trained to predict \\(E(r)\\) from atomic positions. Once trained, evaluating the model is much cheaper than running DFT, enabling large systems and long simulations. Most MLIPs decompose the total energy into contributions from each atom: \\[E = \\sum_i \\varepsilon_i\\] where \\(\\varepsilon_i\\) depends on the local environment around atom \\(i\\) — the species and positions of nearby atoms within some cutoff. The model learns a mapping from local environment to atomic energy contribution. This locality assumption is physically motivated: in most materials, an atom’s energy contribution depends primarily on its immediate surroundings. It also makes the model size-transferable — a model trained on small cells can be applied to larger systems. There is considerable research into how local environments should be represented (descriptors, symmetry functions) and what model architectures work best (neural networks, Gaussian processes, equivariant networks). These details are beyond our scope. The key point is that MLIPs are flexible models with many parameters, fitted to reproduce DFT energies and forces. 4.3 The landscape The MLIP field is evolving rapidly. Broadly, there are three approaches. Purpose-trained potentials are models developed specifically for a particular system — a battery cathode material, a solid electrolyte, a class of alloys. The training data is generated for that system, covering the configurations relevant to the intended application. This typically gives the best accuracy, because the model is focused on what matters. But it requires substantial effort: generating training data means running many DFT calculations, which is expensive. Training the model and validating it carefully adds further work. This is analogous to developing a classical potential, but without assuming a functional form. Foundation models take a different approach. These are pre-trained on massive datasets spanning much of the periodic table and many structure types, then released for general use. Training such a model requires enormous computational resources — work done by large research groups or companies (Google, Meta, Microsoft) with access to that scale of compute. The result is a model that provides reasonable accuracy out of the box for many systems, without needing to generate training data or train anything. Examples include MACE-MP, CHGNet, SevenNet, and others. Because foundation models have seen diverse training data, they tend to be more transferable across different chemistries. The tradeoff is that accuracy for any specific system may be lower than a purpose-trained model would achieve. Fine-tuning offers a middle path. Starting from a foundation model, additional training data for a specific system is added and training continues. The foundation model provides a starting point — it already captures something about interatomic interactions in general — and fine-tuning adapts it to a particular chemistry. This can give good accuracy with less effort than training from scratch. Which approach was used affects how much the results should be trusted. A purpose-trained potential with careful validation is more reliable than a foundation model applied to a system far from its training data. 4.4 Strengths and limitations MLIPs learn the shape of \\(E(r)\\) from data, rather than assuming a functional form. This avoids the limitations of classical potentials, where accuracy is bounded by the mathematical form chosen. Once trained, evaluation is fast — approaching classical potential speeds for some architectures. The approach is also systematically improvable: more training data generally means a better model. If the potential fails in some region of configuration space, more data can be added there and the model retrained. Foundation models have also made the approach more accessible — reasonable results are possible without training anything. The limitations are significant. The model can only be as good as its training data. Errors in the DFT calculations propagate to the potential, and the model inherits the limitations of whichever functional was used — if PBE gets something wrong, so will the MLIP trained on PBE data. More fundamentally, extrapolation is dangerous. MLIPs interpolate well within their training domain but can fail badly outside it. Unlike DFT, which will give some answer for any configuration, an MLIP may give confident but wrong predictions for configurations far from training data. This failure can be silent — the model does not know it is extrapolating. MLIPs are also less interpretable than classical potentials. A Buckingham potential has parameters with physical meaning — charges, repulsion coefficients. An MLIP has thousands of internal parameters with no direct interpretation. When something goes wrong, diagnosing why is harder. This places a validation burden on the user: the potential must be checked for accuracy on the configurations that matter. 4.5 When to use MLIPs are increasingly the default choice when larger systems or longer timescales than DFT allows are needed, when classical potentials do not exist for the system or are not accurate enough, and when careful validation is feasible. They are the wrong choice when DFT is affordable for the problem (simpler, no training overhead), when the configurations of interest are far from any available training data, or when guaranteed reliability with no possibility of silent failure is required. The field is moving fast. What is state-of-the-art now may be superseded within a year. When evaluating computational work that uses MLIPs, the key questions are: what potential was used, what was it trained on, and how was it validated for the system being studied? "],["finding-minima-geometry-optimisation.html", "5 Finding minima: geometry optimisation 5.1 Why compute a structure? 5.2 Structures and minima 5.3 Steepest descent 5.4 Doing better with curvature 5.5 Local and global minima", " 5 Finding minima: geometry optimisation 5.1 Why compute a structure? Part I covered methods for calculating \\(E(r)\\). We now turn to what we do with that ability — starting with finding stable structures. Geometry optimisation locates minima on the potential energy surface — finding stable structures. But diffraction already tells us where atoms are. Why compute structures at all? Sometimes the goal is direct comparison: validating a structural model from diffraction analysis. If the optimised structure matches the experimental one, both the calculation and the diffraction analysis are supported. If they disagree, something needs investigation — perhaps the experimental model is incomplete, perhaps the computational method is inadequate for this system, perhaps the disagreement points to something interesting. More often, computation provides complementary information. Diffraction tells you what structure forms; computation tells you why. By calculating the energies of competing structures, we can understand polymorph stability, site preferences for dopants, or the driving forces behind structural distortions. We can also calculate energies for structures that don’t form — understanding what’s metastable, what’s unstable, and why. Computation can also resolve what Bragg diffraction struggles with. Some elements have similar scattering lengths — oxygen and fluorine, for instance, are nearly indistinguishable by neutron diffraction, making oxyfluoride structures hard to solve from scattering data alone. Computation can test whether O or F at a given site is energetically preferred. Similarly, partial site occupancies appear as averages in Bragg diffraction, but computation can test the energetics of specific configurations. Local distortions and short-range order present a different challenge. Bragg diffraction reports only the average structure, so local correlations are invisible. Computation can capture these by optimising large supercells with different local arrangements. Finally, there is prediction. Computational screening can assess the stability of hypothetical materials before synthesis — testing whether a proposed composition would be thermodynamically stable, and if so, what structure it would adopt. This is increasingly important for materials discovery. 5.2 Structures and minima When we say a material has a particular “structure”, we typically mean the positions determined by diffraction — the time-averaged positions of atoms at the measurement temperature. Computationally, a structure corresponds to a basin on the potential energy surface — a region of configuration space that drains to a particular minimum. Different basins correspond to different structures: polymorphs of the same composition, different orderings of atoms on sites, different local arrangements. A real material’s PES has many such basins. The simplest description of a basin is the position of its minimum — the 0 K atomic positions, where atoms sit with no thermal motion. In the harmonic approximation, these are also the mean positions at finite temperature; with anharmonicity they can differ, thermal expansion being the most familiar example. The procedure for finding such minima is called geometry optimisation. The force on an atom is the negative gradient of energy: \\[\\mathbf{F}_i = -\\frac{\\partial E}{\\partial \\mathbf{r}_i}\\] At a minimum, all forces are zero. Geometry optimisation adjusts atomic positions (and usually cell parameters) until this condition is satisfied. 5.3 Steepest descent The simplest approach is steepest descent: calculate the forces on all atoms and move each atom in the direction of its force. Since forces point downhill on the potential energy surface, this reduces the energy. Recalculate forces, move again, iterate. Steepest descent tells you which direction to move, but not how far. Too small a step and convergence is slow; too large and you overshoot, oscillate, or diverge. Various approaches exist — fixed step sizes, line searches to find the minimum along each descent direction, adaptive schemes that adjust based on whether the energy decreased — but none is universally optimal. The algorithm also tends to zigzag in narrow valleys of the energy surface, where the steepest direction points across the valley rather than along it towards the minimum. 5.4 Doing better with curvature Newton-Raphson can do better by using more information. Consider first a one-dimensional case: we have \\(E(x)\\) and want to find the minimum. Near any point \\(x_0\\), we can approximate the energy as a Taylor expansion: \\[E(x) \\approx E(x_0) + E&#39;(x_0)(x - x_0) + \\frac{1}{2}E&#39;&#39;(x_0)(x - x_0)^2\\] This expansion is truncated at second order — we assume the energy surface is approximately quadratic near our current point. This is the harmonic approximation, and its validity depends on where we are on the surface. Near a minimum, where the surface is smooth and bowl-shaped, the quadratic approximation is accurate. Far from a minimum, where the surface may have more complex shape, the approximation can be poor. For a quadratic function, we can find the minimum exactly: differentiate, set to zero, solve. The result is: \\[x = x_0 - \\frac{E&#39;(x_0)}{E&#39;&#39;(x_0)}\\] This is the predicted position of the minimum, under the assumption that the PES is locally harmonic. The step to take — gradient divided by curvature — makes physical sense: if the curvature is large (a steep, narrow well), we take a small step; if the curvature is small (a shallow, broad well), we take a large step. The curvature tells us how far away the minimum is likely to be. Because the harmonic assumption is not exact, \\(x\\) won’t be the exact minimum — but it usually provides a good estimate. From the new position we can repeat the procedure: recalculate the gradient and curvature, predict a new minimum position, move there. Each iteration refines the approximation. Near a minimum, where the harmonic approximation is accurate, Newton-Raphson converges rapidly — often in just a few iterations. In three dimensions with N atoms, the same principle applies. The first derivative \\(E^\\prime\\) becomes a gradient vector \\(\\mathbf{g}\\) with 3N components — one for each atomic coordinate. The second derivative E″ becomes the Hessian matrix H, a 3N × 3N matrix of second derivatives: \\[H_{ij} = \\frac{\\partial^2 E}{\\partial r_i \\partial r_j}\\] The quadratic approximation becomes: \\[E(\\mathbf{r}) \\approx E_0 + \\mathbf{g}^T \\mathbf{u} + \\frac{1}{2}\\mathbf{u}^T \\mathbf{H} \\mathbf{u}\\] where u is the displacement from the current position. The Newton-Raphson step generalises to: \\[\\mathbf{u} = -\\mathbf{H}^{-1}\\mathbf{g}\\] The same iterative logic applies: predict the minimum position, move there, repeat until converged. Newton-Raphson performs well when the starting guess is close to a minimum, where the harmonic approximation is reasonable. Far from a minimum, where the harmonic approximation breaks down, the algorithm may converge slowly, take erratic steps, or fail to find a minimum at all. In practice, optimisation codes often use methods that are more robust across the whole energy surface — conjugate gradient methods, or quasi-Newton methods (such as BFGS) that build up an approximate Hessian from the gradient information accumulated over successive steps. These handle the early stages of optimisation more reliably, while still converging rapidly near a minimum where the harmonic approximation holds. 5.5 Local and global minima Geometry optimisation finds a local minimum — the one nearest your starting point. A real potential energy surface has many local minima, corresponding to different polymorphs, different site orderings, different local arrangements of atoms. Which minimum you find depends on where you start. This is usually fine: you’re testing a specific structural hypothesis, not searching for the global minimum. If you want to compare polymorphs, you optimise each one separately and compare their energies. Finding the global minimum without knowing where to start — structure prediction — is a harder problem, but not one we need to solve for most purposes. When it is needed, the approach is conceptually simple: generate many starting points and optimise each one. Methods differ in how they generate those starting points — random structures with sensible constraints, evolutionary algorithms that breed new candidates from successful ones, or perturbations from known minima — but at the core, they all rely on the same local optimisation we’ve just discussed. "],["phonon-calculations.html", "6 Phonon calculations 6.1 Vibrations around equilibrium 6.2 The harmonic approximation 6.3 Periodic systems and phonon dispersion 6.4 Calculating phonons in practice 6.5 What you get 6.6 Limitations", " 6 Phonon calculations 6.1 Vibrations around equilibrium Geometry optimisation finds a minimum on the potential energy surface — a configuration where forces vanish. But atoms don’t sit motionless at these positions. At any temperature above absolute zero, thermal energy causes atoms to vibrate around their equilibrium sites. Even at 0 K, quantum zero-point motion means atoms are never truly stationary, though for our purposes the classical picture of thermal vibrations suffices. This atomic motion is precisely what inelastic neutron scattering probes. When a neutron scatters inelastically from a sample, it exchanges energy with vibrational modes. The measured spectrum S(Q,ω) encodes information about how atoms move — their vibrational frequencies and displacement patterns. Phonon calculations aim to predict this vibrational behaviour. At a minimum, forces vanish but the Hessian — the matrix of second derivatives — remains, encoding how the energy changes when atoms are displaced from equilibrium. This curvature of the potential energy surface approximately determines vibrational behaviour. Stiff bonds (high curvature) give strong restoring forces and high vibrational frequencies. Soft bonds (low curvature) give weak restoring forces and low frequencies. 6.2 The harmonic approximation Near a minimum, the potential energy surface is smooth and bowl-shaped. Expanding the energy in a Taylor series around equilibrium: \\[E = E_0 + \\sum_i \\frac{\\partial E}{\\partial r_i} \\delta r_i + \\frac{1}{2} \\sum_{ij} \\frac{\\partial^2 E}{\\partial r_i \\partial r_j} \\delta r_i \\delta r_j + \\ldots\\] At equilibrium, the first derivatives vanish. If we truncate at second order — the harmonic approximation — the energy becomes quadratic in displacements: \\[E = E_0 + \\frac{1}{2} \\sum_{ij} H_{ij} \\, \\delta r_i \\, \\delta r_j\\] where H is the Hessian matrix. This is the same approximation we used in Newton-Raphson optimisation, but now it serves a different purpose. In optimisation, it was a computational convenience for finding minima efficiently. Here, it makes the vibrational problem tractable — and for many systems, it’s also physically accurate. A quadratic energy means linear restoring forces: F = −Hδr. The equation of motion for atom i is then: \\[m_i \\frac{d^2 \\delta r_i}{dt^2} = -\\sum_j H_{ij} \\delta r_j\\] This is a system of coupled linear differential equations. We seek solutions where all atoms oscillate at the same frequency — normal modes. For a harmonic oscillator, solutions are sinusoidal in time. It is mathematically convenient to write these as complex exponentials δrᵢ(t) = uᵢ exp(iωt), with the understanding that the physical displacement is the real part. Differentiating twice: \\[\\frac{d^2 \\delta r_i}{dt^2} = -\\omega^2 u_i \\exp(i\\omega t)\\] Substituting into the equation of motion, the exp(iωt) factors appear on both sides and cancel, leaving: \\[-m_i \\omega^2 u_i = -\\sum_j H_{ij} u_j\\] This is an eigenvalue problem, though not quite in standard form because of the mass factor. Introducing the mass-weighted dynamical matrix: \\[D_{ij} = \\frac{H_{ij}}{\\sqrt{m_i m_j}}\\] converts this to standard form. Diagonalising D gives eigenvalues ω² and eigenvectors that describe the displacement patterns — which atoms move, in what directions, with what relative amplitudes. Each eigenvector corresponds to a normal mode: a collective motion in which all atoms oscillate at the same frequency, maintaining fixed phase relationships. For a system with N atoms in three dimensions, there are 3N eigenvalues and hence 3N normal modes. Three of these correspond to uniform translation (zero frequency); for periodic solids these become the acoustic modes at q = 0. 6.3 Periodic systems and phonon dispersion The derivation above applies to any collection of atoms, but for a macroscopic crystal we cannot diagonalise a matrix with 3N rows for every atom in the sample. Translational symmetry solves this. A normal mode requires all atoms to oscillate at the same frequency — that’s what makes it a normal mode. But they need not oscillate in phase. In a periodic crystal, translational symmetry constrains which phase relationships are allowed: if the physics is unchanged by shifting the whole crystal by a lattice vector R, the solution must transform consistently under that shift. The functions with well-defined behaviour under discrete translations are plane waves exp(iq·R). Different wavevectors q correspond to different phase relationships between unit cells. At q = 0, exp(iq·R) = 1 for all R, so all cells move in phase. As q increases, the phase difference between adjacent cells grows. At the Brillouin zone boundary, exp(iq·R) = −1 for nearest-neighbour cells — adjacent cells move in antiphase. Beyond the zone boundary, you’re relabelling the same physical modes, which is why reciprocal space is periodic. For each q, the displacement pattern within a unit cell is an eigenvector of the dynamical matrix D(q); the full solution across the crystal is this pattern multiplied by exp(iq·R). The key result — derived in the appendix — is that D(q) has size 3n × 3n, where n is the number of atoms in the primitive cell. Computationally, this is essential: we diagonalise a small matrix at each q, rather than a single impossibly large matrix for the whole crystal. At each q, diagonalisation gives 3n eigenvalues and eigenvectors, corresponding to the 3n branches of the dispersion relation ω(q). The phonon band structure plots these dispersion curves along high-symmetry directions in the Brillouin zone. The phonon density of states integrates over all wavevectors, giving the distribution of frequencies regardless of which q they came from — this is what powder INS measures directly. 6.4 Calculating phonons in practice Phonon calculations require the Hessian — the matrix of second derivatives of energy with respect to atomic positions. Most \\(E(r)\\) methods provide forces (first derivatives) directly, but not second derivatives. However, second derivatives can be constructed from forces numerically. The Hessian element Hᵢⱼ = ∂²E/∂rᵢ∂rⱼ can be written as: \\[H_{ij} = -\\frac{\\partial F_i}{\\partial r_j}\\] where Fᵢ = −∂E/∂rᵢ is the force on coordinate i. This suggests a numerical approach: displace coordinate j by a small amount δr and see how the forces change. The finite displacement method constructs the Hessian by displacing each atom in turn. For each atom and each Cartesian direction, we shift the atom by +δr, calculate the forces on all atoms, then shift by −δr and recalculate. The central difference gives: \\[H_{ij} \\approx -\\frac{F_i(r_j + \\delta r) - F_i(r_j - \\delta r)}{2\\delta r}\\] For a system with n atoms in the primitive cell, this requires 6n force calculations (positive and negative displacements in x, y, z for each atom). Each force calculation gives one column of the Hessian. Symmetry can reduce the number of required calculations. For a periodic system, we work in a supercell. The supercell must be large enough that the displaced atom doesn’t interact significantly with its own periodic images — otherwise the force constants are contaminated by artificial periodicity. The supercell size also determines the q-point sampling: a 2×2×2 supercell gives the dynamical matrix at q-points commensurate with that supercell. Density functional perturbation theory (DFPT) takes a different approach, calculating the response of the electronic structure to atomic displacements analytically within perturbation theory. This avoids the need for supercells and gives phonons at arbitrary q-points directly, but is more complex to implement and only available for certain \\(E(r)\\) methods. In practice, both approaches are implemented in standard codes. The finite displacement method works with any \\(E(r)\\) method that provides forces — DFT, MLIPs, even classical potentials. DFPT is typically used with DFT. 6.5 What you get Phonon calculations provide both eigenvalues (frequencies) and eigenvectors (displacement patterns). Experimentally, INS gives the vibrational spectrum — the density of states \\(g(\\omega)\\) — but extracting mode-specific information about which atoms move and how requires careful analysis, often guided by computation. The calculation provides both quantities directly. The phonon band structure shows dispersion curves along high-symmetry paths, revealing the frequencies of specific modes and any soft modes or instabilities. Single-crystal measurements can probe specific branches, but for complex materials or difficult-to-grow crystals, computation may be the only practical route to the full dispersion. The phonon density of states gives the distribution of vibrational frequencies, summed over all wavevectors and branches — this is what powder INS measures directly. From the phonon frequencies, thermodynamic quantities follow from standard statistical mechanics. The vibrational free energy: \\[F_{\\text{vib}} = \\sum_{\\mathbf{q},\\nu} \\left[ \\frac{\\hbar\\omega_{\\mathbf{q}\\nu}}{2} + k_B T \\ln\\left(1 - e^{-\\hbar\\omega_{\\mathbf{q}\\nu}/k_B T}\\right) \\right]\\] where the sum runs over wavevectors \\(q\\) and branches \\(\\nu\\). From this, heat capacity and vibrational entropy follow by differentiation. These quantities matter for phase stability at finite temperature — a phase with lower energy may not be stable if another phase has higher entropy. 6.6 Limitations The harmonic approximation assumes small displacements around a well-defined minimum. This breaks down in several situations. At high temperatures, atomic displacements become large and the quadratic approximation fails. Anharmonic terms in the potential — the higher-order terms we truncated — become significant. This leads to effects like phonon-phonon scattering, finite thermal conductivity, and temperature-dependent phonon frequencies. This is true anharmonicity: atoms still oscillate around equilibrium positions, but the potential well is not quadratic. When the harmonic approximation fails in this sense, one option stays within the lattice dynamics framework but includes higher-order terms. Recall that the harmonic approximation truncates the Taylor expansion at second order. Including third and fourth order terms: \\[E = E_0 + \\frac{1}{2}\\sum_{ij} H_{ij} \\delta r_i \\delta r_j + \\frac{1}{6}\\sum_{ijk} \\Phi_{ijk} \\delta r_i \\delta r_j \\delta r_k + \\frac{1}{24}\\sum_{ijkl} \\Phi_{ijkl} \\delta r_i \\delta r_j \\delta r_k \\delta r_l + \\ldots\\] The third-order terms $_{ijk} describe three-phonon processes — one phonon decaying into two, or two combining into one. These are responsible for finite thermal conductivity. The fourth-order terms describe four-phonon processes. The problem is that while the Hessian has n2 elements (for n atomic coordinates), the third-order tensor has n3 elements and the fourth-order tensor has n4. More fundamentally, the harmonic problem gives 3N independent modes. Including anharmonicity couples these modes to each other: you need to consider how each mode interacts with every other mode (for three-phonon) or every pair of modes (for four-phonon). The number of terms grows combinatorially, making these calculations substantially more expensive. Some systems go beyond anharmonicity — the picture of atoms oscillating around fixed equilibrium positions breaks down entirely. Superionic conductors, where ions diffuse rapidly through a solid framework, cannot be described by vibrations around fixed sites. Soft modes that approach zero frequency signal structural instabilities. Phase transitions involve large-amplitude motion between different structural basins. For these, no Taylor expansion around a single minimum will suffice. "],["molecular-dynamics-principles.html", "7 Molecular dynamics: principles 7.1 From local curvature to global exploration 7.2 Equations of motion 7.3 Numerical integration 7.4 Phase space and trajectories 7.5 Ensembles and what they mean 7.6 Thermostats and barostats 7.7 What you get 7.8 The \\(E(r)\\) choice", " 7 Molecular dynamics: principles 7.1 From local curvature to global exploration Geometry optimisation finds minima on the potential energy surface. Phonon calculations characterise the curvature at those minima — how the energy changes for small displacements. Both assume atoms remain close to their equilibrium positions: optimisation iterates toward a minimum, and the harmonic approximation expands the energy around it. At finite temperature, this picture becomes incomplete. Atoms have kinetic energy and explore the potential energy surface. For mildly anharmonic systems, we can extend lattice dynamics with higher-order terms — but as we saw, this becomes expensive. For systems where atoms don’t remain near any single minimum — diffusing ions, phase transitions, liquids — no Taylor expansion around one point will suffice. Molecular dynamics takes a different approach. Rather than expanding \\(E(r)\\) and truncating, we follow the actual motion of atoms over the potential energy surface. Given positions and velocities at some instant, we calculate the forces on all atoms, use these to update the velocities and positions a short time later, and repeat. The result is a trajectory: a sequence of configurations showing how the system evolves in time. 7.2 Equations of motion The force on atom i is the negative gradient of the potential energy: \\[\\mathbf{F}_i = -\\nabla_i E(\\mathbf{r})\\] Given the force, Newton’s second law gives the acceleration: \\[m_i \\frac{d^2 \\mathbf{r}_i}{dt^2} = \\mathbf{F}_i\\] The motion of our system is described by two coupled differential equations: \\[\\frac{d\\mathbf{r}}{dt} = \\mathbf{v}\\] \\[\\frac{d\\mathbf{v}}{dt} = \\mathbf{a} = \\frac{\\mathbf{F}}{m}\\] If we could integrate these equations exactly, we could predict the positions and velocities at any future time. But the forces depend on positions, which change as the system evolves — for any realistic system, we cannot solve these equations analytically. 7.3 Numerical integration Instead, we integrate numerically. The principle is to approximate the continuous equations over small time intervals. If we assume the acceleration is roughly constant over a short time Δt, we can write: \\[\\mathbf{v}(t + \\Delta t) \\approx \\mathbf{v}(t) + \\mathbf{a}(t) \\Delta t\\] \\[\\mathbf{r}(t + \\Delta t) \\approx \\mathbf{r}(t) + \\mathbf{v}(t) \\Delta t + \\frac{1}{2}\\mathbf{a}(t) \\Delta t^2\\] This is Euler integration: use the current acceleration and velocity to predict the state at the next timestep, then repeat. The approximation becomes exact as Δt → 0, but practical calculations require finite timesteps. Euler integration has problems. First, it introduces systematic errors at each step — small discrepancies between the predicted trajectory and the true one. These errors accumulate over many steps, causing the total energy to drift rather than remaining constant as it should for an isolated system. Second, Euler integration is not time-reversible. Newtonian mechanics is symmetric under time reversal: if we reverse all velocities, the system should retrace its path. Euler’s method breaks this symmetry because it uses information only from the beginning of each timestep. A forward step followed by reversing velocities and stepping again does not return to the starting point. The velocity Verlet algorithm, used in most practical MD codes, avoids these problems. It updates positions using both the current and next accelerations: \\[\\mathbf{r}(t + \\Delta t) = \\mathbf{r}(t) + \\mathbf{v}(t) \\Delta t + \\frac{1}{2}\\mathbf{a}(t) \\Delta t^2\\] \\[\\mathbf{v}(t + \\Delta t) = \\mathbf{v}(t) + \\frac{1}{2}[\\mathbf{a}(t) + \\mathbf{a}(t + \\Delta t)] \\Delta t\\] The velocity update uses the average of the old and new accelerations, making the algorithm time-reversible. Energy is not conserved exactly, but fluctuates around the correct value without systematic drift. The timestep must be small enough to capture the fastest motion in the system — typically the highest-frequency vibrations — which means Δt is usually around 1 femtosecond. A nanosecond trajectory therefore requires 10⁶ timesteps, each requiring a force evaluation. 7.4 Phase space and trajectories The state of a classical system is specified by the positions and momenta of all atoms — a point in phase space. As we integrate the equations of motion, this point traces out a trajectory through phase space. For an isolated system (no external heat bath), the total energy E = K + V is conserved — kinetic energy converts to potential and back, but the sum remains constant. The trajectory is confined to a surface of constant energy in phase space. This is the microcanonical ensemble: all accessible states have the same total energy. The temperature in such a system is related to the average kinetic energy: \\[\\langle K \\rangle = \\frac{3}{2} N k_B T\\] where N is the number of atoms. This follows from the equipartition theorem: each quadratic degree of freedom contributes ½kT to the average energy, and kinetic energy is quadratic in the velocities. 7.5 Ensembles and what they mean Real experiments are usually not performed at constant energy. A sample in a furnace exchanges heat with its surroundings; it has a well-defined temperature, not a fixed total energy. Different experimental conditions correspond to different statistical ensembles. The microcanonical ensemble (NVE) has fixed particle number N, volume V, and energy E. This is what unmodified Newtonian dynamics samples. It describes an isolated system. The canonical ensemble (NVT) has fixed N, V, and temperature T. The system exchanges energy with a heat bath at temperature T. States are weighted by the Boltzmann factor exp(−E/kT): lower-energy configurations are more probable, but higher-energy configurations are accessible with probability that decreases exponentially. The isothermal-isobaric ensemble (NPT) has fixed N, pressure P, and temperature T. The system exchanges both energy and volume with its surroundings. This corresponds to most laboratory conditions — a sample at ambient pressure and controlled temperature. Which ensemble we simulate determines what statistical distribution we sample. This matters for calculating thermodynamic properties and for comparing with experiment. 7.6 Thermostats and barostats To simulate NVT or NPT, we need to modify the equations of motion so that the trajectory samples the correct statistical distribution. A thermostat couples the system to a heat bath; a barostat couples it to a pressure reservoir. The details of how thermostats work are technical, but the key point is conceptual: a good thermostat is designed so that, over a long trajectory, configurations are visited with the correct Boltzmann weights. The time-average of any property over the trajectory then equals the ensemble average — the thermodynamic expectation value. This is the ergodic hypothesis in practice: for a sufficiently long trajectory, the time spent in each region of phase space is proportional to its statistical weight. We can therefore compute ensemble averages — the quantities that connect to thermodynamics and to time-averaged experimental measurements — from a single long trajectory. 7.7 What you get A molecular dynamics simulation produces a trajectory: atomic positions (and velocities) at each timestep. From this trajectory, we can calculate: Equilibrium properties — averages over the trajectory. The mean potential energy, the average structure, the distribution of atomic positions. These are ensemble averages, directly comparable to time-averaged experimental measurements. Dynamic properties — how the system evolves in time. How do atomic positions correlate between different times? How quickly do correlations decay? These time correlation functions contain information about dynamics that equilibrium measurements cannot access. Transport properties — diffusion coefficients, viscosities, thermal conductivities. These emerge from how quickly the system loses memory of its initial state. 7.8 The \\(E(r)\\) choice Every MD timestep requires calculating forces — one evaluation of \\(E(r)\\) and its gradient. A typical simulation might run for millions of timesteps. The cost of \\(E(r)\\) therefore determines what simulations are feasible. Classical potentials are cheap to evaluate. Simulations of millions of atoms for nanoseconds to microseconds are routine. The limitation is accuracy: the functional form constrains what physics can be captured. Ab initio MD (AIMD) calculates forces from DFT at each timestep. This is accurate but expensive — practical for hundreds of atoms and picoseconds. Many dynamical processes happen on longer timescales or require larger system sizes. Machine-learned interatomic potentials approach DFT accuracy at near-classical cost. They are increasingly the method of choice when classical potentials aren’t accurate enough but AIMD is too expensive. The caveats discussed in Chapter @ref{mlips} apply: MLIPs inherit limitations from their training data, and can fail outside their training domain. The choice is problem-dependent. For a quick estimate of dynamics in a well-characterised material, classical potentials may suffice. For quantitative comparison with experiment in a system where accuracy matters, MLIPs or AIMD may be necessary. "],["monte-carlo-sampling.html", "8 Monte Carlo sampling 8.1 Beyond molecular dynamics 8.2 The goal: ensemble averages 8.3 Why not sample uniformly? 8.4 Markov chain Monte Carlo 8.5 The Metropolis algorithm 8.6 What you get 8.7 What you don’t get 8.8 Neutron connection", " 8 Monte Carlo sampling 8.1 Beyond molecular dynamics MD samples configurations by following the physical dynamics of the system. At finite temperature, atoms have kinetic energy and explore the potential energy surface. For fast processes — ionic diffusion in superionic conductors, thermal vibrations — MD captures the relevant configurations. But some configurational questions involve rearrangements that are far too slow for MD. Consider cation ordering in a mixed-metal oxide, or site occupancies in an alloy. The atoms can, in principle, rearrange — but the mechanisms are slow, involving vacancy diffusion or other activated processes that happen on timescales of seconds to hours, not nanoseconds. MD will never visit these configurations; the simulation is trapped in whatever arrangement it started with. Monte Carlo sampling takes a different approach. Rather than following dynamics, we propose configurational changes directly and accept or reject them based on energetics. This lets us sample configurations that MD cannot reach. 8.2 The goal: ensemble averages Statistical mechanics tells us that macroscopic properties are ensemble averages. The expectation value of a property A in the canonical ensemble is: \\[\\langle A \\rangle = \\sum_i A_i P_i = \\frac{1}{Z} \\sum_i A_i \\exp(-E_i / k_B T)\\] where the sum runs over all microstates i, Ei is the energy of state i, and Z is the partition function: \\[Z = \\sum_i \\exp(-E_i / k_B T)\\] In principle, this is straightforward: enumerate all states, calculate their energies and properties, weight by Boltzmann factors, sum. In practice, the number of states is astronomically large. For configurational disorder on N sites with two possible occupants per site, there are 2N configurations. Even for modest N, explicit enumeration is impossible. 8.3 Why not sample uniformly? One idea: sample configurations randomly and weight by their Boltzmann factors. The problem is that uniform sampling is inefficient. Most randomly chosen configurations have high energy and negligible Boltzmann weight — we waste effort sampling configurations that contribute almost nothing to the average. The solution is importance sampling: sample configurations in proportion to their Boltzmann weights. Then each sample contributes equally to the average: \\[\\langle A \\rangle \\approx \\frac{1}{M} \\sum_{m=1}^{M} A_m\\] No explicit weights needed. The question is how to generate samples from this distribution. 8.4 Markov chain Monte Carlo The solution is to construct a Markov chain — a random walk through configuration space where each step depends only on the current configuration. At each step we propose a move to a new configuration and either accept or reject it. The question is: how should we choose the acceptance probabilities to ensure we sample from our target distribution? Consider two states. At equilibrium, the frequency of transitions from state 1 to state 2 must equal the frequency from 2 to 1 — otherwise probability would accumulate in one state. If we visit state 1 with probability π1 and accept moves to state 2 with probability P(1→2), then balance requires: \\[\\pi_1 \\, P(1 \\to 2) = \\pi_2 \\, P(2 \\to 1)\\] Rearranging: the ratio of acceptance probabilities must equal the ratio of target probabilities: \\[\\frac{P(1 \\to 2)}{P(2 \\to 1)} = \\frac{\\pi_2}{\\pi_1}\\] For many states, we impose this constraint on every pair. The result is that we sample each state with probability proportional to our target distribution. For the Boltzmann distribution, the ratio of target probabilities is: \\[\\frac{\\pi_2}{\\pi_1} = \\frac{e^{-E_2/kT}}{e^{-E_1/kT}} = e^{-\\Delta E/kT}\\] This depends only on the energy difference between the two states — not on absolute energies. Since we only ever compare states pairwise, we never need to know the partition function or evaluate absolute energies. Any acceptance rule where the ratio of forward to backward acceptance probabilities equals \\(\\exp(-\\Delta E/kT)\\) will sample from the Boltzmann distribution. The Metropolis criterion is one such choice. 8.5 The Metropolis algorithm The Metropolis algorithm is the standard approach. The procedure is: Start from some configuration with energy E1 Propose a move — for configurational disorder, typically swapping two atoms of different types Calculate the energy E2 of the new configuration Accept or reject the move: If ΔE ≤ 0 (energy decreased): accept If ΔE &gt; 0 (energy increased): accept with probability \\(\\exp(-\\Delta E/kT)\\) If accepted, the new configuration becomes the current one; if rejected, stay in the old configuration Repeat from step 2 The acceptance rule ensures that, after many steps, the chain visits configurations with probability proportional to \\(\\exp(-E/kT)\\). The intuition for why the algorithm works: downhill moves are always accepted, so the chain readily finds low-energy configurations. Uphill moves are sometimes accepted — the probability \\(\\exp(-\\Delta E/kT)\\) decreases exponentially with the energy cost, but is never zero. This allows the chain to escape local minima and explore configuration space. Without occasional uphill moves, we would simply find the nearest local minimum — an optimisation algorithm, not a sampling algorithm. Temperature controls exploration: at high temperature, \\(\\exp(-\\Delta E/kT)\\) is close to 1 for moderate ΔE, so most moves are accepted and the system explores widely; at low temperature, only small uphill moves are accepted and the system stays near low-energy configurations. 8.6 What you get After an initial equilibration period, the MC chain samples configurations from the target ensemble. The expectation value of any quantity A is simply the average over sampled configurations: \\[\\langle A \\rangle = \\frac{1}{M} \\sum_{m=1}^{M} A_m\\] No Boltzmann weights appear because the sampling already accounts for them. Thermodynamic properties follow: from fluctuations in energy we get heat capacity; from the temperature dependence of order parameters we identify phase transitions. Snapshots from the MC chain represent the distribution of configurations at thermal equilibrium, and can be analysed for structural features or compared with experiment. 8.7 What you don’t get MC does not give dynamics. The sequence of configurations is a random walk designed to sample the equilibrium distribution, not a physical trajectory through time. There is no “MC time” that corresponds to real time. Questions about rates, diffusion coefficients, or time correlation functions cannot be answered by MC. This is the fundamental distinction: MD samples by following dynamics and gives both equilibrium and dynamic properties; MC samples configuration space directly and gives only equilibrium properties. 8.8 Neutron connection Neutron diffraction measures a spatial average — the scattering from many unit cells within the illuminated volume. For crystallographically disordered systems where atoms are not diffusing, this spatial average determines the measured structure. MC models this as an average over configurations. If the system is statistically homogeneous — if any region looks like any other on average — then the spatial average equals the ensemble average over configurations. "],["cluster-expansion.html", "9 Cluster expansion 9.1 The problem 9.2 The idea 9.3 Symmetry and truncation 9.4 Fitting the expansion 9.5 What you get 9.6 Limitations", " 9 Cluster expansion 9.1 The problem Some questions in materials science concern configurational disorder on a lattice: which atoms sit on which sites? In a mixed-metal oxide, how are the cations distributed? In an alloy, do the components cluster or order? These are questions about a discrete configurational space — the lattice sites are fixed, only the occupancies vary. Monte Carlo sampling handles this naturally: propose a swap of two atoms, accept or reject based on energy change, repeat. The challenge is evaluating the energy. A typical MC simulation involves millions of proposed moves, each requiring ΔE to be calculated. To make matters worse, atoms do not sit exactly on ideal lattice positions — they relax slightly depending on the local chemical environment. To calculate ΔE properly, each proposed configuration requires a geometry optimisation. The computational cost is not just millions of energy evaluations, but millions of geometry optimisations. With DFT, this is completely impractical. With classical potentials, it is possible but computationally demanding and slow. Cluster expansions sidestep this entirely. The idea is to construct an effective Hamiltonian that depends only on site occupancies — not on continuous atomic positions. This Hamiltonian is parameterised by fitting to the energies of relaxed structures calculated with a more accurate method, typically DFT. Because the training data comes from fully relaxed configurations, the relaxation energy is already incorporated into the fitted coefficients. During MC sampling, no geometry optimisation is needed — evaluating the energy for any configuration is computationally trivial, enabling hundreds of MC steps per second with near-DFT accuracy. 9.2 The idea How is the effective Hamiltonian constructed? For a binary system (A and B atoms on fixed sites), the configuration can be represented using occupation variables: assign σi = +1 if site i has atom A, σi = −1 if site i has atom B. The full configuration is then specified by the set of all σi. The cluster expansion writes the energy as a function of these occupation variables: \\[E(\\{\\sigma\\}) = J_0 + \\sum_i J_i \\sigma_i + \\sum_{i&lt;j} J_{ij} \\sigma_i \\sigma_j + \\sum_{i&lt;j&lt;k} J_{ijk} \\sigma_i \\sigma_j \\sigma_k + \\ldots\\] The first term J0 is a constant (the reference energy). The second sum runs over single sites — but for a stoichiometric system, the number of A and B atoms is fixed, so Σσi is constant and this term just shifts the reference. The third sum runs over pairs of sites; Jij captures how the energy depends on whether sites i and j have the same or different atoms. Higher-order terms capture three-body, four-body, and larger cluster interactions. 9.3 Symmetry and truncation For a periodic crystal, symmetry constrains the expansion. All pairs related by symmetry have the same coefficient: nearest-neighbour pairs share one J value, second-nearest-neighbour pairs share another, and so on. Similarly for triplets and larger clusters. Symmetry groups the clusters into types, reducing the number of independent parameters from one per cluster to one per symmetry-distinct cluster type. The expansion becomes: \\[E = J_0 + \\sum_\\alpha J_\\alpha \\, m_\\alpha \\, \\langle \\Pi_\\alpha \\rangle\\] where α labels cluster types, mα is the multiplicity (how many clusters of that type per site), and ⟨Πα⟩ is the average value of the product of occupation variables over all clusters of type α. An infinite cluster expansion — including all cluster types — is provably exact; it can represent any function of the site occupancies. In practice, the expansion is truncated by selecting which cluster types to include. This is done by limiting both the cluster size (e.g., up to triplets or quartets) and the cluster radius (e.g., pairs only up to third-nearest neighbours). 9.4 Fitting the expansion The coefficients {Jα} are unknown. They are determined by fitting to DFT calculations: a set of training configurations is generated, covering different arrangements of atoms on the lattice, and the DFT energy of each is calculated. The cluster expansion coefficients are then fitted to reproduce these energies. Because the energy is linear in the coefficients Jα, this is a linear least-squares problem. The training set typically includes 50–200 configurations, each requiring a DFT calculation. This is expensive upfront, but pays off when the fitted expansion enables millions of energy evaluations that would be impossible with direct DFT. 9.5 What you get A fitted cluster expansion evaluates E(σ) for any configuration as a sum over clusters — a fraction of a second, rather than the minutes to hours required for DFT. This makes extensive MC sampling possible: millions of steps at effectively DFT accuracy. We can map out phase diagrams, find order-disorder transition temperatures, and characterise the distribution of configurations at any temperature. Ground state searching becomes tractable — systematically enumerating configurations or using optimisation algorithms to find the lowest-energy orderings. Thermodynamic properties such as heat capacities, ordering enthalpies, and entropies follow from MC sampling. 9.6 Limitations Cluster expansions require that every relaxed structure can be mapped back to exactly one on-lattice configuration. The atoms in the training structures are fully relaxed — they sit at their true equilibrium positions, not ideal lattice sites — but each structure must correspond unambiguously to a particular set of site occupancies. This works well when relaxations are small, but breaks down if atoms move so far from their nominal sites that the mapping becomes ambiguous. The expansion assumes the energy can be written as a sum over clusters. This works well for many systems but may fail when long-range interactions (beyond the clusters included) are important, or when the electronic structure changes qualitatively with ordering. The quality depends entirely on the training data and the choice of clusters. A cluster expansion is a fit to DFT — it inherits the accuracy of the underlying DFT calculations and can only interpolate, not extrapolate. If the training set doesn’t include configurations similar to what you’re asking about, the expansion may give wrong answers. "],["short-range-order-and-pdf.html", "10 Short-range order and PDF 10.1 Long-range order vs short-range order 10.2 What Bragg diffraction misses 10.3 PDF captures local structure 10.4 The computational approach", " 10 Short-range order and PDF 10.1 Long-range order vs short-range order Between fully ordered and maximally disordered (maximum entropy) lies an intermediate regime: the system is disordered in the sense that there is no long-range periodic pattern, but local correlations exist. This is short-range order. Consider a binary alloy A0.5B0.5 on a BCC lattice. At low temperature, it might order into a CsCl-type structure: all A atoms on one sublattice, all B on the other. This is long-range order (LRO) — correlations extend throughout the crystal, producing a periodic pattern. Bragg diffraction sees this clearly: the ordering produces superlattice reflections at positions forbidden by the disordered structure. At high temperature, entropy wins and the alloy disorders: A and B atoms distributed over sites with no correlations between them. No long-range pattern, no superlattice reflections. This is maximum entropy — the arrangement is completely uncorrelated. But there is an intermediate regime. Even without long-range order, local correlations may exist — short-range order (SRO). Perhaps A atoms preferentially have B neighbours, or tend to avoid other A atoms, but only over the first few coordination shells. Beyond that, correlations decay and the arrangement becomes uncorrelated. There’s no periodic pattern, but there is local structure. 10.2 What Bragg diffraction misses Bragg diffraction is sensitive to the average, periodic structure. For a disordered alloy without LRO, the pattern shows only the underlying lattice with average scattering length. The local correlations show up as diffuse scattering between Bragg peaks — broad features that are often ignored or hard to analyse quantitatively. Consider what diffraction tells you about site occupancies. If a site is 50% A and 50% B on average, that’s what the measurement reports — regardless of whether A and B are randomly distributed or have strong local correlations. Two structures with the same average occupancy but different SRO give the same Bragg intensities. This is a fundamental limitation: Bragg diffraction reports the spatially averaged structure. Local correlations that don’t produce a periodic pattern are invisible to it. 10.3 PDF captures local structure Total scattering includes both Bragg and diffuse contributions. The Fourier transform of the total scattering gives the pair distribution function G(r), which measures the probability of finding atoms at separation r. Unlike Bragg diffraction, which sees only the average periodic structure, PDF is sensitive to local correlations. For a single-element material, g(r) has peaks at the neighbour distances — first shell, second shell, and so on. For a multi-component material, partial pair distribution functions gαβ(r) can be defined: the probability of finding a β atom at distance r from an α atom. These partials encode information about local ordering. If A atoms preferentially have B neighbours, the A-B partial shows enhanced correlations at the nearest-neighbour distance compared to an uncorrelated distribution. If A atoms cluster together, the A-A partial is enhanced instead. When short-range order is present, it shows up in the PDF: the short-range region is not well described by the model fitted to Bragg data. But while PDF can reveal that local structure differs from the long-range average, understanding what is actually happening is harder. The measured PDF sums over all partials, weighted by scattering lengths and concentrations — separating the contributions requires additional information. Even if correlations can be quantified, this does not explain them: we may learn that A prefers B neighbours, but not why. And many different local arrangements can produce similar g(r), so determining the structure from PDF alone requires additional constraints or physical insight. 10.4 The computational approach This is where cluster expansion and Monte Carlo methods help. CE+MC samples configurations at thermal equilibrium, weighted by their Boltzmann probabilities. This is not just generating plausible structures — it’s sampling from the correct statistical distribution. From these equilibrium configurations we can calculate ensemble-averaged properties: the probability that an A atom has a B neighbour, how correlations decay with distance, which local arrangements are favoured and which are suppressed. Comparing with experimental PDF requires an additional step. The CE operates on-lattice, but real atoms relax from ideal positions. To generate a predicted g(r), we select representative configurations from the MC sampling, relax each using DFT (or another accurate method), and average g(r) over these relaxed structures. This ensemble-averaged g(r) can then be compared directly with experiment. Alternatively, the relaxed structures can serve as starting models for PDF refinement. Rather than comparing computed and experimental g(r) directly, the DFT-relaxed configurations provide physically motivated structural models that experimentalists can refine against their data. If experiment and computation agree, we have a validated atomistic model. We can then interrogate that model for insight that experiment alone cannot provide: why certain correlations exist, what drives the local ordering, how it changes with temperature or composition. "],["case-study-short-range-order-in-li2feso.html", "11 Case study: short-range order in Li2FeSO 11.1 The puzzle 11.2 The computational approach 11.3 The result: preferential short-range order 11.4 Validation against experiment 11.5 The physical origin: anion polarisation 11.6 What this example illustrates", " 11 Case study: short-range order in Li2FeSO This chapter presents a case study illustrating the computational workflow developed in the preceding chapters. The example — cation ordering in the antiperovskite cathode material Li2FeSO — demonstrates how cluster expansion, Monte Carlo sampling, and pair distribution function analysis combine to characterise short-range order that Bragg diffraction cannot detect. Full details are available in Coles et al., Journal of Materials Chemistry A (2023). 11.1 The puzzle Li2FeSO adopts the antiperovskite structure. Sulfur occupies the 12-coordinate site, oxygen occupies the 6-coordinate octahedral site, and lithium and iron share the remaining sites in a 2:1 ratio (Fig. X). Previous diffraction studies found no superlattice reflections — no evidence of long-range cation order — leading to the assumption that Li and Fe are randomly distributed over their shared sites. [Figure: Li2FeSO antiperovskite structure showing the shared Li/Fe sites] But this assumption is puzzling on physical grounds. Fe2+ and Li+ have different formal charges. Simple electrostatic arguments suggest that configurations which separate the higher-charged Fe2+ ions should be lower in energy than configurations that cluster them together. If an energetic preference exists, why would the cation distribution be random? The absence of long-range order does not necessarily imply the absence of short-range order. Diffraction probes the average, periodic structure; local correlations that do not produce a periodic pattern are invisible to Bragg peaks. The question is whether Li2FeSO exhibits preferential short-range cation ordering, and if so, what form it takes. 11.2 The computational approach To address this question, we calculated DFT energies for approximately 100 different Li/Fe configurations in supercells of Li2FeSO. These energies were used to fit a cluster expansion — a parameterised model that expresses the configurational energy as a function of site occupancies, as described in Chapter 92. With the cluster expansion in hand, Monte Carlo sampling becomes computationally tractable. We performed MC simulations in an 8×8×8 supercell at 1025 K, corresponding to the experimental synthesis temperature. The simulation generates an ensemble of configurations representative of thermal equilibrium at that temperature. From the MC trajectory, we can extract the probability distribution of local coordination environments. Each oxygen in the structure is surrounded by six cation sites. In a material with 2:1 Li:Fe stoichiometry, the possible oxygen coordinations range from OLi6 (all lithium) to OFe6 (all iron), with intermediate compositions OLi₅Fe, OLi4Fe2, OLi3Fe3, OLi2Fe4, and OLiFe5. For OLi4Fe2 coordination — four lithium and two iron around the central oxygen — there are two distinct geometric arrangements. In cis-OLi4Fe2, the two iron atoms occupy adjacent vertices of the octahedron. In trans-OLi4Fe2, they occupy opposite vertices. [Figure: Schematic of cis versus trans OLi4Fe2 coordination environments] 11.3 The result: preferential short-range order The Monte Carlo simulations predict that the cation distribution is not random. For a random arrangement of Li and Fe over the available sites, the probability of each oxygen coordination environment follows a binomial distribution. The most probable coordination would be OLi4Fe2, occurring in approximately 33% of oxygen environments. The DFT-derived cluster expansion predicts a markedly different distribution. OLi4Fe2 coordination accounts for 65% of oxygen environments — roughly twice the random expectation. This enhanced preference for OLi4Fe2 is consistent with Pauling’s second rule: coordination environments that achieve local electroneutrality are energetically favoured. [Figure: Oxygen coordination probability distributions comparing DFT-CE model, random distribution, and point-charge model] More unexpectedly, within the OLi4Fe2 environments, 81% adopt the cis configuration and only 19% adopt the trans configuration. This result contradicts simple electrostatic reasoning. If Li+ and Fe2+ are treated as point charges at their formal crystallographic positions, the electrostatic energy of an OLi4Fe2 octahedron is minimised when the two Fe2+ ions occupy opposite vertices — the trans configuration — maximising their separation. A point-charge model predicts preferential trans-OLi4Fe2 coordination and, consequently, long-range cation order. The DFT-derived model predicts preferential cis-OLi4Fe2 coordination, which produces long-range disorder. 11.4 Validation against experiment The PDF provides a direct test of these competing structural models. From the Monte Carlo configurations, we can calculate the pair distribution function and compare it to experimental total scattering data. This particular study used X-ray total scattering, which is most sensitive to the heavier Fe and S atoms; neutron PDF on the same system would provide complementary sensitivity to the lithium correlations. The principles of the comparison — generating configurations computationally and comparing calculated g(r) to experiment — are identical regardless of the probe. Three models were tested. The DFT-derived cluster expansion model, with its preference for cis-OLi4Fe2 coordination, gives the best agreement with experiment (\\(R_\\mathrm{w} = 14.0%\\)). A random Li/Fe distribution gives poorer agreement (\\(R_\\mathrm{w} = 16.5%\\)) — the difference is modest but systematic, reflecting the failure to capture the enhanced OLi4Fe2 coordination that the DFT model predicts. Most tellingly, a point-charge ground state with 100% trans-OLi4Fe2 coordination gives substantially worse agreement (\\(R_\\mathrm{w} = 35.2%\\)). The structure that simple electrostatics predicts is incompatible with the experimental PDF at short range. [Figure: Comparison of experimental and simulated PDFs for the three models] The PDF comparison validates the DFT-derived model. The short-range order in Li2FeSO is real, and it takes the form of preferential cis-OLi4Fe2 coordination — not the trans preference that point-charge electrostatics would suggest. 11.5 The physical origin: anion polarisation Why does DFT favour cis over trans, when simple electrostatics predicts the opposite? The key lies in the symmetry of the anion coordination environment. In trans-OLi4Fe2 coordination, the oxygen sits at a centre of symmetry — the distribution of cations around it is centrosymmetric. In cis-OLi4Fe2 coordination, the oxygen sits in a non-centrosymmetric environment. Anions in non-centrosymmetric environments experience an asymmetric electric field from the surrounding cations. This field induces electronic polarisation — the anion’s electron density shifts in response. The induced dipole lowers the total electrostatic energy of the system. Point charges cannot polarise. A model that treats ions as fixed charges at crystallographic positions misses this physics entirely. It correctly captures the preference for OLi4Fe2 coordination (local electroneutrality), but incorrectly predicts trans over cis because it cannot account for the stabilisation of polar coordination environments through anion polarisation. Analysis of the DFT calculations confirms this picture. In structures with cis-OLi4Fe2 coordination, the oxygen anions exhibit significant electronic dipole moments. In structures with trans-OLi4Fe2 coordination, the oxygen dipole moments are zero by symmetry. The polarisation energy tips the balance from trans to cis. 11.6 What this example illustrates This case study demonstrates the interplay between experiment and computation that runs through the preceding sections. Bragg diffraction correctly reported no long-range order, but could not distinguish between a truly random cation distribution and one with short-range order that lacks long-range periodicity. The combination of DFT, cluster expansion, and Monte Carlo sampling predicted a specific form of short-range order — preferential cis-OLi4Fe2 coordination — and the PDF comparison validated this prediction against experiment. Further analysis then revealed the physical origin of this preference: anion polarisation in non-centrosymmetric coordination environments, physics that the experiment could not directly access but that computation reveals. The relationship was bidirectional throughout. The experimental observation of “no long-range order” prompted the computational investigation. The computational result was validated against experimental PDF data. And the physical insight emerged from analysing why the DFT results differed from simple electrostatic expectations. For this material, “disordered” does not mean “random.” The distinction matters: short-range cation order in cathode materials affects lithium transport, redox behaviour, and electrochemical performance. Characterising that order required the combination of total scattering and computation that this case study illustrates. "],["conclusion.html", "Conclusion 11.7 Four themes 11.8 What now?", " Conclusion These notes have covered substantial ground: methods for calculating$ \\(E(r)\\), techniques for using those calculations to answer scientific questions, and approaches for treating configurational disorder. Before closing, it is worth drawing out the threads that run through this material. 11.7 Four themes Neutrons and computation probe the same physics. This is not a loose analogy. Phonon densities of states, dynamic structure factors, pair distribution functions — these quantities can be obtained from either neutron experiments or computation, and the results can be compared directly. When a modeller calculates \\(S(Q,\\omega)\\) from molecular dynamics and an experimentalist measures \\(S(Q,\\omega)\\) with QENS, the comparison is quantitative: the curves can be overlaid, the agreement or disagreement assessed. This shared language exists because neutrons scatter from nuclei, and the computational methods covered in these notes model nuclear positions and motion. Both approaches interrogate the same underlying physics from different directions. Method choice depends on the question. There is no universally best computational approach. Classical potentials are fast but limited by their functional form. DFT is transferable but expensive. Machine-learned potentials offer a middle path but require careful validation. Geometry optimisation finds static structures; molecular dynamics captures finite-temperature motion; Monte Carlo samples configuration space without following dynamics. The choice of \\(E(r)\\) method and the choice of what to do with it are independent decisions, both determined by what you want to learn. A quick survey of dynamics in a well-characterised material might use classical potentials; a quantitative comparison of competing structures might require DFT; extensive sampling of configurational disorder might need cluster expansion. Recognising which tool fits which problem is more important than knowing the technical details of any single method. Structural information becomes increasingly subtle. Bragg diffraction determines average, periodic structure — the positions that atoms occupy on average, with the periodicity of the crystal. This is the starting point, but not always the complete picture. Finite-temperature dynamics means atoms are not stationary at their crystallographic sites. Ensemble averaging becomes relevant when thermal motion or disorder is present. And local correlations — short-range order — may exist even when long-range periodicity does not. The trajectory through these notes has moved from 0 K structures to finite-temperature dynamics to ensemble averages to configurational disorder and short-range order. Each step requires both experimental techniques (moving from Bragg diffraction to total scattering, from elastic to inelastic and quasielastic scattering) and computational approaches suited to the question. Computation provides the “why” behind observations. Experiment reveals what is present: this structure, this diffusion rate, this vibrational spectrum, this PDF. Computation can explain why. Why does this phase have lower energy than that one? What barrier controls diffusion, and what mechanism operates? Why does Li₂FeSO adopt cis-OLi4Fe2 coordination rather than the trans arrangement that simple electrostatics predicts? This explanatory power is distinct from prediction. Computation does not merely reproduce experimental observations — it provides access to the energetics, the mechanisms, and the physical principles that underlie them. This is information that experiment alone cannot directly access. 11.8 What now? When reading computational papers in your field, you are now equipped to ask critical questions. What \\(E(r)\\) method was used — classical potential, DFT, machine-learned potential? Is that method appropriate for the question being asked? How do the calculated quantities connect to experimental observables? Was the comparison to experiment direct and quantitative, or indirect and qualitative? What are the limitations of the approach, and are they acknowledged? These questions do not require you to run calculations yourself, but they do require understanding what different methods can and cannot do. When considering collaboration with modellers, you can ask more productive questions. What can computation provide that your measurements cannot directly access — energetics, barriers, mechanisms, partials of the PDF? What experimental data would constrain the modelling or validate the results? Collaboration works best when both sides understand what the other can contribute. When interpreting your own experimental results, you might recognise opportunities. An unexpected feature in a PDF, a relaxation process that does not fit standard models, a structure that does not quite match expectations — these are the puzzles that drive productive computational investigation. The case study in the previous section began with exactly this kind of observation: diffraction said no long-range order, but is the distribution really random? You do not need to become a computational scientist. But recognising what computation can and cannot do, understanding how its outputs connect to your measurements, and knowing what questions to ask — these make you a more effective scientist and a more valuable collaborator. Computation and experiment together are more powerful than either alone. "],["derivation-of-the-dynamical-matrix-for-periodic-systems.html", "A Derivation of the dynamical matrix for periodic systems A.1 Setup and notation A.2 Translational symmetry A.3 Plane-wave ansatz A.4 Substitution into equations of motion A.5 The dynamical matrix A.6 The key result A.7 Connection to experiment", " A Derivation of the dynamical matrix for periodic systems This appendix provides the full derivation of how translational symmetry reduces the phonon problem from a 3N × 3N eigenvalue problem (for N atoms in the crystal) to a 3n × 3n problem (for n atoms in the primitive cell) at each wavevector \\(\\mathbf{q}\\). A.1 Setup and notation Consider a periodic crystal with primitive lattice vectors. Let \\(\\mathbf{R}\\)ₗ denote the position of unit cell \\(l\\), and let atom \\(i\\) within the cell sit at position \\(\\mathbf{R}_l + \\boldsymbol{\\tau}_i\\), where \\(\\boldsymbol{\\tau_i}\\) is the position of atom \\(i\\) within the primitive cell. The displacement of atom \\(i\\) in cell \\(l\\) from its equilibrium position is \\(\\mathbf{u}_i(l)\\). The equation of motion is: \\[m_i \\frac{d^2 \\mathbf{u}_i(l)}{dt^2} = -\\sum_{l&#39;} \\sum_j \\Phi_{ij}(l, l&#39;) \\, \\mathbf{u}_j(l&#39;)\\] where \\(\\Phi_{ij}(l, l&#39;)\\) is the force constant matrix coupling atom \\(i\\) in cell \\(l\\) to atom \\(j\\) in cell \\(l&#39;\\). This is a 3 × 3 matrix (coupling x, y, z components), but we suppress the Cartesian indices for clarity. A.2 Translational symmetry The key property is translational invariance: the force constants depend only on the separation between cells, not on which cell we started from: \\[\\Phi_{ij}(l, l&#39;) = \\Phi_{ij}(\\mathbf{R}_l - \\mathbf{R}_{l&#39;}) = \\Phi_{ij}(\\mathbf{R}_{l-l&#39;})\\] We can write this more simply by defining \\(\\mathbf{R} = \\mathbf{R}_l− \\mathbf{R}_{l^\\prime}\\), so \\(\\Psi_{ij}(\\mathbf{R})\\) is the force constant coupling atom \\(j\\) at the origin to atom \\(i\\) in the cell at \\(\\mathbf{R}\\). A.3 Plane-wave ansatz We seek solutions where all atoms oscillate at the same frequency \\(\\omega\\). Based on the discussion in the main text, translational symmetry constrains the allowed phase relationships to plane waves. We therefore assume: \\[\\mathbf{u}_i(l) = \\mathbf{e}_\\mathrm{i} \\exp(i\\mathbf{q} \\cdot \\mathbf{R}_l) \\exp(-\\mathrm{i}\\omega t)\\] where \\(\\mathbf{e}_i\\) is the displacement amplitude (a complex vector) for atom \\(i\\) within the unit cell, and \\(\\mathbf{q}\\) is the wavevector. The time dependence \\(\\exp(-\\mathrm{i}\\omega t)\\) gives the harmonic oscillation; the spatial dependence \\(\\exp(\\mathrm{i}\\mathbf{q}\\cdot\\mathbf{R}_l)\\) encodes the phase variation from cell to cell. A.4 Substitution into equations of motion Substituting the plane-wave form into the equation of motion: \\[-m_i \\omega^2 \\mathbf{e}_i \\exp(i\\mathbf{q} \\cdot \\mathbf{R}_l) = -\\sum_{l&#39;} \\sum_j \\Phi_{ij}(\\mathbf{R}_l - \\mathbf{R}_{l&#39;}) \\, \\mathbf{e}_j \\exp(\\mathrm{i}\\mathbf{q} \\cdot \\mathbf{R}_{l&#39;})\\] where we have cancelled the common \\(\\exp(−\\mathrm{i}\\omega t)\\) factor from both sides. Now we use translational invariance. Define \\(\\mathbf{R} = \\mathbf{R}_l - \\mathbf{R}_{l^\\prime}\\), so \\(\\mathbf{R}_{l^\\prime} = \\mathbf{R}_l - \\mathbf{R}\\). The sum over \\(l&#39;\\) becomes a sum over \\(\\mathbf{R}\\): \\[-m_i \\omega^2 \\mathbf{e}_i \\exp(\\mathrm{i}\\mathbf{q} \\cdot \\mathbf{R}_l) = -\\sum_{\\mathbf{R}} \\sum_j \\Phi_{ij}(\\mathbf{R}) \\, \\mathbf{e}_j \\exp(\\mathrm{i}\\mathbf{q} \\cdot (\\mathbf{R}_l - \\mathbf{R}))\\] The \\(\\exp(\\mathrm{i}\\mathbf{q}\\cdot\\mathbf{R}_l)\\) factor appears on both sides: \\[-m_i \\omega^2 \\mathbf{e}_i \\exp(\\mathrm{i}\\mathbf{q} \\cdot \\mathbf{R}_l) = -\\sum_{\\mathbf{R}} \\sum_j \\Phi_{ij}(\\mathbf{R}) \\, \\mathbf{e}_j \\exp(i\\mathbf{q} \\cdot \\mathbf{R}_l) \\exp(-\\mathrm{i}\\mathbf{q} \\cdot \\mathbf{R})\\] Dividing both sides by \\(\\exp(\\mathrm{i}\\mathbf{q}\\cdot\\mathbf{R}_l)\\): \\[m_i \\omega^2 \\mathbf{e}_i = \\sum_{\\mathbf{R}} \\sum_j \\Phi_{ij}(\\mathbf{R}) \\exp(-\\mathrm{i}\\mathbf{q} \\cdot \\mathbf{R}) \\, \\mathbf{e}_j\\] A.5 The dynamical matrix Define the \\(\\mathbf{q}\\)-dependent force constant matrix: \\[C_{ij}(\\mathbf{q}) = \\sum_{\\mathbf{R}} \\Phi_{ij}(\\mathbf{R}) \\exp(-\\mathrm{i}\\mathbf{q} \\cdot \\mathbf{R})\\] This is the Fourier transform of the real-space force constants. The equation of motion becomes: \\[m_i \\omega^2 \\mathbf{e}_i = \\sum_j C_{ij}(\\mathbf{q}) \\, \\mathbf{e}_j\\] To put this in standard eigenvalue form, we define mass-weighted displacements \\(\\mathbf{\\tilde{e}}_i = \\sqrt m_i \\mathbf{e}_i\\) and the dynamical matrix: \\[D_{ij}(\\mathbf{q}) = \\frac{C_{ij}(\\mathbf{q})}{\\sqrt{m_i m_j}}\\] The equation becomes: \\[\\omega^2 \\tilde{\\mathbf{e}}_i = \\sum_j D_{ij}(\\mathbf{q}) \\, \\tilde{\\mathbf{e}}_j\\] or in matrix form: \\[\\omega^2 \\tilde{\\mathbf{e}} = \\mathbf{D}(\\mathbf{q}) \\tilde{\\mathbf{e}}\\] This is a standard eigenvalue problem. The eigenvalues are \\(\\omega^2\\) and the eigenvectors \\(\\mathbf{\\tilde{e}}_i\\) give the (mass-weighted) displacement patterns within the unit cell. A.6 The key result The dynamical matrix \\(D(\\mathbf{q})\\) has dimensions 3n × 3n, where n is the number of atoms in the primitive cell. At each wavevector \\(\\mathbf{q}\\), we diagonalise this matrix to obtain 3n eigenvalues \\(\\omega^2(\\mathbf{q})\\) and eigenvectors. The full displacement pattern across the crystal is reconstructed by multiplying the unit-cell eigenvector by the phase factor: \\[\\mathbf{u}_i(l) = \\frac{\\mathbf{e}_i}{\\sqrt{m_i}} \\exp(i\\mathbf{q} \\cdot \\mathbf{R}_l) \\exp(-i\\omega t)\\] Different values of \\(\\mathbf{q}\\) give different phase relationships between unit cells, but the mathematical structure of the eigenvalue problem is the same at each \\(\\mathbf{q}\\). This is why we can compute phonon dispersion by solving many small problems rather than one impossibly large one. A.7 Connection to experiment The phonon dispersion relation \\(\\omega(\\mathbf{q})\\) can be measured by inelastic neutron scattering on single crystals, where the scattering condition relates the momentum transfer to \\(\\mathbf{q}\\). For powder samples, the measurement averages over all \\(\\mathbf{q}\\), giving the phonon density of states. Computationally, we calculate \\(D(\\mathbf{q})\\) on a grid of \\(\\mathbf{q}\\)-points in the Brillouin zone, diagonalise at each point, and either plot the dispersion along high-symmetry paths or integrate to obtain the density of states. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
