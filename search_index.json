[["index.html", "Neutrons for Sustainable Energy Lecture Notes About", " Neutrons for Sustainable Energy Lecture Notes Benjamin J. Morgan 2025-12-06 About These notes accompany the Computational Tools lectures for the 2025 Neutrons for Sustainable Energy training school. "],["introduction.html", "Introduction Shared observables Scope Energy materials Learning objectives", " Introduction Neutron scattering probes structure and dynamics — where atoms are and how they move. Computational modelling calculates structure and dynamics. In many cases, both approaches produce the same observables: pair distribution functions, phonon densities of states, dynamic structure factors. This overlap is not coincidence. Neutrons scatter from nuclei; computational methods model nuclear positions and motion. The two approaches interrogate the same physics from different directions. This complementarity makes computation valuable for neutron scientists. Calculated observables can be compared directly with measurements. Where experiment and calculation agree, both are supported. Where they disagree, something interesting demands investigation — an inadequate model, an unexpected feature of the material, or a sample that differs from its idealised description. Computation also provides what experiment cannot directly access. Scattering data reveal that atoms occupy certain positions and move at certain rates. Computation can explain why: the energetics that favour one structure over another, the barriers that control diffusion, the anharmonicity that limits thermal conductivity. This explanatory power complements the descriptive power of measurement. The relationship runs both ways. Experimental puzzles drive computational investigation. A PDF that does not fit the expected structure, a relaxation process faster than models predict, an anomalous temperature dependence — these observations prompt computational work. The most productive research often emerges from this dialogue. Shared observables The following tables summarise quantities accessible from both neutron experiments and computation: Structure — where are atoms? Observable From neutrons From computation Structure (ordered) Diffraction Geometry optimisation, structure prediction Structure (disordered / finite T) Diffraction Ensemble average from MD or MC g(r) / PDF Total scattering MD or MC configurations Dynamics — how do atoms move? Observable From neutrons From computation S(Q,ω) — vibrational INS Lattice dynamics or MD S(Q,ω) — diffusive QENS MD → Van Hove → Fourier transform These are not loose analogies. When a modeller calculates S(Q,ω) from molecular dynamics and an experimentalist measures S(Q,ω) with QENS, the results can be overlaid directly. Scope These notes are not an exhaustive survey of computational methods. The focus is on techniques that either calculate observables measurable with neutrons, or provide complementary information about structure and dynamics. This means largely ignoring electronic properties — band gaps, optical response, electronic transport, and magnetism — even though these are important for some applications. Photovoltaics require optical properties, excited states, and electron-phonon coupling. Thermoelectrics require electronic transport properties alongside phonon properties. Magnetocalorics require treatment of magnetic ordering and spin-lattice coupling. These are beyond the present scope — the focus here is structure and dynamics, where neutron scattering and computational methods most directly connect. Neutrons primarily probe nuclear positions and motion — that is where these notes focus. The level of detail reflects the goal of building understanding rather than expertise. The emphasis is on conceptual foundations: what each method does, what approximations it makes, what questions it can answer, and how its outputs connect to experiment. Detailed derivations and underlying theory are not always included, though key results are stated and their significance explained. Practical aspects of running calculations — software choices, input file preparation, convergence testing — are largely omitted. The aim is to produce informed consumers of computational work and effective collaborators, not practicing computational scientists. Energy materials The materials relevant to sustainable energy — batteries, supercapacitors, hydrogen storage, photovoltaics, thermoelectrics — share common features: Ion transport matters (Li, H, O moving through structures) Disorder is often present (mixed site occupancy, local distortions) Finite-temperature behaviour is important (materials operate at room temperature or above, not 0 K) These are exactly the areas where computation and neutron scattering are most powerfully complementary. Neutrons are sensitive to light elements — hydrogen, lithium, oxygen — that X-rays struggle with. Computation can decompose complex dynamics into contributions from different species or different mechanisms, which experiment averages over. Learning objectives By the end of these notes, readers should be able to: Recognise the main computational methods and understand what questions each one answers Understand the limitations of each method and what inputs they require See how simulation outputs connect quantitatively to neutron observables Read computational papers more critically and collaborate with modellers more effectively Recognise when computational input would strengthen an experimental study "],["the-potential-energy-surface.html", "1 The potential energy surface 1.1 Building the concept 1.2 Features of the PES 1.3 Two questions", " 1 The potential energy surface All computational modelling of materials rests on a common foundation: for any arrangement of atoms, there is an associated energy. This mapping — configuration to energy — defines the potential energy surface (PES). 1.1 Building the concept Consider a two-dimensional crystal with two types of atoms in a square unit cell. Call this structure \\(S_1\\), with energy \\(E_1\\). A second structure \\(S_2\\) has the same atoms, but the unit cell has been sheared into a parallelogram, with atoms at the same fractional coordinates. This structure has a different energy, \\(E_2\\). This illustrates the first point: different arrangements of atoms have different energies. The transition from \\(S_1\\) to \\(S_2\\) can be imagined as a continuous deformation — gradually shearing the cell from square to parallelogram. At every point along this deformation, there is a well-defined atomic configuration, and therefore a well-defined energy. The change from \\(S_1\\) to \\(S_2\\) is not a discrete jump; it is a continuous pathway, with an energy defined at every point. Cell deformation is not the only degree of freedom. A third structure \\(S_3\\) keeps the cell shape fixed (back to the original square) but changes where atoms sit within the cell. Sliding one layer of atoms relative to another changes the local ordering: some neighbour pairs that were A-B become A-A or B-B. This structure \\(S_3\\) has its own energy \\(E_3\\), and again there is a continuous pathway from \\(S_1\\) to \\(S_3\\). These examples illustrate two independent directions on the potential energy surface: cell deformation (\\(S_1\\) to \\(S_2\\)) and internal rearrangement (\\(S_1\\) to \\(S_3\\)). Disordered structures extend this picture further. Structures \\(S_4\\), \\(S_5\\), \\(S_6\\) — arrangements where the two atom types do not follow a neat ordered pattern — each have their own energies \\(E_4\\), \\(E_5\\), \\(E_6\\). The continuous pathways connecting ordered structures to disordered ones may involve coordinated motion of many atoms or sequences of local rearrangements, but they are all points on the same continuous surface. These examples are low-dimensional slices through a much higher-dimensional space. The full potential energy surface is \\(E(\\mathbf{r})\\), where \\(\\mathbf{r}\\) represents the positions of all atoms — \\(3N\\) coordinates for \\(N\\) atoms. Every possible configuration of atoms corresponds to a point on this surface; every point has an energy. 1.2 Features of the PES The features of the potential energy surface have direct physical significance. Minima are stable structures. A minimum is a configuration where small displacements in any direction increase the energy — forces on all atoms are zero, and the structure is locally stable. These are the crystal structures determined from diffraction: the system sits at a minimum (or near one, at finite temperature). Relative energies of minima determine thermodynamics. If \\(S_1\\) has lower energy than \\(S_2\\), then \\(S_1\\) is thermodynamically preferred — at least at zero temperature. At finite temperature, entropy also matters, and the relevant quantity becomes the free energy rather than the energy. Barriers control kinetics. Along the pathway from \\(S_1\\) to \\(S_2\\), there is typically a maximum — an energy barrier. The height of this barrier determines how fast the transformation happens. A high barrier means the process is slow; a low barrier means it is fast. This applies to phase transitions, ionic diffusion, and any process where the system moves from one configuration to another. Diffraction reveals which structure is present; the barrier height explains why (a system may be trapped in a metastable minimum). The shape of the PES determines what is physically possible, not just what is energetically preferred. Consider a direct atom swap — two atoms exchanging positions. A continuous pathway for this exists on the PES, but it requires atoms to pass through each other, where the energy diverges. The barrier is infinite; the pathway is kinetically forbidden. In real materials, atoms change places by other mechanisms — via vacancies, interstitials, or correlated motion — pathways through configuration space that avoid catastrophic overlap. Curvature at minima determines vibrational properties. The energy change for small displacements around a minimum depends on the shape of the potential well. A steep, narrow well produces strong restoring forces and high vibrational frequencies. A shallow, broad well produces softer vibrations at lower frequencies. This curvature is what phonon calculations determine — and what inelastic neutron scattering measures. The shape of the PES around a minimum appears directly in INS spectra. 1.3 Two questions With the PES established, two questions arise: How is E(r) calculated? Given an arrangement of atoms, how is its energy determined? Different methods — classical potentials, density functional theory, machine-learned potentials — give different answers with different tradeoffs of accuracy and computational cost. What is done with E(r)? Given the ability to calculate energies, what questions can be answered? Finding the lowest-energy structure is one application. Characterising vibrations around that structure is another. Simulating atomic motion at finite temperature, or sampling over different configurations — these require different approaches. These are independent choices. Molecular dynamics can use a classical potential, DFT, or a machine-learned potential — the choice of how to calculate E(r) is independent of the choice to do MD. A common misconception is that “molecular dynamics” implies classical potentials, or that “DFT” implies static calculations. Neither is true. Any method for calculating E(r) can be combined with any method for using it. The remainder of Part I covers three approaches to calculating E(r): classical potentials, density functional theory, and machine-learned interatomic potentials. Parts II and III turn to what is done with that ability — geometry optimisation, phonon calculations, molecular dynamics, and Monte Carlo sampling. "],["classical-potentials.html", "2 Classical potentials 2.1 Why start here? 2.2 The simplest model: pair potentials 2.3 Building a potential from physics 2.4 Periodic systems and the Ewald problem 2.5 When pair potentials aren’t enough 2.6 The parameterisation problem 2.7 Strengths and limitations", " 2 Classical potentials 2.1 Why start here? Classical potentials are not the most accurate method for calculating E(r), but they are the most transparent. You can write down the function, plot it, and see the potential energy surface directly. This builds intuition before moving to methods where E(r) comes out of a more complex calculation. 2.2 The simplest model: pair potentials Classical potentials are parameterised analytical functions that give the energy of a configuration of atoms. A mathematical form is chosen, parameters are fitted to reproduce known properties, and the resulting function is used to calculate energies for any configuration. The simplest approach is to decompose the total energy into contributions from pairs of atoms: \\[E = \\sum_{ij} V(r_{ij})\\] where \\(r_{ij}\\) is the distance between atoms \\(i\\) and \\(j\\), and \\(V(r)\\) is a pair potential. For each pair of atoms, we look up their separation, evaluate the pair potential, and sum over all pairs. This is conceptually simple and computationally cheap. 2.3 Building a potential from physics What should \\(V(r)\\) look like for an ionic crystal? The dominant contribution is electrostatic. Ions carry charges, so opposite charges attract and like charges repel. For two ions with charges \\(q_i\\) and \\(q_j\\) separated by distance \\(r\\), the Coulomb energy is \\(q_iq_j/r\\). This is long-range — it falls off slowly with distance. Electrostatics alone would cause the crystal to collapse — opposite charges attracting without limit. What prevents this is short-range repulsion. When ions get close, their electron clouds overlap, and this costs energy. The repulsion is modelled with a steeply rising function at short range. A common choice is the Born-Mayer form: \\(A\\exp(−r/\\rho)\\), an exponential that increases rapidly as \\(r\\) decreases. There is also a weak, long-range attraction from van der Waals forces, typically modelled as \\(−C/r^6\\). For hard ions this is often a small correction, but it matters more for polarisable species. Combining Coulomb, exponential repulsion, and dispersion gives a Buckingham-type potential — the workhorse for simulating ionic solids like oxides and halides. 2.4 Periodic systems and the Ewald problem To simulate a bulk crystal rather than a finite cluster, we use periodic boundary conditions: a unit cell is treated as if it tiles infinitely in all directions. For short-range interactions, the minimum image convention applies — each atom interacts with the nearest periodic image of every other atom, and if the potential decays fast enough, it can be truncated at some cutoff. For the Coulomb term, simple truncation doesn’t work. The sum over periodic images is conditionally convergent — the answer depends on how you truncate, which is unphysical. The solution is Ewald summation. Each point charge is screened with a compensating Gaussian distribution. The screened charges are short-ranged and sum quickly in real space; the smooth Gaussian distributions sum quickly in reciprocal space. A correction is then applied for the self-interaction of each charge with its own screening cloud. Both sums are absolutely convergent, and the result is well-defined. The reciprocal-space machinery will be familiar from diffraction. 2.5 When pair potentials aren’t enough Pair potentials assume the energy contribution from two atoms depends only on their separation. This works well for ionic materials where bonding is non-directional. But for covalent or partially covalent materials, where bond angles matter, pair potentials are insufficient. In a silicate, for instance, the O–Si–O angle isn’t arbitrary — there’s a preferred tetrahedral geometry. Capturing this requires three-body terms that penalise deviations from the preferred angle. For molecular systems or structures with well-defined bonded units, the “force field” approach uses harmonic springs for bond stretches, angle terms for bond angles, and torsional terms for dihedrals. More complex functional forms exist for metals (embedded atom method), semiconductors (Tersoff potentials), and reactive systems (ReaxFF). The basic ionic model also treats ions as fixed point charges. Real ions are polarisable — their electron clouds respond to local electric fields — and may have non-spherical charge distributions. Extensions address this through explicit induced dipoles, higher-order multipoles, variable ion size and shape, or charge equilibration schemes where charges vary self-consistently subject to a total charge constraint. Classical potentials can be much more sophisticated than the basic Buckingham form. But there is always a tradeoff: more realism means more parameters, more complexity, and more fitting required. 2.6 The parameterisation problem A classical potential has parameters — charges, repulsion coefficients, cutoffs. Where do they come from? Historically, parameters were fitted to reproduce experimental properties: lattice constants, elastic moduli, phonon frequencies, defect energies. The potential is only as good as the data it was fitted to, and only as transferable as that data allows. There is also a risk of overfitting: fitting five parameters to three experimental observables leaves the problem underdetermined. Parameter sets may reproduce those observables perfectly but for the wrong reasons, and fail elsewhere. Increasingly, potentials are fitted to DFT calculations. A set of configurations is generated, their energies and forces are calculated with DFT, and the potential parameters are fitted to reproduce them. This covers regions of configuration space that experiment doesn’t probe directly, and provides enough data points to constrain all parameters. Either way, the quality of a classical potential depends entirely on how well the parameterisation captures the relevant physics. And there is a more fundamental issue: the functional form itself bounds the accuracy. The physics that can be captured is limited by the mathematical form chosen. If the real interactions don’t match that form, no amount of parameter fitting will fix it. 2.7 Strengths and limitations Classical potentials are fast — orders of magnitude cheaper than quantum-mechanical methods. Simulations of millions of atoms for nanoseconds are routine. With methods like particle-mesh Ewald or fast multipole for electrostatics, computational cost scales as O(N log N) with system size. Classical potentials are also interpretable: each term’s contribution is visible, and parameter–property relationships can be understood. When something goes wrong, diagnosis is possible. For some material classes — simple ionic solids, for instance — good potentials exist and have been extensively validated. The limitations are significant. The treatment of electrostatics is typically simple: fixed point charges throughout the simulation, with no polarisation response or charge transfer. Electronic structure is implicit — electrons aren’t explicitly treated, so processes involving electronic rearrangement cannot be described. Developing a good classical potential is hard work — designing a functional form, generating training data, fitting, validating, finding problems, and iterating. This requires substantial effort with no guarantee of success. Classical potentials also have limited transferability. A potential fitted to one material, or one region of configuration space, may fail for another. Bulk doesn’t guarantee surfaces; ambient conditions don’t guarantee high pressure. Extrapolation fails silently — the potential gives a number even outside its valid domain. Classical potentials are the right choice for large system sizes or long timescales, for well-characterised material classes where good potentials exist, or for quick exploratory calculations. They are the wrong choice when the required accuracy exceeds what the potential can provide, or when moving outside its validated domain. "],["density-functional-theory.html", "3 Density functional theory 3.1 Why first principles? 3.2 The problem: the many-electron wavefunction 3.3 The DFT insight 3.4 The Kohn-Sham approach 3.5 The Kohn-Sham equations 3.6 Still exact—in principle 3.7 Exchange-correlation functionals 3.8 Strengths and limitations", " 3 Density functional theory 3.1 Why first principles? Classical potentials have fundamental limitations: the accuracy is bounded by the functional form you choose, and someone has to develop and validate a potential for your specific system. First-principles methods address this. Instead of parameterising interactions, we calculate E(r) from quantum mechanics—solving (approximately) for the electrons in your system. The physics is built in, not fitted. 3.2 The problem: the many-electron wavefunction In principle, the energy of a collection of electrons and nuclei comes from solving the Schrödinger equation. The ground-state wavefunction \\(\\psi(r_1, r_2, \\ldots, r_n)\\) gives you everything—the energy, the electron density, all ground-state properties. The problem is dimensionality. For N electrons, the wavefunction is a function of 3N spatial coordinates. The complexity grows exponentially with the number of electrons—intractable for real materials. 3.3 The DFT insight The electron density \\(\\rho(r)\\) is always three-dimensional—ten electrons or ten thousand, it’s still just \\(\\rho(x, y, z)\\). The first Hohenberg-Kohn theorem (1964) says that for ground states, the energy can be written as a functional of the density, \\(E = E[\\rho]\\). (A functional maps a function to a number—here \\(\\rho(r)\\) is a function of position, and \\(E\\) is a single number.) The theorem doesn’t tell us what this functional is—just that it exists. The second Hohenberg-Kohn theorem says this functional is variational: the density that minimises $E[$ is the true ground-state density.] is the electron-electron repulsion, and the last term is the electron-nuclear attraction (integrating the density against the nuclear potential \\(v\\)). The last term we can calculate—we know \\(v\\) from the nuclear positions. The first two we cannot: we don’t know how to write them as functionals of the density that we can evaluate. 3.4 The Kohn-Sham approach Kohn and Sham (1965) addressed this by separating each unknown term into a part we can calculate plus a correction. For the electron-electron interaction, separate \\(V_\\mathrm{ee}\\) into a classical part plus the remainder: \\[V_{ee}[\\rho] = J[\\rho] + (V_{ee}[\\rho] - J[\\rho])\\] where \\(J[\\rho]\\) is the classical Coulomb energy—the electrostatic self-repulsion of a charge distribution: \\[J[\\rho] = \\frac{1}{2}\\int\\int \\frac{\\rho(\\mathbf{r})\\rho(\\mathbf{r}&#39;)}{|\\mathbf{r}-\\mathbf{r}&#39;|} d\\mathbf{r} d\\mathbf{r}&#39;\\] This we can calculate directly from the density. The remainder contains two effects we’re not capturing: exchange (electrons with parallel spin avoid each other, a consequence of the Pauli principle) and correlation (electrons avoid each other due to their mutual repulsion). For the kinetic energy, there’s a system for which this is straightforward: non-interacting electrons. For these, the ground state is a Slater determinant of one-electron orbitals \\(\\phi_i\\), and the kinetic energy is just the sum of one-electron contributions: \\[T_s = -\\frac{1}{2}\\sum_i \\langle\\phi_i|\\nabla^2|\\phi_i\\rangle\\] So we separate \\(T\\) into this non-interacting kinetic energy \\(T_\\mathrm{s}\\) plus the remainder: \\[T[\\rho] = T_s[\\rho] + (T[\\rho] - T_s[\\rho])\\] The remainder exists because interacting electrons move differently from non-interacting ones—but it’s small. Both systems have the same density, so electrons are confined to the same regions of space, and kinetic energy is largely determined by spatial confinement. The remainders from both separations are bundled into a single correction called the exchange-correlation functional: \\[E_{xc}[\\rho] = (T[\\rho] - T_s[\\rho]) + (V_{ee}[\\rho] - J[\\rho])\\] The total energy becomes: \\[E[\\rho] = T_s[\\rho] + J[\\rho] + \\int v(\\mathbf{r})\\rho(\\mathbf{r})d\\mathbf{r} + E_{xc}[\\rho]\\] The first three terms we can calculate. All the unknowns are in \\(E_\\mathrm{xc}\\). 3.5 The Kohn-Sham equations To compute \\(T_\\mathrm{s}\\) we need to find the orbitals of our non-interacting system. These satisfy one-electron Schrödinger equations in an effective potential \\(v_\\mathrm{eff} = v + v_J + v_\\mathrm{xc}\\), where \\(v\\) is the nuclear potential, \\(v_J\\) is the classical Coulomb potential from the electron density, and \\(v_\\mathrm{xc}\\) is the functional derivative of \\(E_\\mathrm{xc}\\) with respect to \\(\\rho\\). From the orbitals we construct the density: \\[\\rho(\\mathbf{r}) = \\sum_i |\\phi_i(\\mathbf{r})|^2\\] Since \\(v_\\mathrm{j}\\) and \\(v_\\mathrm{xc}\\) depend on \\(\\rho\\), which depends on the orbitals, which depend on \\(v_\\mathrm{eff}\\), we must iterate until self-consistent. 3.6 Still exact—in principle Nothing so far is an approximation—it’s exact repackaging into terms we can calculate plus a correction. If we knew \\(E_\\mathrm{xc}[\\rho]\\) exactly, we’d get the exact ground-state energy. All the approximation in practical DFT lives in one place: \\(E_\\mathrm{xc}\\). 3.7 Exchange-correlation functionals The simplest approximation is the local density approximation (LDA): at each point, use the exchange-correlation energy of a uniform electron gas with that density. This is surprisingly effective, but tends to overbind—predicting bonds that are too strong and too short. Generalised gradient approximations (GGAs) improve on this by including how the density varies—not just \\(\\rho\\) but also \\(\\nabla \\rho\\). PBE is the most common GGA in materials science. Meta-GGAs go further, including the kinetic energy density as well. r2SCAN is increasingly the reference level for materials science calculations. Hybrid functionals mix in a fraction of exact exchange from Hartree-Fock theory. These often give better band gaps and reaction energies, but are significantly more expensive—you’re reintroducing the costly exact exchange integrals. 3.8 Strengths and limitations DFT is first-principles and transferable—the same functional works across different chemistries without system-specific parameterisation. It’s accurate for structures and relative energies, and scales as roughly N3, making it practical for systems of hundreds of atoms. The limitations are significant. The computational cost means DFT is practical for hundreds of atoms, not thousands, and for static calculations or short dynamics rather than the long timescales often needed to study diffusion or phase transitions. For those problems, you need classical potentials or machine-learned interatomic potentials. More fundamentally, DFT offers no systematic path to the exact answer. In wavefunction methods, you can improve accuracy by including more excitations or larger basis sets—the path is clear even if expensive. In DFT, accuracy depends on choosing a good functional, and a better functional requires new physical insight, not just more computer time. This also means different functionals give different answers, and it’s not always obvious which to trust. Finally, approximate functionals suffer from self-interaction error: an electron spuriously repels itself because \\(J[\\rho]\\) includes the interaction of each electron with the total density, including its own contribution. This causes particular problems for localised states and transition-metal chemistry, where the error doesn’t cancel out. "],["machine-learned-interatomic-potentials.html", "4 Machine-learned interatomic potentials 4.1 Motivation 4.2 The idea 4.3 Local atomic environments 4.4 The landscape 4.5 Strengths and limitations 4.6 When to use 4.7 Summary and what comes next", " 4 Machine-learned interatomic potentials 4.1 Motivation We’ve now seen two approaches to calculating \\(E(r)\\). Classical potentials are fast and scale well, but require choosing a functional form and fitting parameters — the accuracy is bounded by these choices. DFT is transferable and doesn’t assume a functional form, but the computational cost limits us to hundreds of atoms and short timescales. Machine-learned interatomic potentials sit between these. Like classical potentials, they’re parameterised models fitted to data. But unlike classical potentials, they don’t assume a functional form for the interactions — the model learns the shape of E(r) from training data. And the training data typically comes from DFT, so the model inherits DFT’s accuracy for systems similar to those it was trained on. 4.2 The idea The principle is supervised learning. We generate a dataset of atomic configurations with their DFT energies and forces. We then train a model — typically a neural network or similar — to predict \\(E(r)\\) from atomic positions. Once trained, evaluating the model is much cheaper than running DFT, so we can use it for large systems and long simulations. The model learns a mapping: atomic positions → energy. But how should this mapping be structured? 4.3 Local atomic environments Most MLIPs decompose the total energy into contributions from each atom: \\[E = \\sum_i \\varepsilon_i\\] where \\(\\varepsilon_i\\) depends on the local environment around atom i — the species and positions of nearby atoms within some cutoff. The model learns a mapping from local environment to atomic energy contribution. This locality assumption is physically motivated: in most materials, an atom’s energy contribution depends primarily on its immediate surroundings. It also makes the model size-transferable — a model trained on small cells can be applied to larger systems. There is considerable research into how local environments should be represented (descriptors, symmetry functions) and what model architectures work best (neural networks, Gaussian processes, equivariant networks). These details are beyond our scope. The key point is that MLIPs are flexible models with many parameters, fitted to reproduce DFT energies and forces. 4.4 The landscape The MLIP field is evolving rapidly. Broadly, you’ll encounter three approaches. Purpose-trained potentials are models developed specifically for a particular system — a battery cathode material, a solid electrolyte, a class of alloys. The training data is generated for that system, covering the configurations relevant to the intended application. This typically gives the best accuracy, because the model is focused on what matters. But it requires substantial effort: generating training data means running many DFT calculations, which is expensive. Training the model and validating it carefully adds further work. This is analogous to developing a classical potential, but without assuming a functional form. Foundation models take a different approach. These are pre-trained on massive datasets spanning much of the periodic table and many structure types, then released for general use. Training such a model requires enormous computational resources — this is work done by large research groups or companies (Google, Meta, Microsoft) with access to that scale of compute. The result is a model that provides reasonable accuracy out of the box for many systems, without needing to generate training data or train anything yourself. Examples include MACE-MP, CHGNet, SevenNet, and others. Because foundation models have seen diverse training data, they tend to be more transferable across different chemistries. The tradeoff is that accuracy for any specific system may be lower than a purpose-trained model would achieve. Fine-tuning offers a middle path. Starting from a foundation model, you add training data for your specific system and continue training. The foundation model provides a starting point — it already knows something about interatomic interactions in general — and fine-tuning adapts it to your chemistry. This can give good accuracy with less effort than training from scratch. Which approach was used affects how much you should trust the results. A purpose-trained potential with careful validation is more reliable than a foundation model applied to a system far from its training data. 4.5 Strengths and limitations MLIPs learn the shape of E(r) from data, rather than assuming a functional form. This avoids the limitations of classical potentials, where accuracy is bounded by the mathematical form you write down. Once trained, evaluation is fast — approaching classical potential speeds for some architectures. And the approach is systematically improvable: more training data generally means a better model. If the potential fails in some region of configuration space, you can add more data there and retrain. Foundation models have also made the approach more accessible — you can get reasonable results without training anything yourself. The limitations are significant. The model can only be as good as its training data. Errors in the DFT calculations propagate to the potential, and the model inherits the limitations of whichever functional was used — if PBE gets something wrong, so will the MLIP trained on PBE data. More fundamentally, extrapolation is dangerous. MLIPs interpolate well within their training domain but can fail badly outside it. Unlike DFT, which will give you some answer for any configuration, an MLIP may give confident but wrong predictions for configurations far from training data. This failure can be silent — the model doesn’t know it’s extrapolating. MLIPs are also less interpretable than classical potentials. A Buckingham potential has parameters you can understand — charges, repulsion coefficients. An MLIP has thousands of internal parameters with no direct physical meaning. When something goes wrong, it’s harder to diagnose why. This places a validation burden on the user: you need to check that the potential is accurate for the configurations you care about. 4.6 When to use MLIPs are increasingly the default choice when you need larger systems or longer timescales than DFT allows, when classical potentials don’t exist for your system or aren’t accurate enough, and when you’re willing to validate carefully. They’re the wrong choice when DFT is affordable for your problem (simpler, no training overhead), when you’re studying configurations far from any available training data, or when you need guaranteed reliability with no possibility of silent failure. The field is moving fast. What’s state-of-the-art now may be superseded within a year. For your purposes as consumers of computational work: ask what potential was used, what it was trained on, and how it was validated for the system being studied. 4.7 Summary and what comes next We have now covered three approaches to calculating E(r). Classical potentials are parameterised analytical functions — fast and interpretable, but bounded by the functional form you choose. The physics you can capture depends on the mathematical form you write down, and developing a good potential requires substantial effort. Density functional theory calculates E(r) from quantum mechanics, with all approximation localised in the exchange-correlation functional. It’s transferable across chemistries without system-specific fitting, but computationally expensive — practical for hundreds of atoms, not millions. Machine-learned interatomic potentials learn E(r) from training data, typically DFT calculations. They avoid assuming a functional form, and can approach DFT accuracy at near-classical cost. But they inherit the limitations of their training data, and can fail silently outside their training domain. These are methods for calculating E(r) — given a configuration of atoms, what is its energy? But calculating the energy is not the end goal. The questions we actually want to answer determine what we do with that ability. In Lecture II, we turn to the second axis: methods that use E(r). We’ll cover geometry optimisation (finding stable structures), phonon calculations (characterising vibrations), and molecular dynamics (simulating atomic motion at finite temperature). Each of these can be combined with any of the E(r) methods we’ve discussed — the choices are independent. In Lecture III, we’ll cover Monte Carlo sampling and cluster expansion methods for treating configurational disorder, and look at case studies where experimental puzzles drove computational investigation. "],["introduction-1.html", "5 Introduction", " 5 Introduction Part I introduced the potential energy surface — the mapping from atomic configuration to energy. For any arrangement of atoms, there is an associated energy, and the features of this surface encode the physics we care about. Minima correspond to stable structures. Curvature at a minimum determines how atoms vibrate around equilibrium. Barriers between minima control the rates of transitions — diffusion, phase transformations, chemical reactions. We then covered three approaches to calculating \\(E(r)\\): classical potentials, which are fast but bounded by their functional form; density functional theory, which is transferable but expensive; and machine-learned interatomic potentials, which learn from DFT data and can approach DFT accuracy at near-classical cost. These methods differ in their tradeoffs, but any of them can be combined with the techniques we’ll cover today. The goal now is to answer scientific questions. What structure is stable at a given composition? How do atoms vibrate, and what frequencies characterise that motion? How do atoms move at finite temperature — do they diffuse, and by what mechanism? These questions connect directly to what you measure: stable structures to diffraction, vibrations to inelastic neutron scattering, diffusive motion to quasielastic scattering. This lecture covers methods that use \\(E(r)\\) to interrogate the potential energy surface. Geometry optimisation locates minima — finding the stable structures. Phonon calculations characterise the curvature at those minima — predicting vibrational frequencies. Molecular dynamics simulates atomic motion at finite temperature, capturing behaviour that goes beyond the harmonic picture. Each of these methods works with any of the \\(E(r)\\) approaches from Part I. "],["finding-minima-geometry-optimisation.html", "6 Finding minima: geometry optimisation 6.1 Why compute a structure? 6.2 Structures and minima 6.3 Steepest descent 6.4 Doing better with curvature 6.5 Local and global minima", " 6 Finding minima: geometry optimisation 6.1 Why compute a structure? Geometry optimisation locates minima on the potential energy surface — finding stable structures. But diffraction already tells us where atoms are. Why compute structures at all? Sometimes the goal is direct comparison: validating a structural model from diffraction analysis. If the optimised structure matches the experimental one, both the calculation and the diffraction analysis are supported. If they disagree, something needs investigation — perhaps the experimental model is incomplete, perhaps the computational method is inadequate for this system, perhaps the disagreement points to something interesting. More often, computation provides complementary information. Diffraction tells you what structure forms; computation tells you why. By calculating the energies of competing structures, we can understand polymorph stability, site preferences for dopants, or the driving forces behind structural distortions. We can also calculate energies for structures that don’t form — understanding what’s metastable, what’s unstable, and why. Computation can also resolve what Bragg diffraction struggles with. Some elements have similar scattering lengths — oxygen and fluorine, for instance, are nearly indistinguishable by neutron diffraction, making oxyfluoride structures hard to solve from scattering data alone. Computation can test whether O or F at a given site is energetically preferred. Similarly, partial site occupancies appear as averages in Bragg diffraction, but computation can test the energetics of specific configurations. Local distortions and short-range order present a different challenge. Bragg diffraction reports only the average structure, so local correlations are invisible. Computation can capture these by optimising large supercells with different local arrangements. Finally, there is prediction. Computational screening can assess the stability of hypothetical materials before synthesis — testing whether a proposed composition would be thermodynamically stable, and if so, what structure it would adopt. This is increasingly important for materials discovery. 6.2 Structures and minima When we say a material has a particular “structure”, we typically mean the positions determined by diffraction — the time-averaged positions of atoms at the measurement temperature. Computationally, a structure corresponds to a basin on the potential energy surface — a region of configuration space that drains to a particular minimum. Different basins correspond to different structures: polymorphs of the same composition, different orderings of atoms on sites, different local arrangements. A real material’s PES has many such basins. The simplest description of a basin is the position of its minimum — the 0 K atomic positions, where atoms sit with no thermal motion. In the harmonic approximation, these are also the mean positions at finite temperature; with anharmonicity they can differ, thermal expansion being the most familiar example. The procedure for finding such minima is called geometry optimisation. The force on an atom is the negative gradient of energy: \\[\\mathbf{F}_i = -\\frac{\\partial E}{\\partial \\mathbf{r}_i}\\] At a minimum, all forces are zero. Geometry optimisation adjusts atomic positions (and usually cell parameters) until this condition is satisfied. 6.3 Steepest descent The simplest approach is steepest descent: calculate the forces on all atoms and move each atom in the direction of its force. Since forces point downhill on the potential energy surface, this reduces the energy. Recalculate forces, move again, iterate. Steepest descent tells you which direction to move, but not how far. Too small a step and convergence is slow; too large and you overshoot, oscillate, or diverge. Various approaches exist — fixed step sizes, line searches to find the minimum along each descent direction, adaptive schemes that adjust based on whether the energy decreased — but none is universally optimal. The algorithm also tends to zigzag in narrow valleys of the energy surface, where the steepest direction points across the valley rather than along it towards the minimum. 6.4 Doing better with curvature Newton-Raphson can do better by using more information. Consider first a one-dimensional case: we have \\(E(x)\\) and want to find the minimum. Near any point \\(x_0\\), we can approximate the energy as a Taylor expansion: \\[E(x) \\approx E(x_0) + E&#39;(x_0)(x - x_0) + \\frac{1}{2}E&#39;&#39;(x_0)(x - x_0)^2\\] This expansion is truncated at second order — we assume the energy surface is approximately quadratic near our current point. This is the harmonic approximation, and its validity depends on where we are on the surface. Near a minimum, where the surface is smooth and bowl-shaped, the quadratic approximation is accurate. Far from a minimum, where the surface may have more complex shape, the approximation can be poor. For a quadratic function, we can find the minimum exactly: differentiate, set to zero, solve. The result is: \\[x = x_0 - \\frac{E&#39;(x_0)}{E&#39;&#39;(x_0)}\\] This is the predicted position of the minimum, under the assumption that the PES is locally harmonic. The step to take — gradient divided by curvature — makes physical sense: if the curvature is large (a steep, narrow well), we take a small step; if the curvature is small (a shallow, broad well), we take a large step. The curvature tells us how far away the minimum is likely to be. Because the harmonic assumption is not exact, \\(x\\) won’t be the exact minimum — but it usually provides a good estimate. From the new position we can repeat the procedure: recalculate the gradient and curvature, predict a new minimum position, move there. Each iteration refines the approximation. Near a minimum, where the harmonic approximation is accurate, Newton-Raphson converges rapidly — often in just a few iterations. In three dimensions with N atoms, the same principle applies. The first derivative \\(E^\\prime\\) becomes a gradient vector \\(\\mathbf{g}\\) with 3N components — one for each atomic coordinate. The second derivative E″ becomes the Hessian matrix H, a 3N × 3N matrix of second derivatives: \\[H_{ij} = \\frac{\\partial^2 E}{\\partial r_i \\partial r_j}\\] The quadratic approximation becomes: \\[E(\\mathbf{r}) \\approx E_0 + \\mathbf{g}^T \\mathbf{u} + \\frac{1}{2}\\mathbf{u}^T \\mathbf{H} \\mathbf{u}\\] where u is the displacement from the current position. The Newton-Raphson step generalises to: \\[\\mathbf{u} = -\\mathbf{H}^{-1}\\mathbf{g}\\] The same iterative logic applies: predict the minimum position, move there, repeat until converged. Newton-Raphson performs well when the starting guess is close to a minimum, where the harmonic approximation is reasonable. Far from a minimum, where the harmonic approximation breaks down, the algorithm may converge slowly, take erratic steps, or fail to find a minimum at all. In practice, optimisation codes often use methods that are more robust across the whole energy surface — conjugate gradient methods, or quasi-Newton methods (such as BFGS) that build up an approximate Hessian from the gradient information accumulated over successive steps. These handle the early stages of optimisation more reliably, while still converging rapidly near a minimum where the harmonic approximation holds. 6.5 Local and global minima Geometry optimisation finds a local minimum — the one nearest your starting point. A real potential energy surface has many local minima, corresponding to different polymorphs, different site orderings, different local arrangements of atoms. Which minimum you find depends on where you start. This is usually fine: you’re testing a specific structural hypothesis, not searching for the global minimum. If you want to compare polymorphs, you optimise each one separately and compare their energies. Finding the global minimum without knowing where to start — structure prediction — is a harder problem, but not one we need to solve for most purposes. When it is needed, the approach is conceptually simple: generate many starting points and optimise each one. Methods differ in how they generate those starting points — random structures with sensible constraints, evolutionary algorithms that breed new candidates from successful ones, or perturbations from known minima — but at the core, they all rely on the same local optimisation we’ve just discussed. "],["phonon-calculations.html", "7 Phonon calculations 7.1 Vibrations around equilibrium 7.2 The harmonic approximation 7.3 Periodic systems and phonon dispersion 7.4 Calculating phonons in practice 7.5 What you get 7.6 Limitations", " 7 Phonon calculations 7.1 Vibrations around equilibrium Geometry optimisation finds a minimum on the potential energy surface — a configuration where forces vanish. But atoms don’t sit motionless at these positions. At any temperature above absolute zero, thermal energy causes atoms to vibrate around their equilibrium sites. Even at 0 K, quantum zero-point motion means atoms are never truly stationary, though for our purposes the classical picture of thermal vibrations suffices. This atomic motion is precisely what inelastic neutron scattering probes. When a neutron scatters inelastically from a sample, it exchanges energy with vibrational modes. The measured spectrum S(Q,ω) encodes information about how atoms move — their vibrational frequencies and displacement patterns. Phonon calculations aim to predict this vibrational behaviour. At a minimum, forces vanish but the Hessian — the matrix of second derivatives — remains, encoding how the energy changes when atoms are displaced from equilibrium. This curvature of the potential energy surface approximately determines vibrational behaviour. Stiff bonds (high curvature) give strong restoring forces and high vibrational frequencies. Soft bonds (low curvature) give weak restoring forces and low frequencies. 7.2 The harmonic approximation Near a minimum, the potential energy surface is smooth and bowl-shaped. Expanding the energy in a Taylor series around equilibrium: \\[E = E_0 + \\sum_i \\frac{\\partial E}{\\partial r_i} \\delta r_i + \\frac{1}{2} \\sum_{ij} \\frac{\\partial^2 E}{\\partial r_i \\partial r_j} \\delta r_i \\delta r_j + \\ldots\\] At equilibrium, the first derivatives vanish. If we truncate at second order — the harmonic approximation — the energy becomes quadratic in displacements: \\[E = E_0 + \\frac{1}{2} \\sum_{ij} H_{ij} \\, \\delta r_i \\, \\delta r_j\\] where H is the Hessian matrix. This is the same approximation we used in Newton-Raphson optimisation, but now it serves a different purpose. In optimisation, it was a computational convenience for finding minima efficiently. Here, it makes the vibrational problem tractable — and for many systems, it’s also physically accurate. A quadratic energy means linear restoring forces: F = −Hδr. The equation of motion for atom i is then: \\[m_i \\frac{d^2 \\delta r_i}{dt^2} = -\\sum_j H_{ij} \\delta r_j\\] This is a system of coupled linear differential equations. We seek solutions where all atoms oscillate at the same frequency — normal modes. For a harmonic oscillator, solutions are sinusoidal in time. It is mathematically convenient to write these as complex exponentials δrᵢ(t) = uᵢ exp(iωt), with the understanding that the physical displacement is the real part. Differentiating twice: \\[\\frac{d^2 \\delta r_i}{dt^2} = -\\omega^2 u_i \\exp(i\\omega t)\\] Substituting into the equation of motion, the exp(iωt) factors appear on both sides and cancel, leaving: \\[-m_i \\omega^2 u_i = -\\sum_j H_{ij} u_j\\] This is an eigenvalue problem, though not quite in standard form because of the mass factor. Introducing the mass-weighted dynamical matrix: \\[D_{ij} = \\frac{H_{ij}}{\\sqrt{m_i m_j}}\\] converts this to standard form. Diagonalising D gives eigenvalues ω² and eigenvectors that describe the displacement patterns — which atoms move, in what directions, with what relative amplitudes. Each eigenvector corresponds to a normal mode: a collective motion in which all atoms oscillate at the same frequency, maintaining fixed phase relationships. For a system with N atoms in three dimensions, there are 3N eigenvalues and hence 3N normal modes. Three of these correspond to uniform translation (zero frequency); for periodic solids these become the acoustic modes at q = 0. 7.3 Periodic systems and phonon dispersion The derivation above applies to any collection of atoms, but for a macroscopic crystal we cannot diagonalise a matrix with 3N rows for every atom in the sample. Translational symmetry solves this. A normal mode requires all atoms to oscillate at the same frequency — that’s what makes it a normal mode. But they need not oscillate in phase. In a periodic crystal, translational symmetry constrains which phase relationships are allowed: if the physics is unchanged by shifting the whole crystal by a lattice vector R, the solution must transform consistently under that shift. The functions with well-defined behaviour under discrete translations are plane waves exp(iq·R). Different wavevectors q correspond to different phase relationships between unit cells. At q = 0, exp(iq·R) = 1 for all R, so all cells move in phase. As q increases, the phase difference between adjacent cells grows. At the Brillouin zone boundary, exp(iq·R) = −1 for nearest-neighbour cells — adjacent cells move in antiphase. Beyond the zone boundary, you’re relabelling the same physical modes, which is why reciprocal space is periodic. For each q, the displacement pattern within a unit cell is an eigenvector of the dynamical matrix D(q); the full solution across the crystal is this pattern multiplied by exp(iq·R). The key result — derived in the appendix — is that D(q) has size 3n × 3n, where n is the number of atoms in the primitive cell. Computationally, this is essential: we diagonalise a small matrix at each q, rather than a single impossibly large matrix for the whole crystal. At each q, diagonalisation gives 3n eigenvalues and eigenvectors, corresponding to the 3n branches of the dispersion relation ω(q). The phonon band structure plots these dispersion curves along high-symmetry directions in the Brillouin zone. The phonon density of states integrates over all wavevectors, giving the distribution of frequencies regardless of which q they came from — this is what powder INS measures directly. 7.4 Calculating phonons in practice Phonon calculations require the Hessian — the matrix of second derivatives of energy with respect to atomic positions. Most E(r) methods provide forces (first derivatives) directly, but not second derivatives. However, second derivatives can be constructed from forces numerically. The Hessian element Hᵢⱼ = ∂²E/∂rᵢ∂rⱼ can be written as: \\[H_{ij} = -\\frac{\\partial F_i}{\\partial r_j}\\] where Fᵢ = −∂E/∂rᵢ is the force on coordinate i. This suggests a numerical approach: displace coordinate j by a small amount δr and see how the forces change. The finite displacement method constructs the Hessian by displacing each atom in turn. For each atom and each Cartesian direction, we shift the atom by +δr, calculate the forces on all atoms, then shift by −δr and recalculate. The central difference gives: \\[H_{ij} \\approx -\\frac{F_i(r_j + \\delta r) - F_i(r_j - \\delta r)}{2\\delta r}\\] For a system with n atoms in the primitive cell, this requires 6n force calculations (positive and negative displacements in x, y, z for each atom). Each force calculation gives one column of the Hessian. Symmetry can reduce the number of required calculations. For a periodic system, we work in a supercell. The supercell must be large enough that the displaced atom doesn’t interact significantly with its own periodic images — otherwise the force constants are contaminated by artificial periodicity. The supercell size also determines the q-point sampling: a 2×2×2 supercell gives the dynamical matrix at q-points commensurate with that supercell. Density functional perturbation theory (DFPT) takes a different approach, calculating the response of the electronic structure to atomic displacements analytically within perturbation theory. This avoids the need for supercells and gives phonons at arbitrary q-points directly, but is more complex to implement and only available for certain \\(E(r)\\) methods. In practice, both approaches are implemented in standard codes. The finite displacement method works with any \\(E(r)\\) method that provides forces — DFT, MLIPs, even classical potentials. DFPT is typically used with DFT. 7.5 What you get Phonon calculations provide both eigenvalues (frequencies) and eigenvectors (displacement patterns). Experimentally, INS gives the vibrational spectrum — the density of states \\(g(\\omega)\\) — but extracting mode-specific information about which atoms move and how requires careful analysis, often guided by computation. The calculation provides both quantities directly. The phonon band structure shows dispersion curves along high-symmetry paths, revealing the frequencies of specific modes and any soft modes or instabilities. Single-crystal measurements can probe specific branches, but for complex materials or difficult-to-grow crystals, computation may be the only practical route to the full dispersion. The phonon density of states gives the distribution of vibrational frequencies, summed over all wavevectors and branches — this is what powder INS measures directly. From the phonon frequencies, thermodynamic quantities follow from standard statistical mechanics. The vibrational free energy: \\[F_{\\text{vib}} = \\sum_{\\mathbf{q},\\nu} \\left[ \\frac{\\hbar\\omega_{\\mathbf{q}\\nu}}{2} + k_B T \\ln\\left(1 - e^{-\\hbar\\omega_{\\mathbf{q}\\nu}/k_B T}\\right) \\right]\\] where the sum runs over wavevectors \\(q\\) and branches \\(\\nu\\). From this, heat capacity and vibrational entropy follow by differentiation. These quantities matter for phase stability at finite temperature — a phase with lower energy may not be stable if another phase has higher entropy. 7.6 Limitations The harmonic approximation assumes small displacements around a well-defined minimum. This breaks down in several situations. At high temperatures, atomic displacements become large and the quadratic approximation fails. Anharmonic terms in the potential — the higher-order terms we truncated — become significant. This leads to effects like phonon-phonon scattering, finite thermal conductivity, and temperature-dependent phonon frequencies. This is true anharmonicity: atoms still oscillate around equilibrium positions, but the potential well is not quadratic. When the harmonic approximation fails in this sense, one option stays within the lattice dynamics framework but includes higher-order terms. Recall that the harmonic approximation truncates the Taylor expansion at second order. Including third and fourth order terms: \\[E = E_0 + \\frac{1}{2}\\sum_{ij} H_{ij} \\delta r_i \\delta r_j + \\frac{1}{6}\\sum_{ijk} \\Phi_{ijk} \\delta r_i \\delta r_j \\delta r_k + \\frac{1}{24}\\sum_{ijkl} \\Phi_{ijkl} \\delta r_i \\delta r_j \\delta r_k \\delta r_l + \\ldots\\] The third-order terms $_{ijk} describe three-phonon processes — one phonon decaying into two, or two combining into one. These are responsible for finite thermal conductivity. The fourth-order terms describe four-phonon processes. The problem is that while the Hessian has n2 elements (for n atomic coordinates), the third-order tensor has n3 elements and the fourth-order tensor has n4. More fundamentally, the harmonic problem gives 3N independent modes. Including anharmonicity couples these modes to each other: you need to consider how each mode interacts with every other mode (for three-phonon) or every pair of modes (for four-phonon). The number of terms grows combinatorially, making these calculations substantially more expensive. Some systems go beyond anharmonicity — the picture of atoms oscillating around fixed equilibrium positions breaks down entirely. Superionic conductors, where ions diffuse rapidly through a solid framework, cannot be described by vibrations around fixed sites. Soft modes that approach zero frequency signal structural instabilities. Phase transitions involve large-amplitude motion between different structural basins. For these, no Taylor expansion around a single minimum will suffice. "],["molecular-dynamics-principles.html", "8 Molecular dynamics: principles 8.1 From local curvature to global exploration 8.2 Equations of motion 8.3 Numerical integration 8.4 Phase space and trajectories 8.5 Ensembles and what they mean 8.6 Thermostats and barostats 8.7 What you get 8.8 The E(r) choice", " 8 Molecular dynamics: principles 8.1 From local curvature to global exploration Geometry optimisation finds minima on the potential energy surface. Phonon calculations characterise the curvature at those minima — how the energy changes for small displacements. Both assume atoms remain close to their equilibrium positions: optimisation iterates toward a minimum, and the harmonic approximation expands the energy around it. At finite temperature, this picture becomes incomplete. Atoms have kinetic energy and explore the potential energy surface. For mildly anharmonic systems, we can extend lattice dynamics with higher-order terms — but as we saw, this becomes expensive. For systems where atoms don’t remain near any single minimum — diffusing ions, phase transitions, liquids — no Taylor expansion around one point will suffice. Molecular dynamics takes a different approach. Rather than expanding E(r) and truncating, we follow the actual motion of atoms over the potential energy surface. Given positions and velocities at some instant, we calculate the forces on all atoms, use these to update the velocities and positions a short time later, and repeat. The result is a trajectory: a sequence of configurations showing how the system evolves in time. 8.2 Equations of motion The force on atom i is the negative gradient of the potential energy: \\[\\mathbf{F}_i = -\\nabla_i E(\\mathbf{r})\\] Given the force, Newton’s second law gives the acceleration: \\[m_i \\frac{d^2 \\mathbf{r}_i}{dt^2} = \\mathbf{F}_i\\] The motion of our system is described by two coupled differential equations: \\[\\frac{d\\mathbf{r}}{dt} = \\mathbf{v}\\] \\[\\frac{d\\mathbf{v}}{dt} = \\mathbf{a} = \\frac{\\mathbf{F}}{m}\\] If we could integrate these equations exactly, we could predict the positions and velocities at any future time. But the forces depend on positions, which change as the system evolves — for any realistic system, we cannot solve these equations analytically. 8.3 Numerical integration Instead, we integrate numerically. The principle is to approximate the continuous equations over small time intervals. If we assume the acceleration is roughly constant over a short time Δt, we can write: \\[\\mathbf{v}(t + \\Delta t) \\approx \\mathbf{v}(t) + \\mathbf{a}(t) \\Delta t\\] \\[\\mathbf{r}(t + \\Delta t) \\approx \\mathbf{r}(t) + \\mathbf{v}(t) \\Delta t + \\frac{1}{2}\\mathbf{a}(t) \\Delta t^2\\] This is Euler integration: use the current acceleration and velocity to predict the state at the next timestep, then repeat. The approximation becomes exact as Δt → 0, but practical calculations require finite timesteps. Euler integration has problems. First, it introduces systematic errors at each step — small discrepancies between the predicted trajectory and the true one. These errors accumulate over many steps, causing the total energy to drift rather than remaining constant as it should for an isolated system. Second, Euler integration is not time-reversible. Newtonian mechanics is symmetric under time reversal: if we reverse all velocities, the system should retrace its path. Euler’s method breaks this symmetry because it uses information only from the beginning of each timestep. A forward step followed by reversing velocities and stepping again does not return to the starting point. The velocity Verlet algorithm, used in most practical MD codes, avoids these problems. It updates positions using both the current and next accelerations: \\[\\mathbf{r}(t + \\Delta t) = \\mathbf{r}(t) + \\mathbf{v}(t) \\Delta t + \\frac{1}{2}\\mathbf{a}(t) \\Delta t^2\\] \\[\\mathbf{v}(t + \\Delta t) = \\mathbf{v}(t) + \\frac{1}{2}[\\mathbf{a}(t) + \\mathbf{a}(t + \\Delta t)] \\Delta t\\] The velocity update uses the average of the old and new accelerations, making the algorithm time-reversible. Energy is not conserved exactly, but fluctuates around the correct value without systematic drift. The timestep must be small enough to capture the fastest motion in the system — typically the highest-frequency vibrations — which means Δt is usually around 1 femtosecond. A nanosecond trajectory therefore requires 10⁶ timesteps, each requiring a force evaluation. 8.4 Phase space and trajectories The state of a classical system is specified by the positions and momenta of all atoms — a point in phase space. As we integrate the equations of motion, this point traces out a trajectory through phase space. For an isolated system (no external heat bath), the total energy E = K + V is conserved — kinetic energy converts to potential and back, but the sum remains constant. The trajectory is confined to a surface of constant energy in phase space. This is the microcanonical ensemble: all accessible states have the same total energy. The temperature in such a system is related to the average kinetic energy: \\[\\langle K \\rangle = \\frac{3}{2} N k_B T\\] where N is the number of atoms. This follows from the equipartition theorem: each quadratic degree of freedom contributes ½kT to the average energy, and kinetic energy is quadratic in the velocities. 8.5 Ensembles and what they mean Real experiments are usually not performed at constant energy. A sample in a furnace exchanges heat with its surroundings; it has a well-defined temperature, not a fixed total energy. Different experimental conditions correspond to different statistical ensembles. The microcanonical ensemble (NVE) has fixed particle number N, volume V, and energy E. This is what unmodified Newtonian dynamics samples. It describes an isolated system. The canonical ensemble (NVT) has fixed N, V, and temperature T. The system exchanges energy with a heat bath at temperature T. States are weighted by the Boltzmann factor exp(−E/kT): lower-energy configurations are more probable, but higher-energy configurations are accessible with probability that decreases exponentially. The isothermal-isobaric ensemble (NPT) has fixed N, pressure P, and temperature T. The system exchanges both energy and volume with its surroundings. This corresponds to most laboratory conditions — a sample at ambient pressure and controlled temperature. Which ensemble we simulate determines what statistical distribution we sample. This matters for calculating thermodynamic properties and for comparing with experiment. 8.6 Thermostats and barostats To simulate NVT or NPT, we need to modify the equations of motion so that the trajectory samples the correct statistical distribution. A thermostat couples the system to a heat bath; a barostat couples it to a pressure reservoir. The details of how thermostats work are technical, but the key point is conceptual: a good thermostat is designed so that, over a long trajectory, configurations are visited with the correct Boltzmann weights. The time-average of any property over the trajectory then equals the ensemble average — the thermodynamic expectation value. This is the ergodic hypothesis in practice: for a sufficiently long trajectory, the time spent in each region of phase space is proportional to its statistical weight. We can therefore compute ensemble averages — the quantities that connect to thermodynamics and to time-averaged experimental measurements — from a single long trajectory. 8.7 What you get A molecular dynamics simulation produces a trajectory: atomic positions (and velocities) at each timestep. From this trajectory, we can calculate: Equilibrium properties — averages over the trajectory. The mean potential energy, the average structure, the distribution of atomic positions. These are ensemble averages, directly comparable to time-averaged experimental measurements. Dynamic properties — how the system evolves in time. How do atomic positions correlate between different times? How quickly do correlations decay? These time correlation functions contain information about dynamics that equilibrium measurements cannot access. Transport properties — diffusion coefficients, viscosities, thermal conductivities. These emerge from how quickly the system loses memory of its initial state. 8.8 The E(r) choice Every MD timestep requires calculating forces — one evaluation of E(r) and its gradient. A typical simulation might run for millions of timesteps. The cost of E(r) therefore determines what simulations are feasible. Classical potentials are cheap to evaluate. Simulations of millions of atoms for nanoseconds to microseconds are routine. The limitation is accuracy: the functional form constrains what physics can be captured. Ab initio MD (AIMD) calculates forces from DFT at each timestep. This is accurate but expensive — practical for hundreds of atoms and picoseconds. Many dynamical processes happen on longer timescales or require larger system sizes. Machine-learned interatomic potentials approach DFT accuracy at near-classical cost. They are increasingly the method of choice when classical potentials aren’t accurate enough but AIMD is too expensive. The caveats from Lecture I apply: they inherit limitations from their training data, and can fail outside their training domain. The choice is problem-dependent. For a quick estimate of dynamics in a well-characterised material, classical potentials may suffice. For quantitative comparison with experiment in a system where accuracy matters, MLIPs or AIMD may be necessary. "],["introduction-2.html", "9 Introduction", " 9 Introduction In Lecture I, we covered methods for calculating E(r) — classical potentials, DFT, and machine-learned interatomic potentials. In Lecture II, we turned to what we do with that ability: finding stable structures through geometry optimisation, characterising vibrations through phonon calculations, and simulating atomic motion through molecular dynamics. MD samples configurations by following the physical dynamics of the system. At finite temperature, atoms have kinetic energy and explore the potential energy surface. For fast processes — ionic diffusion in superionic conductors, thermal vibrations — MD captures the relevant configurations. But some configurational questions involve rearrangements that are far too slow for MD. Consider cation ordering in a mixed-metal oxide, or site occupancies in an alloy. The atoms can, in principle, rearrange — but the mechanisms are slow, involving vacancy diffusion or other activated processes that happen on timescales of seconds to hours, not nanoseconds. MD will never visit these configurations; the simulation is trapped in whatever arrangement it started with. Monte Carlo sampling takes a different approach. Rather than following dynamics, we propose configurational changes directly and accept or reject them based on energetics. This lets us sample configurations that MD cannot reach. "],["monte-carlo-sampling.html", "10 Monte Carlo sampling 10.1 The goal: ensemble averages 10.2 Why not sample uniformly? 10.3 Markov chain Monte Carlo 10.4 The Metropolis algorithm 10.5 What you get 10.6 What you don’t get 10.7 MC for configurational disorder 10.8 Neutron connection 10.9 The E(r) cost", " 10 Monte Carlo sampling 10.1 The goal: ensemble averages Statistical mechanics tells us that macroscopic properties are ensemble averages. The expectation value of a property A in the canonical ensemble is: \\[\\langle A \\rangle = \\sum_i A_i P_i = \\frac{1}{Z} \\sum_i A_i \\exp(-E_i / k_B T)\\] where the sum runs over all microstates i, Eᵢ is the energy of state i, and Z is the partition function: \\[Z = \\sum_i \\exp(-E_i / k_B T)\\] In principle, this is straightforward: enumerate all states, calculate their energies and properties, weight by Boltzmann factors, sum. In practice, the number of states is astronomically large. For configurational disorder on N sites with two possible occupants per site, there are 2N configurations. Even for modest N, explicit enumeration is impossible. 10.2 Why not sample uniformly? One idea: sample configurations randomly and weight by their Boltzmann factors. The problem is that uniform sampling is inefficient. Most randomly chosen configurations have high energy and negligible Boltzmann weight — we waste effort sampling configurations that contribute almost nothing to the average. The solution is importance sampling: sample configurations in proportion to their Boltzmann weights. Then each sample contributes equally to the average: \\[\\langle A \\rangle \\approx \\frac{1}{M} \\sum_{m=1}^{M} A_m\\] No explicit weights needed. The question is how to generate samples from this distribution. 10.3 Markov chain Monte Carlo The solution is to construct a Markov chain — a random walk through configuration space where each step depends only on the current configuration. At each step we propose a move to a new configuration and either accept or reject it. The question is: how should we choose the acceptance probabilities to ensure we sample from our target distribution? Consider two states. At equilibrium, the frequency of transitions from state 1 to state 2 must equal the frequency from 2 to 1 — otherwise probability would accumulate in one state. If we visit state 1 with probability π₁ and accept moves to state 2 with probability P(1→2), then balance requires: \\[\\pi_1 \\, P(1 \\to 2) = \\pi_2 \\, P(2 \\to 1)\\] Rearranging: the ratio of acceptance probabilities must equal the ratio of target probabilities: \\[\\frac{P(1 \\to 2)}{P(2 \\to 1)} = \\frac{\\pi_2}{\\pi_1}\\] For many states, we impose this constraint on every pair. The result is that we sample each state with probability proportional to our target distribution. For the Boltzmann distribution, the ratio of target probabilities is: \\[\\frac{\\pi_2}{\\pi_1} = \\frac{e^{-E_2/kT}}{e^{-E_1/kT}} = e^{-\\Delta E/kT}\\] This depends only on the energy difference between the two states — not on absolute energies. Since we only ever compare states pairwise, we never need to know the partition function or evaluate absolute energies. Any acceptance rule where the ratio of forward to backward acceptance probabilities equals exp(−ΔE/kT) will sample from the Boltzmann distribution. The Metropolis criterion is one such choice. 10.4 The Metropolis algorithm The Metropolis algorithm is the standard approach. The procedure is: Start from some configuration with energy E₁ Propose a move — for configurational disorder, typically swapping two atoms of different types Calculate the energy E₂ of the new configuration Accept or reject the move: If ΔE ≤ 0 (energy decreased): accept If ΔE &gt; 0 (energy increased): accept with probability exp(−ΔE/kT) If accepted, the new configuration becomes the current one; if rejected, stay in the old configuration Repeat from step 2 The acceptance rule ensures that, after many steps, the chain visits configurations with probability proportional to exp(−E/kT). The intuition for why the algorithm works: Downhill moves are always accepted. The chain readily finds low-energy configurations. Uphill moves are sometimes accepted. The probability exp(−ΔE/kT) decreases exponentially with the energy cost, but is never zero. This allows the chain to escape local minima and explore configuration space. Without occasional uphill moves, we would simply find the nearest local minimum — an optimisation algorithm, not a sampling algorithm. Temperature controls exploration. At high temperature, exp(−ΔE/kT) is close to 1 for moderate ΔE, so most moves are accepted and the system explores widely. At low temperature, only small uphill moves are accepted and the system stays near low-energy configurations. 10.5 What you get After an initial equilibration period, the MC chain samples configurations from the target ensemble. From these we calculate: Ensemble averages — the expectation value of any quantity A is simply the average over sampled configurations: \\[\\langle A \\rangle = \\frac{1}{M} \\sum_{m=1}^{M} A_m\\] No Boltzmann weights appear because the sampling already accounts for them. Thermodynamic properties — from fluctuations in energy we get heat capacity; from the temperature dependence of order parameters we identify phase transitions. Equilibrium configurations — snapshots from the MC chain represent the distribution of configurations at thermal equilibrium. These can be analysed for structural features or compared with experiment. 10.6 What you don’t get MC does not give dynamics. The sequence of configurations is a random walk designed to sample the equilibrium distribution, not a physical trajectory through time. There is no “MC time” that corresponds to real time. Questions about rates, diffusion coefficients, or time correlation functions cannot be answered by MC. This is the fundamental distinction: MD samples by following dynamics and gives both equilibrium and dynamic properties; MC samples configuration space directly and gives only equilibrium properties. 10.7 MC for configurational disorder For site disorder — which atom occupies which site — MC is the natural approach. The configuration is specified by site occupancies, and a move swaps two atoms of different types. Consider a binary alloy A₁₋ₓBₓ on a fixed lattice. At high temperature, entropy favours disorder — a random arrangement maximises configurational entropy. At low temperature, energy may favour ordering — if A-B neighbours are more favourable than A-A or B-B, ordered arrangements are lower in energy. MC naturally captures this competition: at each temperature, we sample configurations with the correct Boltzmann weights and observe whether the system orders or disorders. 10.8 Neutron connection Neutron diffraction measures a spatial average — the scattering from many unit cells within the illuminated volume. For crystallographically disordered systems where atoms are not diffusing, this spatial average is what determines the measured structure. MC models this as an average over configurations. If the system is statistically homogeneous — if any region looks like any other on average — then the spatial average equals the ensemble average over configurations. MD provides the same ensemble average, but samples configuration space by following dynamics rather than by proposing direct moves. The pair distribution function from MC configurations can be compared directly with experimental PDF from total scattering. This is particularly powerful for systems with short-range order, where Bragg diffraction sees only the average but PDF captures local correlations. 10.9 The E(r) cost MC requires evaluating the energy change ΔE at every step. A typical MC simulation involves millions of proposed moves. The cost of E(r) therefore matters greatly. For configurational disorder, we need energies for many different arrangements. If E(r) is DFT, this is prohibitively expensive — a single DFT calculation might take minutes to hours, and we need millions of them. We need a faster way to evaluate configurational energies. "],["cluster-expansion.html", "11 Cluster expansion 11.1 The problem 11.2 The idea 11.3 Symmetry and cluster types 11.4 Fitting the expansion 11.5 What you get 11.6 Limitations 11.7 Connection to other methods", " 11 Cluster expansion 11.1 The problem Monte Carlo sampling requires many energy evaluations — typically millions. For configurational disorder, where we’re asking which atoms sit on which sites, we need \\(E(r)\\) for many different atomic arrangements on the same underlying lattice. DFT provides accurate energies but is expensive. A single DFT calculation for a modest supercell might take minutes to hours. Running millions of them is impractical. Classical potentials are fast but may not capture the relevant chemistry. The energy differences between different orderings can be subtle — tens of meV per atom — and depend on details of the electronic structure. We need a method that is fast enough for MC (millions of evaluations) but accurate enough to capture the energetics of configurational disorder (DFT-level accuracy). Cluster expansions provide this. 11.2 The idea For configurational disorder on a fixed lattice, the atomic positions don’t change — only the site occupancies. The configuration is specified by which sites have which atoms. This is a discrete problem: each site has one of a finite number of possible occupants. We can represent the configuration using occupation variables. For a binary system (A and B atoms), assign σᵢ = +1 if site i has atom A, σᵢ = −1 if site i has atom B. The configuration is then specified by the set of all σᵢ. The cluster expansion writes the energy as a function of these occupation variables: \\[E(\\{\\sigma\\}) = J_0 + \\sum_i J_i \\sigma_i + \\sum_{i&lt;j} J_{ij} \\sigma_i \\sigma_j + \\sum_{i&lt;j&lt;k} J_{ijk} \\sigma_i \\sigma_j \\sigma_k + \\ldots\\] The first term J₀ is a constant (the reference energy). The second sum runs over single sites — but for a stoichiometric system, the number of A and B atoms is fixed, so Σσᵢ is constant and this term just shifts the reference. The third sum runs over pairs of sites; Jᵢⱼ captures how the energy depends on whether sites i and j have the same or different atoms. Higher-order terms capture three-body, four-body, and larger cluster interactions. 11.3 Symmetry and cluster types For a periodic crystal, symmetry constrains the expansion. All pairs related by symmetry have the same coefficient: nearest-neighbour pairs share one J value, second-neighbour pairs share another, and so on. We group sites into clusters by type — nearest-neighbour pairs, second-neighbour pairs, nearest-neighbour triangles, etc. — and assign one coefficient per cluster type. The expansion becomes: \\[E = J_0 + \\sum_\\alpha J_\\alpha \\, m_\\alpha \\, \\langle \\Pi_\\alpha \\rangle\\] where \\(\\alpha\\) labels cluster types, mα is the multiplicity (how many clusters of that type per site), and ⟨Πα⟩ is the average value of the product of occupation variables over all clusters of type α. The details of this notation are less important than the key point: symmetry reduces a potentially infinite expansion to a finite set of parameters {\\(J_\\alpha\\)}. 11.4 Fitting the expansion The coefficients {$ are unknown. We determine them by fitting to DFT calculations. The procedure is: Generate a set of training configurations — different arrangements of A and B atoms on the lattice Calculate the DFT energy of each configuration Fit the cluster expansion coefficients to reproduce these energies The fitting is a linear least-squares problem: the energy is linear in the coefficients \\(J_\\alpha\\), so we minimise the squared error between predicted and DFT energies. The choice of training configurations matters. They should span the range of orderings we expect to encounter — not just random configurations, but also ordered structures, configurations with different degrees of short-range order, and any arrangements relevant to the problem at hand. Too few configurations or a poorly chosen set can lead to an expansion that fits the training data but fails elsewhere. The choice of which clusters to include also matters. More clusters give more flexibility but risk overfitting — capturing noise in the DFT data rather than physical trends. Fewer clusters may miss important interactions. Various approaches exist to select clusters and prevent overfitting: cross-validation, regularisation, or systematic testing against configurations not in the training set. 11.5 What you get A fitted cluster expansion gives \\(E(\\sigma)\\) for any configuration almost instantly — it’s just a sum over clusters. This enables: Extensive MC sampling — millions of steps at effectively DFT accuracy. We can map out phase diagrams, find order-disorder transition temperatures, and characterise the distribution of configurations at any temperature. Ground state searching — by systematically enumerating configurations or using optimisation algorithms, we can find the lowest-energy orderings. Thermodynamic properties — heat capacities, ordering enthalpies, and entropies from MC sampling. 11.6 Limitations Cluster expansions are limited to configurational degrees of freedom. The atoms sit on fixed lattice sites; only the occupancies vary. Relaxation — the small displacements of atoms from ideal positions depending on local environment — is not captured directly, though it can be included approximately by fitting to relaxed DFT energies. The expansion assumes the energy can be written as a sum over clusters. This works well for many systems but may fail when long-range interactions (beyond the clusters included) are important, or when the electronic structure changes qualitatively with ordering. The quality depends entirely on the training data and the choice of clusters. A cluster expansion is a fit to DFT — it inherits the accuracy of the underlying DFT calculations and can only interpolate, not extrapolate. If the training set doesn’t include configurations similar to what you’re asking about, the expansion may give wrong answers. 11.7 Connection to other methods Cluster expansions are trained on DFT data — typically supercell calculations for different orderings. The training set might include 50–200 configurations, each requiring a DFT calculation. This is expensive upfront but pays off when we need millions of energy evaluations for MC. The combination of DFT + cluster expansion + MC is a powerful workflow for studying configurational disorder: DFT provides the accurate energetics, the cluster expansion makes it fast, and MC samples the configurations. "],["short-range-order-and-pdf.html", "12 Short-range order and PDF 12.1 Long-range order vs short-range order 12.2 What Bragg diffraction misses 12.3 PDF captures local structure 12.4 Computational approach 12.5 Warren-Cowley parameters 12.6 The connection to energy materials", " 12 Short-range order and PDF 12.1 Long-range order vs short-range order Consider a binary alloy A0.5B0.5 on a BCC lattice. At low temperature, it might order into a CsCl-type structure: all A atoms on one sublattice, all B on the other. This is long-range order (LRO) — a periodic pattern that extends throughout the crystal. Bragg diffraction sees this clearly: the ordering produces superlattice reflections at positions forbidden by the disordered structure. At high temperature, entropy wins and the alloy disorders: A and B atoms distributed randomly over sites. No long-range pattern, no superlattice reflections. But there is an intermediate regime. Even without long-range order, there may be local correlations — short-range order (SRO). Perhaps A atoms prefer B neighbours, or avoid other A atoms, but only over the first few coordination shells. Beyond that, the arrangement is random. There’s no periodic pattern, but there is local structure. 12.2 What Bragg diffraction misses Bragg diffraction is sensitive to the average, periodic structure. For a disordered alloy without LRO, you see only the underlying lattice with average scattering length. The local correlations show up as diffuse scattering between Bragg peaks — broad features that are often ignored or hard to analyse quantitatively. Consider what diffraction tells you about site occupancies. If a site is 50% A and 50% B on average, that’s what you measure — regardless of whether A and B are randomly distributed or have strong local correlations. Two structures with the same average occupancy but different SRO give the same Bragg intensities. This is a fundamental limitation: Bragg diffraction reports the spatially averaged structure. Local correlations that don’t produce a periodic pattern are invisible to it. 12.3 PDF captures local structure Total scattering includes both Bragg and diffuse contributions. The Fourier transform of the total scattering gives the pair distribution function G(r), which measures the probability of finding atoms at separation \\(r\\). For a single-element material, \\(g(r)\\) has peaks at the neighbour distances — first shell, second shell, and so on. For a multi-component material, we can define partial pair distribution functions \\(g_{\\alpha\\beta}(r)\\): the probability of finding a \\(\\beta\\) atom at distance \\(r\\) from an α atom. These partials contain information about local ordering. If A atoms preferentially have B neighbours, the A-B partial shows enhanced correlations at the nearest-neighbour distance compared to a random distribution. If A atoms cluster together, the A-A partial is enhanced instead. PDF analysis can extract this local structural information. The challenge is that a measured PDF sums over all partials, weighted by scattering lengths and concentrations. Separating the contributions — seeing the A-A vs A-B vs B-B correlations independently — requires additional information or constraints. 12.4 Computational approach This is where computation and experiment are powerfully complementary. From MC (with cluster expansion), we generate an ensemble of configurations at thermal equilibrium. Each configuration specifies which atoms are on which sites. From these configurations we directly calculate partial pair distribution functions — we know exactly which atom is where, so separating by species is trivial. We can then: Compare total \\(g(r)\\) to experiment. Sum the partials with appropriate weights and compare to the measured PDF. Agreement means our model captures the local structure; disagreement points to problems with the cluster expansion or the assumed chemistry. Analyse what we can’t measure. The partials themselves — \\(g_\\mathrm{AA}(r)\\), \\(g_\\mathrm{AB}(r)\\), \\(g_\\mathrm{BB}(r)\\) — reveal the nature of the short-range order. Does A prefer A or B neighbours? How far do correlations extend? These are direct answers to structural questions that experiment alone struggles to resolve. Test structural hypotheses. If experimentalists propose a model for local ordering based on PDF analysis, we can test it: does that model correspond to a thermodynamically reasonable arrangement? What temperature would produce that degree of order? 12.5 Warren-Cowley parameters A common way to quantify SRO is through Warren-Cowley parameters. For a binary alloy, the parameter for shell n is: \\[\\alpha_n = 1 - \\frac{p_n^{AB}}{x_B}\\] where pₙᴬᴮ is the probability that a site in shell n around an A atom is occupied by B, and xB is the overall B concentration. If atoms are randomly distributed, \\(p_n^\\mathrm{AB} = x_\\mathrm{B}\\) and \\(α_n = 0\\). If A atoms preferentially have B neighbours in shell n, \\(p_n^\\mathrm{AB} &gt; x_\\mathrm{B}\\) and \\(\\alpha_n &lt; 0\\). If A atoms avoid B neighbours (clustering of like atoms), \\(\\alpha_n &gt; 0\\). These parameters can be extracted from diffuse scattering measurements and compared directly with values calculated from MC simulations. This provides a quantitative check on whether the simulated SRO matches experiment. 12.6 The connection to energy materials Short-range order matters for properties. In battery cathode materials, the arrangement of different transition metals affects voltage, capacity, and cycling stability. In thermoelectrics, local ordering influences phonon scattering and thermal conductivity. In solid electrolytes, how dopants are arranged can enhance or block ion transport. Bragg diffraction might tell you the average composition on each site. But the local arrangement — the short-range order — can be the key to understanding performance. PDF, combined with computational modelling, provides access to this local structure. "],["wrap-up.html", "13 Wrap-up 13.1 The two-axis picture revisited 13.2 E(r) choice is problem-dependent 13.3 Three modes of interaction 13.4 Bidirectionality 13.5 Where to go from here 13.6 Resources", " 13 Wrap-up 13.1 The two-axis picture revisited We began with a conceptual framework: two independent choices determine what a computational study looks like. How do we calculate E(r)? Classical potentials are fast but limited by their functional form. DFT is accurate and transferable but expensive. Machine-learned potentials can approach DFT accuracy at near-classical cost, though they require good training data and can fail outside their domain. What do we do with E(r)? Geometry optimisation finds stable structures. Phonon calculations characterise vibrations in the harmonic limit. Molecular dynamics simulates atomic motion at finite temperature. Monte Carlo samples configurational space directly. These choices are orthogonal. You can do MD with classical potentials, DFT, or MLIPs. You can drive MC with a cluster expansion (fitted to DFT) or an MLIP. The methods for calculating energy and the methods for using that energy are separate decisions, made based on what accuracy you need and what computational cost you can afford. 13.2 E(r) choice is problem-dependent Different questions favour different combinations: Question Method E(r) choice Why Accurate structure Geometry optimisation DFT Few evaluations, accuracy matters Phonons Lattice dynamics DFT or MLIP Need accurate force constants Diffusion dynamics MD MLIP or classical Long trajectories, many atoms Configurational disorder MC CE (from DFT) Many configurations, lattice fixed There’s no single best method. The choice depends on the question you’re asking and the system you’re studying. 13.3 Three modes of interaction Throughout these lectures, we’ve seen three ways computation and experiment connect: Direct comparison. Calculate the same observable — g(r), S(Q,ω), phonon DOS — and compare quantitatively. Agreement gives confidence; disagreement identifies problems or interesting physics. Mechanistic interpretation. Computation can explain why you see what you see. Why does this ion diffuse fast? What causes the local distortion? Where does the anharmonicity come from? The simulation provides microscopic detail that experiment averages over. Complementary information. Some quantities are hard or impossible to measure directly but straightforward to calculate. Energies, barriers, the relative stability of structures, the partials of a pair distribution function — computation provides what experiment cannot access. 13.4 Bidirectionality The relationship goes both ways. Computation predicts; experiment tests. But equally: experiment reveals puzzles that computation can address. A PDF that doesn’t fit the expected structure. Dynamics faster than models predict. An unexpected phase transition. These observations drive computational investigation. “This doesn’t look right” is where interesting science often starts. The most productive work is genuinely collaborative — not modellers serving experimentalists or vice versa, but a dialogue where each side contributes what it does best. 13.5 Where to go from here You don’t need to become a computational scientist. But knowing what’s possible, what’s reliable, and how computational results connect to your measurements makes you a better scientist. When reading papers: What E(r) method was used? What approximations were made? How does the calculated quantity relate to what was measured? Is the comparison direct, or are there steps in between? When collaborating: What question are you trying to answer? What experimental data constrain the problem? What can computation add that experiment can’t provide? When reviewing: Are the computational methods appropriate for the question? Are the comparisons with experiment meaningful? What are the limitations? The tools are increasingly accessible. Codes are available; MLIPs make large simulations feasible; workflows are becoming standardised. The barrier to using computation is lower than it’s ever been. Understanding what it can and cannot tell you is more important than ever. 13.6 Resources Textbooks: [To be added — suggestions for accessible introductions to computational materials science] Software: - DFT: VASP, Quantum ESPRESSO, CASTEP - MD: LAMMPS, ASE - MLIPs: MACE, GAP, CHGNet - Cluster expansion: CLEASE, icet, CASM - Phonons: Phonopy And most importantly: talk to a modeller. "],["appendix-derivation-of-the-dynamical-matrix-for-periodic-systems.html", "A Appendix: Derivation of the dynamical matrix for periodic systems A.1 Setup and notation A.2 Translational symmetry A.3 Plane-wave ansatz A.4 Substitution into equations of motion A.5 The dynamical matrix A.6 The key result A.7 Connection to experiment", " A Appendix: Derivation of the dynamical matrix for periodic systems This appendix provides the full derivation of how translational symmetry reduces the phonon problem from a 3N × 3N eigenvalue problem (for N atoms in the crystal) to a 3n × 3n problem (for n atoms in the primitive cell) at each wavevector q. A.1 Setup and notation Consider a periodic crystal with primitive lattice vectors. Let Rₗ denote the position of unit cell \\(l\\), and let atom \\(i\\) within the cell sit at position Rₗ + τᵢ, where τᵢ is the position of atom \\(i\\) within the primitive cell. The displacement of atom \\(i\\) in cell \\(l\\) from its equilibrium position is uᵢ(l). The equation of motion is: \\[m_i \\frac{d^2 \\mathbf{u}_i(l)}{dt^2} = -\\sum_{l&#39;} \\sum_j \\Phi_{ij}(l, l&#39;) \\, \\mathbf{u}_j(l&#39;)\\] where \\(\\Phi_{ij}(l, l&#39;)\\) is the force constant matrix coupling atom \\(i\\) in cell \\(l\\) to atom \\(j\\) in cell \\(l&#39;\\). This is a 3 × 3 matrix (coupling x, y, z components), but we suppress the Cartesian indices for clarity. A.2 Translational symmetry The key property is translational invariance: the force constants depend only on the separation between cells, not on which cell we started from: \\[\\Phi_{ij}(l, l&#39;) = \\Phi_{ij}(\\mathbf{R}_l - \\mathbf{R}_{l&#39;}) = \\Phi_{ij}(\\mathbf{R}_{l-l&#39;})\\] We can write this more simply by defining \\(\\mathbf{R} = \\mathbf{R}_l− \\mathbf{R}_{l^\\prime}\\), so \\(\\Psi_{ij}(\\mathbf{R})\\) is the force constant coupling atom \\(j\\) at the origin to atom \\(i\\) in the cell at \\(\\mathbf{R}\\). A.3 Plane-wave ansatz We seek solutions where all atoms oscillate at the same frequency ω. Based on the discussion in the main text, translational symmetry constrains the allowed phase relationships to plane waves. We therefore assume: \\[\\mathbf{u}_i(l) = \\mathbf{e}_i \\exp(i\\mathbf{q} \\cdot \\mathbf{R}_l) \\exp(-i\\omega t)\\] where eᵢ is the displacement amplitude (a complex vector) for atom i within the unit cell, and q is the wavevector. The time dependence exp(−iωt) gives the harmonic oscillation; the spatial dependence exp(iq·Rₗ) encodes the phase variation from cell to cell. A.4 Substitution into equations of motion Substituting the plane-wave form into the equation of motion: \\[-m_i \\omega^2 \\mathbf{e}_i \\exp(i\\mathbf{q} \\cdot \\mathbf{R}_l) = -\\sum_{l&#39;} \\sum_j \\Phi_{ij}(\\mathbf{R}_l - \\mathbf{R}_{l&#39;}) \\, \\mathbf{e}_j \\exp(i\\mathbf{q} \\cdot \\mathbf{R}_{l&#39;})\\] where we have cancelled the common exp(−iωt) factor from both sides. Now we use translational invariance. Define R = Rₗ − Rₗ’, so Rₗ’ = Rₗ − R. The sum over l’ becomes a sum over R: \\[-m_i \\omega^2 \\mathbf{e}_i \\exp(i\\mathbf{q} \\cdot \\mathbf{R}_l) = -\\sum_{\\mathbf{R}} \\sum_j \\Phi_{ij}(\\mathbf{R}) \\, \\mathbf{e}_j \\exp(i\\mathbf{q} \\cdot (\\mathbf{R}_l - \\mathbf{R}))\\] The exp(iq·Rₗ) factor appears on both sides: \\[-m_i \\omega^2 \\mathbf{e}_i \\exp(i\\mathbf{q} \\cdot \\mathbf{R}_l) = -\\sum_{\\mathbf{R}} \\sum_j \\Phi_{ij}(\\mathbf{R}) \\, \\mathbf{e}_j \\exp(i\\mathbf{q} \\cdot \\mathbf{R}_l) \\exp(-i\\mathbf{q} \\cdot \\mathbf{R})\\] Dividing both sides by exp(iq·Rₗ): \\[m_i \\omega^2 \\mathbf{e}_i = \\sum_{\\mathbf{R}} \\sum_j \\Phi_{ij}(\\mathbf{R}) \\exp(-i\\mathbf{q} \\cdot \\mathbf{R}) \\, \\mathbf{e}_j\\] A.5 The dynamical matrix Define the q-dependent force constant matrix: \\[C_{ij}(\\mathbf{q}) = \\sum_{\\mathbf{R}} \\Phi_{ij}(\\mathbf{R}) \\exp(-i\\mathbf{q} \\cdot \\mathbf{R})\\] This is the Fourier transform of the real-space force constants. The equation of motion becomes: \\[m_i \\omega^2 \\mathbf{e}_i = \\sum_j C_{ij}(\\mathbf{q}) \\, \\mathbf{e}_j\\] To put this in standard eigenvalue form, we define mass-weighted displacements ẽᵢ = √mᵢ eᵢ and the dynamical matrix: \\[D_{ij}(\\mathbf{q}) = \\frac{C_{ij}(\\mathbf{q})}{\\sqrt{m_i m_j}}\\] The equation becomes: \\[\\omega^2 \\tilde{\\mathbf{e}}_i = \\sum_j D_{ij}(\\mathbf{q}) \\, \\tilde{\\mathbf{e}}_j\\] or in matrix form: \\[\\omega^2 \\tilde{\\mathbf{e}} = \\mathbf{D}(\\mathbf{q}) \\tilde{\\mathbf{e}}\\] This is a standard eigenvalue problem. The eigenvalues are ω² and the eigenvectors ẽ give the (mass-weighted) displacement patterns within the unit cell. A.6 The key result The dynamical matrix D(q) has dimensions 3n × 3n, where n is the number of atoms in the primitive cell. At each wavevector q, we diagonalise this matrix to obtain 3n eigenvalues ω²(q) and eigenvectors. The full displacement pattern across the crystal is reconstructed by multiplying the unit-cell eigenvector by the phase factor: \\[\\mathbf{u}_i(l) = \\frac{\\mathbf{e}_i}{\\sqrt{m_i}} \\exp(i\\mathbf{q} \\cdot \\mathbf{R}_l) \\exp(-i\\omega t)\\] Different values of q give different phase relationships between unit cells, but the mathematical structure of the eigenvalue problem is the same at each q. This is why we can compute phonon dispersion by solving many small problems rather than one impossibly large one. A.7 Connection to experiment The phonon dispersion relation ω(q) can be measured by inelastic neutron scattering on single crystals, where the scattering condition relates the momentum transfer to q. For powder samples, the measurement averages over all q, giving the phonon density of states. Computationally, we calculate D(q) on a grid of q-points in the Brillouin zone, diagonalise at each point, and either plot the dispersion along high-symmetry paths or integrate to obtain the density of states. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
